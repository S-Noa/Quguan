#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºç‰ˆGrad-CAMå¯è§†åŒ–å·¥å…·ï¼ˆä¿®å¤ç‰ˆï¼‰
æ”¯æŒå¤šç§æ•°æ®é›†é€‰æ‹©ï¼šå¹²å‡€æ•°æ®é›† vs ç‰¹å¾æ•°æ®é›†
æ”¯æŒè‡ªä¸»é€‰æ‹©æ¨¡å‹è·¯å¾„
å¤„ç†ä¸åŒåƒç´ å°ºå¯¸çš„å›¾åƒ
"""

import torch
import argparse
import os
import sys
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# PyTorch 2.6+ å…¼å®¹æ€§ä¿®å¤ï¼ˆé¿å…é€’å½’ï¼‰
def apply_pytorch_compatibility():
    """åº”ç”¨PyTorchå…¼å®¹æ€§ä¿®å¤"""
    # åœ¨æ¨¡å—çº§åˆ«ä¿å­˜åŸå§‹å‡½æ•°
    import types
    
    # è·å–çœŸæ­£çš„åŸå§‹torch.load
    original_torch_load = getattr(torch, '_original_load', None)
    if original_torch_load is None:
        # ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œä¿å­˜åŸå§‹å‡½æ•°
        original_torch_load = torch.load
        torch._original_load = original_torch_load
    
    # æ·»åŠ å®‰å…¨å…¨å±€å˜é‡
    if hasattr(torch.serialization, 'add_safe_globals'):
        try:
            torch.serialization.add_safe_globals([argparse.Namespace])
        except:
            pass
    
    def safe_torch_load(path, map_location=None, **kwargs):
        """å®‰å…¨çš„torch.loadå‡½æ•°"""
        # æ¸…ç†å‚æ•°
        clean_kwargs = {k: v for k, v in kwargs.items() if k != 'weights_only'}
        
        try:
            # å°è¯•PyTorch 2.6+
            return original_torch_load(path, map_location=map_location, weights_only=False, **clean_kwargs)
        except TypeError as e:
            if "unexpected keyword argument 'weights_only'" in str(e):
                # å›é€€åˆ°æ—§ç‰ˆæœ¬
                return original_torch_load(path, map_location=map_location, **clean_kwargs)
            raise
        except Exception as e:
            if "argparse.Namespace" in str(e) or "Weights only load failed" in str(e):
                # å¤„ç†Namespaceé”™è¯¯
                try:
                    if hasattr(torch.serialization, 'safe_globals'):
                        with torch.serialization.safe_globals([argparse.Namespace]):
                            return original_torch_load(path, map_location=map_location, weights_only=True, **clean_kwargs)
                except:
                    pass
                # æœ€ç»ˆå›é€€
                return original_torch_load(path, map_location=map_location, **clean_kwargs)
            raise
    
    # åªåœ¨éœ€è¦æ—¶æ›¿æ¢
    if not hasattr(torch.load, '_is_safe_load'):
        torch.load = safe_torch_load
        torch.load._is_safe_load = True
    
    return True

# åº”ç”¨ä¿®å¤
apply_pytorch_compatibility()
import torch.nn as nn
from torchvision import transforms
from cnn_model import CNNFeatureExtractor
from advanced_cnn_models import AdvancedLaserSpotCNN
import numpy as np
import cv2
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.font_manager as fm
import os
import glob
import json
import argparse
from PIL import Image
from pathlib import Path

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®matplotlibçš„ä¸­æ–‡å­—ä½“"""
    try:
        chinese_fonts = [
            'SimHei',  # é»‘ä½“
            'Microsoft YaHei',  # å¾®è½¯é›…é»‘
            'SimSun',  # å®‹ä½“
            'KaiTi',   # æ¥·ä½“
            'FangSong'  # ä»¿å®‹
        ]
        
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        
        for font in chinese_fonts:
            if font in available_fonts:
                plt.rcParams['font.sans-serif'] = [font]
                plt.rcParams['axes.unicode_minus'] = False
                print(f"âœ“ ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
                return font
        
        print("âŒ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
        return None
        
    except Exception as e:
        print(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        return None

class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
    
    def __call__(self, input_tensor, model_type='traditional_cnn'):
        # ä½¿ç”¨æ— hookçš„æ–¹æ³•é¿å…è§†å›¾ä¿®æ”¹é”™è¯¯
        return self._gradcam_without_hooks(input_tensor, model_type)
    
    def _gradcam_without_hooks(self, input_tensor, model_type):
        """ç›´æ¥ä½¿ç”¨ä¼ ç»Ÿhookæ–¹æ³•ï¼Œä¹‹å‰çš„æ— hookæ–¹æ³•æœ‰æ¢¯åº¦é—®é¢˜"""
        print("ğŸ”„ ç›´æ¥ä½¿ç”¨ä¼ ç»Ÿhookæ–¹æ³•...")
        return self._fallback_gradcam(input_tensor, model_type)
    
    def _fallback_gradcam(self, input_tensor, model_type):
        """å¢å¼ºç‰ˆGrad-CAMä¿®å¤æ–¹æ³• - è§£å†³è“è‰²å›¾åƒé—®é¢˜"""
        print("ğŸ”§ ä½¿ç”¨å¢å¼ºç‰ˆGrad-CAMä¿®å¤æ–¹æ³•...")
        
        # é‡ç½®è¾“å…¥
        input_tensor = input_tensor.clone().detach().requires_grad_(True)
        
        # å®Œå…¨ä½¿ç”¨evalæ¨¡å¼ç¡®ä¿é¢„æµ‹å‡†ç¡®
        self.model.eval()
        for param in self.model.parameters():
            param.requires_grad_(True)
        
        # ğŸ¯ å…³é”®ä¿®å¤1: é€‰æ‹©æ›´å¥½çš„ç›®æ ‡å±‚
        if model_type == 'resnet50' and hasattr(self.model, 'backbone'):
            # å°è¯•ä½¿ç”¨layer3è€Œä¸æ˜¯layer4ï¼ˆæ›´æ—©çš„å±‚ï¼Œæ›´å¥½çš„ç©ºé—´åˆ†è¾¨ç‡ï¼‰
            layer3 = self.model.backbone.layer3
            if hasattr(layer3, '__getitem__') and len(layer3) > 0:
                last_block_3 = layer3[-1]
                if hasattr(last_block_3, 'conv2'):
                    target_layer = last_block_3.conv2
                    print(f"   ğŸ¯ ä½¿ç”¨layer3[-1].conv2ä½œä¸ºç›®æ ‡å±‚ï¼ˆæ›´å¥½çš„ç©ºé—´åˆ†è¾¨ç‡ï¼‰")
                else:
                    target_layer = last_block_3
                    print(f"   ğŸ¯ ä½¿ç”¨layer3[-1]ä½œä¸ºç›®æ ‡å±‚")
            else:
                target_layer = self.target_layer
                print(f"   ğŸ¯ ä½¿ç”¨åŸå§‹ç›®æ ‡å±‚")
        else:
            target_layer = self.target_layer
            print(f"   ğŸ¯ ä½¿ç”¨åŸå§‹ç›®æ ‡å±‚")
        
        # ç¡®ä¿ç›®æ ‡å±‚å‚æ•°éœ€è¦æ¢¯åº¦
        for param in target_layer.parameters():
            param.requires_grad_(True)
        
        print(f"   ğŸ“Š æ¨¡å‹æ¨¡å¼: {'è®­ç»ƒ' if self.model.training else 'è¯„ä¼°'} (æ¢¯åº¦å·²å¯ç”¨)")
        print(f"   ğŸ“Š ç›®æ ‡å±‚å‚æ•°éœ€è¦æ¢¯åº¦: {next(target_layer.parameters()).requires_grad}")
        
        # Hookå˜é‡
        activations = None
        gradients = None
        
        def save_activation(module, input, output):
            nonlocal activations
            activations = output
            print(f"   âœ“ æ¿€æ´»æ•è·æˆåŠŸ: {output.shape}, requires_grad: {output.requires_grad}")
        
        def save_gradient(module, grad_input, grad_output):
            nonlocal gradients
            if grad_output[0] is not None:
                gradients = grad_output[0]
                print(f"   âœ“ æ¢¯åº¦æ•è·æˆåŠŸ: {grad_output[0].shape}")
            else:
                print("   âŒ æ¢¯åº¦ä¸ºNone")
        
        # æ³¨å†Œhooks
        h1 = target_layer.register_forward_hook(save_activation)
        h2 = target_layer.register_backward_hook(save_gradient)
        
        try:
            # å‰å‘ä¼ æ’­ï¼ˆevalæ¨¡å¼ï¼Œé¢„æµ‹å‡†ç¡®ï¼‰
            if model_type in ['resnet50', 'vgg']:
                output = self.model(input_tensor)
            elif model_type.startswith('enhanced_laser_spot_cnn'):
                output, _ = self.model(input_tensor)
            else:
                output, _ = self.model(input_tensor)
            
            print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ: è¾“å‡ºå½¢çŠ¶ {output.shape}")
            print(f"   âœ… evalæ¨¡å¼å‡†ç¡®é¢„æµ‹: {output.item():.6f}")
            
            # ğŸ¯ å…³é”®ä¿®å¤2: ä½¿ç”¨å¹³æ–¹è¾“å‡ºå¢å¼ºæ¢¯åº¦
            target = (output ** 2).sum()
            print(f"   ğŸ”§ ä½¿ç”¨å¹³æ–¹è¾“å‡ºå¢å¼ºæ¢¯åº¦: {target.item():.6f}")
            
            # æ¸…é›¶æ¢¯åº¦
            self.model.zero_grad()
            
            # åå‘ä¼ æ’­
            target.backward(retain_graph=False)
            print("   âœ“ åå‘ä¼ æ’­å®Œæˆ")
            
            # ç§»é™¤hooks
            h1.remove()
            h2.remove()
            
            # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æ¿€æ´»å’Œæ¢¯åº¦
            if activations is None:
                print("   âŒ æœªè·å–åˆ°æ¿€æ´»å€¼")
                return None
                
            if gradients is None:
                print("   âŒ æœªè·å–åˆ°æ¢¯åº¦")
                return None
            
            print(f"   âœ“ æ¿€æ´»å½¢çŠ¶: {activations.shape}")
            print(f"   âœ“ æ¢¯åº¦å½¢çŠ¶: {gradients.shape}")
            print(f"   âœ“ æ¢¯åº¦ç»Ÿè®¡: min={gradients.min():.6f}, max={gradients.max():.6f}, mean={gradients.mean():.6f}")
            
            # ğŸ¯ å…³é”®ä¿®å¤3: å¢å¼ºæƒé‡è®¡ç®—
            weights = torch.mean(gradients, dim=[2, 3], keepdim=True)
            # æ”¾å¤§æƒé‡ä»¥å¢å¼ºCAMå¯¹æ¯”åº¦
            weights = weights * 3.0
            print(f"   âœ“ å¢å¼ºæƒé‡å½¢çŠ¶: {weights.shape}, æ”¾å¤§ç³»æ•°: 3.0")
            
            # åŠ æƒæ±‚å’Œå¾—åˆ°CAM
            cam = torch.sum(weights * activations, dim=1, keepdim=True)
            
            # åº”ç”¨ReLU
            cam = torch.relu(cam)
            print(f"   âœ“ CAMå½¢çŠ¶: {cam.shape}")
            
            # ğŸ¯ å…³é”®ä¿®å¤4: æ”¹è¿›å½’ä¸€åŒ–å’Œå¯¹æ¯”åº¦å¢å¼º
            cam_min = torch.min(cam)
            cam_max = torch.max(cam)
            
            if cam_max > cam_min:
                cam_normalized = (cam - cam_min) / (cam_max - cam_min + 1e-8)
                # ä¼½é©¬æ ¡æ­£å¢å¼ºå¯¹æ¯”åº¦
                cam_normalized = torch.pow(cam_normalized, 0.6)
                print(f"   âœ“ åº”ç”¨ä¼½é©¬æ ¡æ­£ (Î³=0.6) å¢å¼ºå¯¹æ¯”åº¦")
            else:
                print("   âš ï¸ CAMå€¼ä¸ºå¸¸æ•°ï¼Œå°è¯•ä½¿ç”¨åŸå§‹æ¿€æ´»å€¼")
                # å¦‚æœCAMå€¼ä¸ºå¸¸æ•°ï¼Œå°è¯•ä½¿ç”¨åŸå§‹æ¿€æ´»å€¼
                cam_normalized = torch.mean(activations, dim=1, keepdim=True)
                cam_normalized = torch.relu(cam_normalized)
                cam_min = torch.min(cam_normalized)
                cam_max = torch.max(cam_normalized)
                if cam_max > cam_min:
                    cam_normalized = (cam_normalized - cam_min) / (cam_max - cam_min + 1e-8)
                else:
                    cam_normalized = torch.ones_like(cam_normalized) * 0.5  # å‡åŒ€åˆ†å¸ƒ
            
            print(f"   âœ“ ä¿®å¤åCAMèŒƒå›´: {cam_normalized.min():.6f} - {cam_normalized.max():.6f}")
            
            # è½¬æ¢ä¸ºnumpy
            result = cam_normalized.squeeze().detach().cpu().numpy()
            print(f"   ğŸ‰ å¢å¼ºç‰ˆGradCAMä¿®å¤æˆåŠŸ! ç»“æœå½¢çŠ¶: {result.shape}")
            
            return result
            
        except Exception as e:
            # ç¡®ä¿hooksè¢«ç§»é™¤
            try:
                h1.remove()
                h2.remove()
            except:
                pass
            print(f"   âŒ GradCAMè®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def remove_hooks(self):
        # å…¼å®¹æ€§æ–¹æ³•ï¼Œæ— hookç‰ˆæœ¬ä¸éœ€è¦æ¸…ç†
        pass

def detect_model_type(model_path):
    """æ£€æµ‹æ¨¡å‹ç±»å‹ - æ”¯æŒå¢å¼ºæ¿€å…‰å…‰æ–‘CNNã€ResNet50ã€VGGã€å¢å¼ºCNNç­‰"""
    checkpoint = torch.load(model_path, map_location='cpu')
    
    # å¤„ç†æ£€æŸ¥ç‚¹æ ¼å¼ï¼šå¯èƒ½æ˜¯ç›´æ¥çš„state_dictï¼Œä¹Ÿå¯èƒ½åŒ…è£…åœ¨checkpointä¸­
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
        print("ğŸ” æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹æ ¼å¼ï¼Œæå–model_state_dict")
    else:
        state_dict = checkpoint
        print("ğŸ” æ£€æµ‹åˆ°ç›´æ¥state_dictæ ¼å¼")
    
    # è·å–æ‰€æœ‰å±‚åç”¨äºè°ƒè¯•
    layer_names = list(state_dict.keys())
    print(f"ğŸ” æ¨¡å‹å±‚åç¤ºä¾‹: {layer_names[:5]}...")
    
    # æ£€æŸ¥å¢å¼ºæ¿€å…‰å…‰æ–‘CNNï¼ˆtrain_enhanced_cnn_with_6modes.pyè®­ç»ƒçš„æ¨¡å‹ï¼‰
    # æ›´å¼ºçš„æ£€æµ‹é€»è¾‘ï¼šæ£€æŸ¥å…³é”®ç»„ä»¶
    enhanced_cnn_indicators = [
        'enhanced_cbam',
        'multi_scale_fusion', 
        'fusion_conv',
        'laser_constraint',
        'regressor.0.weight'  # å¢å¼ºCNNçš„å›å½’å¤´
    ]
    
    if any(any(indicator in name for name in layer_names) for indicator in enhanced_cnn_indicators):
        # è¿›ä¸€æ­¥æ£€æŸ¥éª¨å¹²ç½‘ç»œç±»å‹
        if any('backbone.features' in name for name in layer_names):
            print("âœ… æ£€æµ‹åˆ°å¢å¼ºæ¿€å…‰å…‰æ–‘CNNæ¨¡å‹ (VGGéª¨å¹²)")
            return 'enhanced_laser_spot_cnn_vgg'
        elif any('backbone.layer' in name for name in layer_names):
            print("âœ… æ£€æµ‹åˆ°å¢å¼ºæ¿€å…‰å…‰æ–‘CNNæ¨¡å‹ (ResNet50éª¨å¹²)")
            return 'enhanced_laser_spot_cnn_resnet'
        else:
            print("âœ… æ£€æµ‹åˆ°å¢å¼ºæ¿€å…‰å…‰æ–‘CNNæ¨¡å‹ (VGGéª¨å¹²-é»˜è®¤)")
            return 'enhanced_laser_spot_cnn_vgg'  # é»˜è®¤ä¸ºVGG
    
    # æ£€æŸ¥ResNet50å›å½’æ¨¡å‹
    elif any('backbone.layer4' in name or 'regressor.0.weight' in name for name in layer_names):
        print("âœ… æ£€æµ‹åˆ°ResNet50å›å½’æ¨¡å‹")
        return 'resnet50'
    elif 'backbone.conv1.weight' in state_dict and 'regressor' in str(layer_names):
        print("âœ… æ£€æµ‹åˆ°ResNet50å›å½’æ¨¡å‹")
        return 'resnet50'
    
    # æ£€æŸ¥VGGå›å½’æ¨¡å‹
    elif any('features.0.weight' in name or 'reg_head' in name or 'cbam' in name for name in layer_names):
        print("âœ… æ£€æµ‹åˆ°VGGå›å½’æ¨¡å‹")
        return 'vgg'
    elif 'features.0.weight' in state_dict and ('reg_head.0.weight' in state_dict or 'classifier.0.weight' in state_dict):
        print("âœ… æ£€æµ‹åˆ°VGGå›å½’æ¨¡å‹")
        return 'vgg'
    
    # æ£€æŸ¥ä¼ ç»Ÿå¢å¼ºCNNæ¨¡å‹ï¼ˆadvanced_cnn_models.pyï¼‰
    elif 'layer1.0.conv1.weight' in state_dict or 'classifier.0.weight' in state_dict:
        print("âœ… æ£€æµ‹åˆ°ä¼ ç»Ÿå¢å¼ºCNNæ¨¡å‹")
        return 'advanced_cnn'
        
    # æ£€æŸ¥ä¼ ç»ŸCNNæ¨¡å‹
    elif 'conv1.weight' in state_dict and 'fc2.weight' in state_dict:
        print("âœ… æ£€æµ‹åˆ°ä¼ ç»ŸCNNæ¨¡å‹")
        return 'traditional_cnn'
    
    else:
        print("âš ï¸  æ— æ³•ç¡®å®šæ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨ä¼ ç»ŸCNN")
        print(f"ğŸ” è¯·æ£€æŸ¥è¿™äº›å±‚å: {layer_names[:10]}")
        return 'traditional_cnn'

def load_model_safe(model_path, device):
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹ - æ”¯æŒResNet50ã€VGGã€å¢å¼ºCNNç­‰"""
    print(f"ğŸ” æ£€æµ‹æ¨¡å‹: {model_path}")
    
    # æ£€æµ‹æ¨¡å‹ç±»å‹
    model_type = detect_model_type(model_path)
    print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {model_type}")
    
    # æ ¹æ®ç±»å‹åˆ›å»ºç›¸åº”çš„æ¨¡å‹
    if model_type.startswith('enhanced_laser_spot_cnn'):
        print("ğŸš€ åˆ›å»ºå¢å¼ºæ¿€å…‰å…‰æ–‘CNNæ¨¡å‹...")
        # å¯¼å…¥å¢å¼ºæ¿€å…‰å…‰æ–‘CNNæ¨¡å‹
        try:
            sys.path.append('src')
            from enhanced_laser_spot_cnn import create_enhanced_laser_spot_model
            
            # æ ¹æ®æ£€æµ‹åˆ°çš„éª¨å¹²ç½‘ç»œç±»å‹é€‰æ‹©
            if model_type == 'enhanced_laser_spot_cnn_vgg':
                backbone = 'vgg16'
            elif model_type == 'enhanced_laser_spot_cnn_resnet':
                backbone = 'resnet50'
            else:
                backbone = 'vgg16'  # é»˜è®¤
            
            model = create_enhanced_laser_spot_model(
                backbone=backbone,
                constraint_weight=0.1,
                freeze_backbone=True
            ).to(device)
            
            print(f"   éª¨å¹²ç½‘ç»œ: {backbone}")
            
        except ImportError as e:
            print(f"âš ï¸ æ— æ³•å¯¼å…¥å¢å¼ºæ¿€å…‰å…‰æ–‘CNNæ¨¡å‹: {e}")
            print("   ä½¿ç”¨ä¼ ç»ŸCNNä½œä¸ºå¤‡é€‰...")
            model = CNNFeatureExtractor().to(device)
            model_type = 'traditional_cnn'
            
    elif model_type == 'resnet50':
        print("ğŸš€ åˆ›å»ºResNet50å›å½’æ¨¡å‹...")
        # å¯¼å…¥ResNet50æ¨¡å‹
        try:
            sys.path.append('cloud_vgg_training_package/src')
            from resnet_regression import ResNet50Regression
            model = ResNet50Regression(freeze_backbone=False).to(device)
        except ImportError:
            print("âš ï¸ æ— æ³•å¯¼å…¥ResNet50æ¨¡å‹ï¼Œå°è¯•ä»torchvisionåˆ›å»º...")
            from torchvision import models
            import torch.nn as nn
            backbone = models.resnet50(weights='IMAGENET1K_V2')
            backbone.fc = nn.Identity()
            model = nn.Sequential(
                backbone,
                nn.Sequential(
                    nn.Dropout(0.5),
                    nn.Linear(2048, 512),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.3),
                    nn.Linear(512, 128),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.15),
                    nn.Linear(128, 1)
                )
            ).to(device)
            
    elif model_type == 'vgg':
        print("ğŸš€ åˆ›å»ºVGGå›å½’æ¨¡å‹...")
        # å¯¼å…¥VGGæ¨¡å‹
        try:
            sys.path.append('cloud_vgg_training_package/src')
            from vgg_regression import VGGRegressionCBAM
            model = VGGRegressionCBAM(freeze_features=True).to(device)
        except ImportError:
            print("âš ï¸ æ— æ³•å¯¼å…¥VGGæ¨¡å‹ï¼Œå°è¯•ä»srcç›®å½•åˆ›å»º...")
            try:
                sys.path.append('src')
                # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–VGGæ¨¡å‹çš„å¯¼å…¥é€»è¾‘
                from vgg_regression import VGGRegressionCBAM
                model = VGGRegressionCBAM(freeze_features=True).to(device)
            except ImportError:
                print("âŒ æ— æ³•åˆ›å»ºVGGæ¨¡å‹ï¼Œä½¿ç”¨ä¼ ç»ŸCNN")
                model = CNNFeatureExtractor().to(device)
                model_type = 'traditional_cnn'
            
    elif model_type == 'advanced_cnn':
        print("ğŸš€ åˆ›å»ºé«˜çº§CNNæ¨¡å‹...")
        model = AdvancedLaserSpotCNN(
            num_features=512,
            use_attention=True,
            use_multiscale=True,
            use_laser_attention=False  # ç‰¹å¾æ•°æ®é›†ä¸Šç¦ç”¨æ¿€å…‰å…‰æ–‘ä¸“ç”¨æ³¨æ„åŠ›
        ).to(device)
    else:
        print("ğŸ”§ åˆ›å»ºä¼ ç»ŸCNNæ¨¡å‹...")
        model = CNNFeatureExtractor().to(device)
    
    # åŠ è½½æƒé‡ - æ”¹è¿›çš„åŠ è½½é€»è¾‘
    checkpoint = torch.load(model_path, map_location=device)
    
    # å¤„ç†æ£€æŸ¥ç‚¹æ ¼å¼
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
        print("ğŸ“‚ ä»æ£€æŸ¥ç‚¹ä¸­æå–model_state_dict")
    else:
        state_dict = checkpoint
        print("ğŸ“‚ ç›´æ¥ä½¿ç”¨state_dict")
    
    # å¯¹äºå¢å¼ºæ¿€å…‰å…‰æ–‘CNNï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŠ è½½ç­–ç•¥
    if model_type.startswith('enhanced_laser_spot_cnn'):
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥åŠ è½½ï¼ˆstrict=Falseå…è®¸éƒ¨åˆ†åŒ¹é…ï¼‰
            model.load_state_dict(state_dict, strict=False)
            print("âœ… å¢å¼ºCNNæ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼ˆstrict=Falseï¼‰")
            
            # è®¡ç®—åŒ¹é…ç‡
            model_dict = model.state_dict()
            matched_keys = [k for k in state_dict.keys() if k in model_dict and 
                          model_dict[k].shape == state_dict[k].shape]
            match_rate = len(matched_keys) / len(state_dict) * 100
            
        except Exception as e:
            print(f"âš ï¸ ç›´æ¥åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•è¿‡æ»¤åŠ è½½...")
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šè¿‡æ»¤åŠ è½½
            model_dict = model.state_dict()
            filtered_dict = {k: v for k, v in state_dict.items() 
                           if k in model_dict and model_dict[k].shape == v.shape}
            
            model_dict.update(filtered_dict)
            model.load_state_dict(model_dict)
            match_rate = len(filtered_dict) / len(state_dict) * 100
            
    else:
        # å…¶ä»–æ¨¡å‹ç±»å‹ä½¿ç”¨åŸæ¥çš„è¿‡æ»¤åŠ è½½æ–¹å¼
        model_dict = model.state_dict()
        filtered_dict = {k: v for k, v in state_dict.items() 
                        if k in model_dict and model_dict[k].shape == v.shape}
        
        model_dict.update(filtered_dict)
        model.load_state_dict(model_dict)
        match_rate = len(filtered_dict) / len(state_dict) * 100
    
    # ä¸è®¾ç½®model.eval()ï¼Œè®©GradCAMè‡ªå·±æ§åˆ¶æ¨¡å‹æ¨¡å¼
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: æƒé‡åŒ¹é…ç‡ {match_rate:.1f}%")
    
    if match_rate < 50:
        print("âš ï¸  è­¦å‘Š: æƒé‡åŒ¹é…ç‡è¾ƒä½ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½")
    elif match_rate > 90:
        print("ğŸ‰ æƒé‡åŒ¹é…ç‡å¾ˆé«˜ï¼Œæ¨¡å‹åº”è¯¥å·¥ä½œæ­£å¸¸")
    
    return model, model_type

def detect_available_models():
    """æ£€æµ‹å¯ç”¨çš„å·²è®­ç»ƒæ¨¡å‹"""
    models = {}
    model_paths = {
        'bg0': [
            'best_model_bg0.pth',
            'baseline_cnn_bg0_results_20250604_174456/best_model.pth',
            'baseline_cnn_checkpoint_resume/best_model.pth',
            'models/best_model_bg0.pth'
        ],
        'bg1': [
            'best_model_bg1.pth',
            'models/best_model_bg1.pth'
        ],
        'all': [
            'best_model.pth',
            'models/best_model.pth'
        ]
    }
    
    for model_type, paths in model_paths.items():
        for path in paths:
            if os.path.exists(path):
                models[model_type] = path
                print(f"âœ… å‘ç°{model_type}æ¨¡å‹: {path}")
                break
    
    return models

def find_feature_dataset_images(feature_dataset_path=None, bg_mode=None, power_filter=None):
    """æŸ¥æ‰¾ç‰¹å¾æ•°æ®é›†ä¸­çš„å›¾åƒï¼Œæ”¯æŒ6æ¡£ç»†åˆ†æ¨¡å¼è¿‡æ»¤"""
    if feature_dataset_path is None:
        # è‡ªåŠ¨æ£€æµ‹ç‰¹å¾æ•°æ®é›† - å¢å¼ºäº‘ç«¯å…¼å®¹æ€§
        print("ğŸ” è‡ªåŠ¨æ£€æµ‹ç‰¹å¾æ•°æ®é›†...")
        
        # å€™é€‰ç›®å½•åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        candidate_dirs = []
        
        # 1. å¸¦æ—¶é—´æˆ³çš„ç‰¹å¾æ•°æ®é›†ï¼ˆæœ¬åœ°å¸¸è§æ ¼å¼ï¼‰
        timestamped_datasets = glob.glob("feature_dataset_*")
        candidate_dirs.extend(timestamped_datasets)
        
        # 2. ç®€å•çš„feature_datasetç›®å½•ï¼ˆäº‘ç«¯å¸¸è§æ ¼å¼ï¼‰
        if os.path.exists("feature_dataset") and os.path.isdir("feature_dataset"):
            candidate_dirs.append("feature_dataset")
            
        # 3. å…¶ä»–å¯èƒ½çš„å˜ä½“
        for variant in ["Feature_Dataset", "FEATURE_DATASET", "feature-dataset"]:
            if os.path.exists(variant) and os.path.isdir(variant):
                candidate_dirs.append(variant)
        
        print(f"   æ‰¾åˆ°å€™é€‰ç›®å½•: {candidate_dirs}")
        
        if not candidate_dirs:
            print("âŒ æœªæ‰¾åˆ°ç‰¹å¾æ•°æ®é›†ç›®å½•")
            print("   æ”¯æŒçš„ç›®å½•æ ¼å¼: feature_dataset, feature_dataset_*, Feature_Dataset, feature-dataset")
            return []
        
        # é€‰æ‹©æœ€ä½³å€™é€‰ç›®å½•ï¼ˆä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆçš„ç›®å½•ï¼‰
        valid_datasets = []
        for dataset_dir in candidate_dirs:
            images_dir = os.path.join(dataset_dir, "images")
            info_dir = os.path.join(dataset_dir, "original_info")
            
            if os.path.exists(images_dir) and os.path.exists(info_dir):
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
                image_count = len(glob.glob(os.path.join(images_dir, "*.jpg"))) + \
                             len(glob.glob(os.path.join(images_dir, "*.png"))) + \
                             len(glob.glob(os.path.join(images_dir, "*.jpeg")))
                info_count = len(glob.glob(os.path.join(info_dir, "*.json")))
                
                if image_count > 0 and info_count > 0:
                    valid_datasets.append({
                        'path': dataset_dir,
                        'image_count': image_count,
                        'info_count': info_count,
                        'mtime': os.path.getmtime(dataset_dir)
                    })
                    print(f"   âœ… æœ‰æ•ˆæ•°æ®é›†: {dataset_dir} (å›¾åƒ: {image_count}, ä¿¡æ¯: {info_count})")
                else:
                    print(f"   âš ï¸ ç©ºæ•°æ®é›†: {dataset_dir} (å›¾åƒ: {image_count}, ä¿¡æ¯: {info_count})")
            else:
                print(f"   âŒ æ— æ•ˆç»“æ„: {dataset_dir}")
        
        if not valid_datasets:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾æ•°æ®é›†")
            return []
        
        # é€‰æ‹©æœ€æ–°çš„æœ‰æ•ˆæ•°æ®é›†
        feature_dataset_path = max(valid_datasets, key=lambda x: x['mtime'])['path']
        print(f"ğŸ¯ é€‰æ‹©æ•°æ®é›†: {feature_dataset_path}")
    
    # éªŒè¯é€‰å®šçš„æ•°æ®é›†ç»“æ„
    
    images_dir = os.path.join(feature_dataset_path, "images")
    info_dir = os.path.join(feature_dataset_path, "original_info")
    
    if not os.path.exists(images_dir):
        print(f"âŒ ç‰¹å¾æ•°æ®é›†å›¾åƒç›®å½•ä¸å­˜åœ¨: {images_dir}")
        return []
    
    if not os.path.exists(info_dir):
        print(f"âŒ ç‰¹å¾æ•°æ®é›†ä¿¡æ¯ç›®å½•ä¸å­˜åœ¨: {info_dir}")
        return []
    
    print(f"ğŸ” æ‰«æç‰¹å¾æ•°æ®é›†: {feature_dataset_path}")
    print(f"   å›¾åƒç›®å½•: {images_dir}")
    print(f"   ä¿¡æ¯ç›®å½•: {info_dir}")
    
    # æ˜¾ç¤ºè¿‡æ»¤æ¡ä»¶
    if bg_mode:
        print(f"   èƒŒæ™¯è¿‡æ»¤: {bg_mode}")
    if power_filter:
        print(f"   åŠŸç‡è¿‡æ»¤: {power_filter}")
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png']:
        image_files.extend(glob.glob(os.path.join(images_dir, ext)))
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(image_files)} ä¸ªç‰¹å¾å›¾åƒæ–‡ä»¶")
    
    # éšæœºæ‰“ä¹±å›¾åƒæ–‡ä»¶åˆ—è¡¨ï¼Œç¡®ä¿é‡‡æ ·çš„å¤šæ ·æ€§
    import random
    import time
    random.seed(int(time.time()))  # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºéšæœºç§å­
    random.shuffle(image_files)
    
    found_images = []
    valid_count = 0
    filtered_count = 0
    
    # å¢åŠ å¤„ç†æ•°é‡åˆ°5000ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬å¤šæ ·æ€§
    sample_size = min(5000, len(image_files))
    print(f"ğŸ² éšæœºé‡‡æ · {sample_size} ä¸ªæ–‡ä»¶è¿›è¡Œè§£æ")
    
    for img_path in image_files[:sample_size]:
        basename = os.path.basename(img_path)
        info_path = os.path.join(info_dir, basename.replace('.jpg', '.json').replace('.png', '.json'))
        
        if os.path.exists(info_path):
            try:
                with open(info_path, 'r', encoding='utf-8') as f:
                    info_data = json.load(f)
                
                concentration = info_data.get('concentration')
                bg_type = info_data.get('bg_type', 'unknown')
                power_value = info_data.get('power', info_data.get('power_value', 'unknown'))
                
                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if concentration is not None:
                    # èƒŒæ™¯ç±»å‹è¿‡æ»¤
                    if bg_mode and bg_type != bg_mode:
                        filtered_count += 1
                        continue
                    
                    # åŠŸç‡è¿‡æ»¤
                    if power_filter and power_value != power_filter:
                        filtered_count += 1
                        continue
                    
                    # æ„å»ºå®Œæ•´çš„æ¨¡å¼æè¿°
                    power_suffix = f"_{power_value}" if power_value != 'unknown' else ""
                    mode_description = f"{bg_type}{power_suffix}"
                    
                    found_images.append({
                        'path': img_path,
                        'concentration': float(concentration),
                        'bg_type': bg_type,
                        'power_value': power_value,
                        'filename': basename,
                        'description': f'{mode_description}-{concentration}',
                        'dataset_type': 'feature',
                        'info_data': info_data,
                        'mode_description': mode_description
                    })
                    valid_count += 1
                    
            except Exception as e:
                continue
    
    # æŒ‰mode_descriptionå’Œæµ“åº¦æ’åºï¼ˆä½†åœ¨å‡½æ•°å¤–éƒ¨ä¼šæ ¹æ®random_selectå‚æ•°å†³å®šæ˜¯å¦ä¿æŒéšæœºé¡ºåºï¼‰
    found_images.sort(key=lambda x: (x['mode_description'], x['concentration']))
    
    print(f"âœ… æˆåŠŸè§£æ {len(found_images)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ ·æœ¬ (è¿‡æ»¤æ‰ {filtered_count} ä¸ª)")
    
    # 6æ¡£ç»†åˆ†ç»Ÿè®¡
    mode_stats = {}
    for img in found_images:
        mode = img['mode_description']
        if mode not in mode_stats:
            mode_stats[mode] = 0
        mode_stats[mode] += 1
    
    print(f"ğŸ“Š 6æ¡£ç»†åˆ†ç»Ÿè®¡:")
    for mode, count in sorted(mode_stats.items()):
        print(f"   {mode}: {count} å¼ ")
    
    return found_images

def get_transform_for_dataset():
    """è·å–å›¾åƒé¢„å¤„ç†å˜æ¢"""
    return transforms.Compose([
        transforms.Resize((224, 224)),  # ç»Ÿä¸€è°ƒæ•´åˆ°224x224
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

def load_and_preprocess_image(image_path, transform):
    """åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ"""
    try:
        image = Image.open(image_path).convert('RGB')
        
        # è®°å½•åŸå§‹å°ºå¯¸
        original_size = image.size
        
        # åº”ç”¨å˜æ¢
        tensor = transform(image)
        
        print(f"    åŸå§‹å°ºå¯¸: {original_size}, å¤„ç†å: {tensor.shape}")
        
        return tensor, True, original_size
    except Exception as e:
        print(f"Failed to load image {image_path}: {e}")
        return None, False, None

def get_gradcam_target_layer(model, model_type):
    """æ ¹æ®æ¨¡å‹ç±»å‹è·å–Grad-CAMç›®æ ‡å±‚ - æ”¯æŒå¢å¼ºæ¿€å…‰å…‰æ–‘CNNã€ResNet50ã€VGGã€å¢å¼ºCNNç­‰"""
    if model_type.startswith('enhanced_laser_spot_cnn'):
        # å¢å¼ºæ¿€å…‰å…‰æ–‘CNNçš„æœ€ä½³ç›®æ ‡å±‚é€‰æ‹©ç­–ç•¥
        print("ğŸ” ä¸ºå¢å¼ºæ¿€å…‰å…‰æ–‘CNNé€‰æ‹©æœ€ä½³Grad-CAMç›®æ ‡å±‚...")
        
        # ä¼˜å…ˆçº§1: èåˆå·ç§¯å±‚ï¼ˆæœ€èƒ½ä½“ç°å¤šå°ºåº¦ç‰¹å¾èåˆæ•ˆæœï¼‰
        if hasattr(model, 'fusion_conv'):
            print(f"ğŸ¯ å¢å¼ºæ¿€å…‰å…‰æ–‘CNNç›®æ ‡å±‚: fusion_conv (æœ€ä½³é€‰æ‹©)")
            return model.fusion_conv
            
        # ä¼˜å…ˆçº§2: å¢å¼ºCBAMçš„ç©ºé—´æ³¨æ„åŠ›å±‚
        elif hasattr(model, 'enhanced_cbam') and hasattr(model.enhanced_cbam, 'spatial_attention'):
            print(f"ğŸ¯ å¢å¼ºæ¿€å…‰å…‰æ–‘CNNç›®æ ‡å±‚: enhanced_cbam.spatial_attention")
            return model.enhanced_cbam.spatial_attention
            
        # ä¼˜å…ˆçº§3: å¤šå°ºåº¦èåˆå±‚ä¸­çš„æœ€åä¸€ä¸ª
        elif hasattr(model, 'multi_scale_fusion') and len(model.multi_scale_fusion) > 0:
            target_layer = model.multi_scale_fusion[-1]  # 5x5å·ç§¯å±‚
            print(f"ğŸ¯ å¢å¼ºæ¿€å…‰å…‰æ–‘CNNç›®æ ‡å±‚: multi_scale_fusion[-1] (5x5å·ç§¯)")
            return target_layer
            
        # ä¼˜å…ˆçº§4: backboneçš„æœ€åä¸€å±‚
        elif hasattr(model, 'backbone'):
            if model_type == 'enhanced_laser_spot_cnn_vgg':
                # VGGéª¨å¹²ï¼šä½¿ç”¨featuresçš„æœ€åä¸€ä¸ªå·ç§¯å±‚
                for i in range(len(model.backbone) - 1, -1, -1):
                    if isinstance(model.backbone[i], nn.Conv2d):
                        print(f"ğŸ¯ å¢å¼ºæ¿€å…‰å…‰æ–‘CNNç›®æ ‡å±‚: backbone[{i}] (VGG Conv2d)")
                        return model.backbone[i]
            elif model_type == 'enhanced_laser_spot_cnn_resnet':
                # ResNet50éª¨å¹²ï¼šä½¿ç”¨layer4
                if hasattr(model.backbone, 'layer4'):
                    print(f"ğŸ¯ å¢å¼ºæ¿€å…‰å…‰æ–‘CNNç›®æ ‡å±‚: backbone.layer4")
                    return model.backbone.layer4
                else:
                    # SequentialåŒ…è£…çš„ResNet50
                    for name, layer in model.backbone.named_children():
                        if 'layer4' in name:
                            print(f"ğŸ¯ å¢å¼ºæ¿€å…‰å…‰æ–‘CNNç›®æ ‡å±‚: backbone.{name}")
                            return layer
            # é€šç”¨å¤‡é€‰ï¼šbackboneæœ€åä¸€å±‚
            backbone_layers = list(model.backbone.children())
            if backbone_layers:
                print(f"ğŸ¯ å¢å¼ºæ¿€å…‰å…‰æ–‘CNNç›®æ ‡å±‚: backboneæœ€åä¸€å±‚")
                return backbone_layers[-1]
        else:
            print("âš ï¸ å¢å¼ºæ¿€å…‰å…‰æ–‘CNNç»“æ„æœªçŸ¥ï¼Œä½¿ç”¨æ¨¡å‹å€’æ•°ç¬¬äºŒå±‚")
            model_children = list(model.children())
            if len(model_children) >= 2:
                return model_children[-2]
            else:
                return model_children[-1] if model_children else None
            
    elif model_type == 'resnet50':
        # ResNet50ä½¿ç”¨backboneçš„æœ€åä¸€ä¸ªå·ç§¯å±‚
        if hasattr(model, 'backbone'):
            # è‡ªå®šä¹‰ResNet50æ¨¡å‹ - é€‰æ‹©layer4ä¸­çš„æœ€åä¸€ä¸ªå·ç§¯å±‚
            layer4 = model.backbone.layer4
            if hasattr(layer4, '__getitem__') and len(layer4) > 0:
                # layer4æ˜¯Sequentialï¼Œé€‰æ‹©æœ€åä¸€ä¸ªBasicBlockæˆ–Bottleneck
                last_block = layer4[-1]
                # åœ¨æœ€åä¸€ä¸ªblockä¸­æ‰¾åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚
                if hasattr(last_block, 'conv2'):
                    print(f"ğŸ¯ ResNet50ç›®æ ‡å±‚: backbone.layer4[-1].conv2")
                    return last_block.conv2
                elif hasattr(last_block, 'conv3'):
                    print(f"ğŸ¯ ResNet50ç›®æ ‡å±‚: backbone.layer4[-1].conv3")
                    return last_block.conv3
                else:
                    print(f"ğŸ¯ ResNet50ç›®æ ‡å±‚: backbone.layer4[-1] (æ•´ä¸ªblock)")
                    return last_block
            else:
                print(f"ğŸ¯ ResNet50ç›®æ ‡å±‚: backbone.layer4")
                return layer4
        else:
            # Sequentialæ¨¡å‹ä¸­çš„ResNet50
            layer4 = model[0].layer4
            if hasattr(layer4, '__getitem__') and len(layer4) > 0:
                last_block = layer4[-1]
                if hasattr(last_block, 'conv2'):
                    print(f"ğŸ¯ ResNet50ç›®æ ‡å±‚: [0].layer4[-1].conv2")
                    return last_block.conv2
                elif hasattr(last_block, 'conv3'):
                    print(f"ğŸ¯ ResNet50ç›®æ ‡å±‚: [0].layer4[-1].conv3")
                    return last_block.conv3
                else:
                    return last_block
            else:
                return layer4
            
    elif model_type == 'vgg':
        # VGGæ¨¡å‹ä½¿ç”¨featuresçš„æœ€åå‡ å±‚
        if hasattr(model, 'features'):
            # æ‰¾åˆ°featuresä¸­çš„æœ€åä¸€ä¸ªå·ç§¯å±‚
            for i in range(len(model.features) - 1, -1, -1):
                if isinstance(model.features[i], nn.Conv2d):
                    print(f"ğŸ¯ VGGç›®æ ‡å±‚: features[{i}] (Conv2d)")
                    return model.features[i]
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨featuresçš„æœ€åä¸€å±‚
            print(f"ğŸ¯ VGGç›®æ ‡å±‚: features[-1] (fallback)")
            return model.features[-1]
        elif hasattr(model, 'backbone') and hasattr(model.backbone, 'features'):
            # å¸¦backboneçš„VGGæ¨¡å‹
            for i in range(len(model.backbone.features) - 1, -1, -1):
                if isinstance(model.backbone.features[i], nn.Conv2d):
                    print(f"ğŸ¯ VGGç›®æ ‡å±‚: backbone.features[{i}] (Conv2d)")
                    return model.backbone.features[i]
            print(f"ğŸ¯ VGGç›®æ ‡å±‚: backbone.features[-1] (fallback)")
            return model.backbone.features[-1]
        else:
            print("âš ï¸ VGGæ¨¡å‹ç»“æ„æœªçŸ¥ï¼Œä½¿ç”¨é»˜è®¤å±‚")
            model_children = list(model.children())
            if len(model_children) >= 2:
                return model_children[-2]
            else:
                return model_children[-1] if model_children else None
            
    elif model_type == 'advanced_cnn':
        # é«˜çº§CNNä½¿ç”¨layer4ï¼ˆæœ€åçš„æ®‹å·®å±‚ï¼‰
        return model.layer4
    else:
        # ä¼ ç»ŸCNNä½¿ç”¨conv5
        return model.conv5

def create_gradcam_visualization(model, model_name, image_info, device, model_type='traditional_cnn', use_chinese=True):
    """ä¸ºå•ä¸ªæ¨¡å‹å’Œå›¾åƒåˆ›å»ºGrad-CAMå¯è§†åŒ–ï¼Œæ”¯æŒ6æ¡£ç»†åˆ†ä¿¡æ¯"""
    image_path = image_info['path']
    concentration = image_info['concentration']
    bg_type = image_info['bg_type']
    power_value = image_info.get('power_value', 'unknown')
    description = image_info['description']
    filename = image_info['filename']
    dataset_type = image_info['dataset_type']
    mode_description = image_info.get('mode_description', f"{bg_type}_{power_value}")
    
    print(f"  Processing: {filename} ({description}) [{dataset_type}]")
    
    # è·å–å›¾åƒå˜æ¢
    transform = get_transform_for_dataset()
    
    # åŠ è½½å›¾åƒ
    img_tensor, success, original_size = load_and_preprocess_image(image_path, transform)
    if not success:
        return None
    
    input_tensor = img_tensor.unsqueeze(0).to(device)
    
    # è·å–é¢„æµ‹ - æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†è¾“å‡º
    with torch.no_grad():
        if model_type in ['resnet50', 'vgg']:
            # ResNet50å’ŒVGGç›´æ¥è¿”å›æµ“åº¦å€¼
            prediction = model(input_tensor)
        elif model_type.startswith('enhanced_laser_spot_cnn'):
            # å¢å¼ºæ¿€å…‰å…‰æ–‘CNNè¿”å›(æµ“åº¦, ç‰¹å¾)å…ƒç»„
            prediction, _ = model(input_tensor)
        else:
            # ä¼ ç»ŸCNNå’Œå¢å¼ºCNNè¿”å›(æµ“åº¦, ç‰¹å¾)å…ƒç»„
            prediction, _ = model(input_tensor)
        pred_value = prediction.item()
    
    print(f"    True: {concentration}, Predicted: {pred_value:.1f}, Error: {abs(concentration - pred_value):.1f}")
    print(f"    Mode: {mode_description}, Original size: {original_size}")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©Grad-CAMç›®æ ‡å±‚
    target_layer = get_gradcam_target_layer(model, model_type)
    print(f"    ğŸ¯ Grad-CAMç›®æ ‡å±‚: {target_layer.__class__.__name__}")
    
    # ç”ŸæˆGrad-CAMï¼ˆä½¿ç”¨æ— hookæ–¹æ³•é¿å…è§†å›¾ä¿®æ”¹é”™è¯¯ï¼‰
    print(f"    ğŸ”§ ä½¿ç”¨æ— hook GradCAMæ–¹æ³•ï¼ˆé¿å…è§†å›¾ä¿®æ”¹é”™è¯¯ï¼‰")
    grad_cam = GradCAM(model, target_layer)
    cam = grad_cam(input_tensor, model_type)
    
    # æ£€æŸ¥Grad-CAMæ˜¯å¦æˆåŠŸ
    if cam is None:
        print("    âŒ Grad-CAMç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
        return None
    
    # å¤„ç†å¯è§†åŒ–
    # å°†tensorè½¬æ¢ä¸ºnumpyç”¨äºæ˜¾ç¤ºï¼ˆéœ€è¦detachä»¥é¿å…æ¢¯åº¦é”™è¯¯ï¼‰
    img_np = img_tensor.permute(1, 2, 0).detach().cpu().numpy()
    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
    img_np = np.clip(img_np, 0, 1)
    
    # è°ƒæ•´CAMå°ºå¯¸åˆ°224x224
    cam_resized = cv2.resize(cam, (224, 224))
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    heatmap = cm.jet(cam_resized)[:, :, :3]
    
    # åˆ›å»ºå åŠ å›¾åƒ
    overlay = 0.6 * img_np + 0.4 * heatmap
    
    return {
        'img_np': img_np,
        'heatmap': heatmap,
        'overlay': overlay,
        'cam': cam_resized,
        'concentration': concentration,
        'pred_value': pred_value,
        'error': abs(concentration - pred_value),
        'filename': filename,
        'bg_type': bg_type,
        'power_value': power_value,
        'dataset_type': dataset_type,
        'original_size': original_size,
        'mode_description': mode_description
    }

def filter_images_by_mode(images, bg_mode=None, power_filter=None):
    """æ ¹æ®6æ¡£ç»†åˆ†æ¨¡å¼è¿‡æ»¤å›¾åƒ"""
    if not bg_mode and not power_filter:
        return images
    
    filtered_images = []
    for img in images:
        # æ£€æŸ¥èƒŒæ™¯ç±»å‹
        if bg_mode and img['bg_type'] != bg_mode:
            continue
        
        # æ£€æŸ¥åŠŸç‡çº§åˆ«
        if power_filter and img['power_value'] != power_filter:
            continue
        
        filtered_images.append(img)
    
    mode_desc = ""
    if bg_mode:
        mode_desc += bg_mode
    if power_filter:
        mode_desc += f"_{power_filter}"
    
    print(f"ğŸ” {mode_desc}æ¨¡å¼è¿‡æ»¤å: {len(filtered_images)}/{len(images)} å¼ å›¾åƒ")
    return filtered_images

def parse_bg_mode(bg_mode_str):
    """è§£æèƒŒæ™¯æ¨¡å¼å­—ç¬¦ä¸²ï¼Œæ”¯æŒ6æ¡£ç»†åˆ†"""
    if bg_mode_str == 'all':
        return None, None
    elif bg_mode_str in ['bg0', 'bg1']:
        return bg_mode_str, None
    elif '_' in bg_mode_str:
        # 6æ¡£ç»†åˆ†æ¨¡å¼ï¼Œå¦‚ bg0_20mw
        parts = bg_mode_str.split('_')
        if len(parts) == 2:
            bg_type = parts[0]
            power_level = parts[1]
            return bg_type, power_level
    
    print(f"âš ï¸ æœªè¯†åˆ«çš„èƒŒæ™¯æ¨¡å¼: {bg_mode_str}")
    return None, None

def save_individual_visualization(result, output_dir, sample_id, use_chinese=True):
    """ä¿å­˜å•ä¸ªæ ·æœ¬çš„å¯è§†åŒ–ï¼Œæ”¯æŒ6æ¡£ç»†åˆ†æ¨¡å¼æ˜¾ç¤º"""
    filename = result['filename'].replace('.jpg', '').replace('.jpeg', '').replace('.png', '')
    bg_type = result['bg_type']
    power_value = result.get('power_value', 'unknown')
    concentration = result['concentration']
    pred_value = result['pred_value']
    dataset_type = result['dataset_type']
    mode_description = result.get('mode_description', f"{bg_type}_{power_value}")
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    title = f'æ ·æœ¬{sample_id}: {filename} ({mode_description})[{dataset_type}]' if use_chinese else f'Sample{sample_id}: {filename} ({mode_description})[{dataset_type}]'
    fig.suptitle(title, fontsize=14)
    
    # åŸå§‹å›¾åƒ
    axes[0, 0].imshow(result['img_np'])
    axes[0, 0].set_title('åŸå§‹å›¾åƒ' if use_chinese else 'Original Image')
    axes[0, 0].axis('off')
    
    # çƒ­åŠ›å›¾
    axes[0, 1].imshow(result['heatmap'])
    axes[0, 1].set_title('Grad-CAMçƒ­åŠ›å›¾' if use_chinese else 'Grad-CAM Heatmap')
    axes[0, 1].axis('off')
    
    # å åŠ å›¾åƒ
    axes[1, 0].imshow(result['overlay'])
    axes[1, 0].set_title('å åŠ å¯è§†åŒ–' if use_chinese else 'Overlay Visualization')
    axes[1, 0].axis('off')
    
    # é¢„æµ‹ä¿¡æ¯ - æ›´æ–°ä»¥åŒ…å«åŠŸç‡ä¿¡æ¯
    if use_chinese:
        info_text = f"çœŸå®æµ“åº¦: {concentration:.1f}\né¢„æµ‹æµ“åº¦: {pred_value:.1f}\né¢„æµ‹è¯¯å·®: {result['error']:.1f}\nèƒŒæ™¯ç±»å‹: {bg_type}\næ¿€å…‰åŠŸç‡: {power_value}\nè®­ç»ƒæ¨¡å¼: {mode_description}\næ•°æ®é›†: {dataset_type}\nåŸå§‹å°ºå¯¸: {result['original_size']}"
    else:
        info_text = f"True Conc: {concentration:.1f}\nPred Conc: {pred_value:.1f}\nError: {result['error']:.1f}\nBG Type: {bg_type}\nPower: {power_value}\nMode: {mode_description}\nDataset: {dataset_type}\nOriginal: {result['original_size']}"
    
    axes[1, 1].text(0.1, 0.9, info_text, fontsize=10, verticalalignment='top')
    axes[1, 1].axis('off')
    
    # ä¿å­˜
    save_name = f'sample_{sample_id:02d}_{mode_description}_{dataset_type}_{filename}_true{concentration:.0f}_pred{pred_value:.0f}.png'
    save_path = os.path.join(output_dir, save_name)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"    âœ… ä¿å­˜: {save_name}")

def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆGrad-CAMå¯è§†åŒ–å·¥å…·ï¼ˆæ”¯æŒ6æ¡£ç»†åˆ†æ¨¡å¼ï¼‰')
    parser.add_argument('--model_path', type=str, required=True,
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰')
    parser.add_argument('--dataset', type=str, choices=['feature'],
                       default='feature', help='æ•°æ®é›†ç±»å‹ï¼ˆå½“å‰ä»…æ”¯æŒç‰¹å¾æ•°æ®é›†ï¼‰')
    parser.add_argument('--bg_mode', type=str, 
                       choices=['bg0', 'bg1', 'all', 'bg0_20mw', 'bg0_100mw', 'bg0_400mw', 
                               'bg1_20mw', 'bg1_100mw', 'bg1_400mw'],
                       default='bg0', help='èƒŒæ™¯æ¨¡å¼ï¼ˆæ”¯æŒ6æ¡£ç»†åˆ†ï¼‰')
    parser.add_argument('--max_samples', type=int, default=20,
                       help='æœ€å¤§æ ·æœ¬æ•°')
    parser.add_argument('--feature_dataset', type=str, default=None,
                       help='æŒ‡å®šç‰¹å¾æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--random_select', action='store_true',
                       help='éšæœºé€‰æ‹©æ ·æœ¬ï¼ˆè€ŒéæŒ‰æµ“åº¦é¡ºåºé€‰æ‹©ï¼‰')
    
    args = parser.parse_args()
    
    print("=== å¢å¼ºç‰ˆGrad-CAMå¯è§†åŒ–åˆ†æï¼ˆ6æ¡£ç»†åˆ†æ”¯æŒï¼‰===")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ•°æ®é›†ç±»å‹: {args.dataset}")
    print(f"èƒŒæ™¯æ¨¡å¼: {args.bg_mode}")
    print(f"æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
    print(f"éšæœºé€‰æ‹©: {'æ˜¯' if args.random_select else 'å¦ï¼ˆæŒ‰æµ“åº¦é¡ºåºï¼‰'}")
    
    # è§£æ6æ¡£ç»†åˆ†æ¨¡å¼
    bg_type, power_filter = parse_bg_mode(args.bg_mode)
    if bg_type:
        print(f"   èƒŒæ™¯ç±»å‹: {bg_type}")
    if power_filter:
        print(f"   åŠŸç‡çº§åˆ«: {power_filter}")
    if not bg_type and not power_filter:
        print(f"   å¤„ç†æ‰€æœ‰æ¨¡å¼çš„æ•°æ®")
    
    # éªŒè¯æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    chinese_font = setup_chinese_font()
    use_chinese = chinese_font is not None
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"è®¾å¤‡: {device}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model, model_type = load_model_safe(args.model_path, device)
        
        # ç”Ÿæˆå”¯ä¸€çš„æ¨¡å‹åç§°ï¼ˆåŒ…å«çˆ¶ç›®å½•ä¿¡æ¯ï¼‰
        model_path_obj = Path(args.model_path)
        model_filename = model_path_obj.stem  # ä¸å¸¦æ‰©å±•åçš„æ–‡ä»¶å
        parent_dir = model_path_obj.parent.name
        
        # å¦‚æœæ–‡ä»¶åæ˜¯é€šç”¨çš„ï¼ˆå¦‚best_modelï¼‰ï¼Œåˆ™ä½¿ç”¨çˆ¶ç›®å½•ä¿¡æ¯
        if model_filename.lower() in ['best_model', 'model', 'final_model']:
            # ä»çˆ¶ç›®å½•æå–æœ‰ç”¨ä¿¡æ¯
            if parent_dir and parent_dir != '.':
                # æå–æ—¶é—´æˆ³å’Œå…¶ä»–æ ‡è¯†ä¿¡æ¯
                import re
                # å°è¯•æå–æ—¶é—´æˆ³
                timestamp_match = re.search(r'(\d{8}_\d{6})', parent_dir)
                if timestamp_match:
                    model_name = f"{parent_dir}_{model_filename}"
                else:
                    model_name = f"{parent_dir}_{model_filename}"
            else:
                model_name = model_filename
        else:
            model_name = model_filename
            
        print(f"ğŸ“‹ ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")
        print(f"ğŸ“‹ æ¨¡å‹æ ‡è¯†: {model_name}")
        
        # æ‰«æç‰¹å¾æ•°æ®é›†
        print("\n=== æ‰«æç‰¹å¾æ•°æ®é›† ===")
        feature_images = find_feature_dataset_images(args.feature_dataset, bg_type, power_filter)
        
        if not feature_images:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å›¾åƒ")
            return
        
        # é€‰æ‹©æ ·æœ¬ - æ–°å¢éšæœºé€‰æ‹©é€»è¾‘
        if args.random_select:
            import random
            random.seed(42)  # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°
            # å…ˆéšæœºæ‰“ä¹±ï¼Œå†é€‰æ‹©å‰max_samplesä¸ª
            random.shuffle(feature_images)
            selected_images = feature_images[:args.max_samples]
            print(f"\nğŸ² éšæœºé€‰æ‹©å¤„ç† {len(selected_images)} å¼ å›¾åƒ")
        else:
            selected_images = feature_images[:args.max_samples]
            print(f"\nğŸ“Š æŒ‰é¡ºåºé€‰æ‹©å¤„ç† {len(selected_images)} å¼ å›¾åƒ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        suffix = 'random' if args.random_select else 'ordered'
        mode_suffix = args.bg_mode if args.bg_mode != 'all' else 'all'
        output_dir = f'enhanced_gradcam_analysis_{model_name}_{mode_suffix}_{args.dataset}_{suffix}'
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # å¤„ç†å›¾åƒ
        successful_visualizations = []
        
        for i, image_info in enumerate(selected_images, 1):
            print(f"\nğŸ“¸ æ ·æœ¬ {i}/{len(selected_images)}")
            
            result = create_gradcam_visualization(
                model, model_name, image_info, device, model_type, use_chinese
            )
            
            if result is not None:
                successful_visualizations.append(result)
                save_individual_visualization(result, output_dir, i, use_chinese)
            else:
                print(f"    âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥")
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼å…±å¤„ç† {len(successful_visualizations)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ æŸ¥çœ‹ç»“æœ: {output_dir}/")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"   æ¨¡å¼è¿‡æ»¤: {args.bg_mode}")
        print(f"   ç‰¹å¾æ•°æ®é›†æ ·æœ¬: {len(successful_visualizations)}")
        avg_error = np.mean([r['error'] for r in successful_visualizations])
        print(f"   å¹³å‡é¢„æµ‹è¯¯å·®: {avg_error:.2f}")
        
        # æ˜¾ç¤ºé€‰æ‹©çš„æµ“åº¦åˆ†å¸ƒ
        concentrations = [r['concentration'] for r in successful_visualizations]
        print(f"   æµ“åº¦åˆ†å¸ƒ: {min(concentrations):.1f} - {max(concentrations):.1f}")
        print(f"   é€‰æ‹©ç­–ç•¥: {'éšæœºé‡‡æ ·' if args.random_select else 'æŒ‰æµ“åº¦é¡ºåº'}")
        
        # 6æ¡£ç»†åˆ†ç»Ÿè®¡
        if successful_visualizations:
            mode_stats = {}
            for result in successful_visualizations:
                mode = result.get('mode_description', f"{result['bg_type']}_{result.get('power_value', 'unknown')}")
                if mode not in mode_stats:
                    mode_stats[mode] = 0
                mode_stats[mode] += 1
            
            print(f"   6æ¡£ç»†åˆ†åˆ†å¸ƒ:")
            for mode, count in sorted(mode_stats.items()):
                print(f"     {mode}: {count} å¼ ")
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 