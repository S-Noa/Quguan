#!/usr/bin/env python3
"""
æ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…· - ç»Ÿè®¡å¤šä¸ªæœ€ä½³æ¨¡å‹çš„è®­ç»ƒæŸå¤±ä¸éªŒè¯æŸå¤±
æ”¯æŒäº‘ç«¯å’Œæœ¬åœ°ç¯å¢ƒ
"""

import os
import json
import pickle
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import argparse
from pathlib import Path
import glob
import torch
import warnings
warnings.filterwarnings("ignore")

class BatchLossAnalyzer:
    def __init__(self, base_dir=None):
        self.base_dir = base_dir or os.getcwd()
        self.models_data = []
        self.setup_chinese_font()
    
    def setup_chinese_font(self):
        """è®¾ç½®ä¸­æ–‡å­—ä½“"""
        try:
            import matplotlib.font_manager as fm
            chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun']
            available_fonts = [f.name for f in fm.fontManager.ttflist]
            
            for font in chinese_fonts:
                if font in available_fonts:
                    plt.rcParams['font.sans-serif'] = [font]
                    plt.rcParams['axes.unicode_minus'] = False
                    print(f"âœ“ ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
                    return
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
        except Exception as e:
            print(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
    
    def detect_model_directories(self):
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç›®å½•"""
        print("ğŸ” æ£€æµ‹æ¨¡å‹ç›®å½•...")
        
        # å¸¸è§çš„æ¨¡å‹ç›®å½•æ¨¡å¼
        patterns = [
            "feature_training_*_results_*",
            "*_training_results_*",
            "*_results_*",
            "model_*",
            "checkpoint_*"
        ]
        
        model_dirs = []
        for pattern in patterns:
            dirs = glob.glob(os.path.join(self.base_dir, pattern))
            model_dirs.extend([d for d in dirs if os.path.isdir(d)])
        
        # å»é‡å¹¶æ’åº
        model_dirs = sorted(list(set(model_dirs)))
        
        # å»é‡ï¼šå¯¹äºå‰ç¼€ç›¸åŒçš„ç›®å½•ï¼Œåªä¿ç•™æœ€æ–°çš„
        model_dirs = self._deduplicate_model_directories(model_dirs)
        
        print(f"ğŸ“ å‘ç° {len(model_dirs)} ä¸ªæ¨¡å‹ç›®å½• (å·²å»é‡):")
        for i, dir_path in enumerate(model_dirs, 1):
            dir_name = os.path.basename(dir_path)
            print(f"   {i:2d}. {dir_name}")
        
        return model_dirs
    
    def _deduplicate_model_directories(self, model_dirs):
        """å»é‡æ¨¡å‹ç›®å½•ï¼Œå¯¹äºå‰ç¼€ç›¸åŒçš„ç›®å½•åªä¿ç•™æœ€ä½³éªŒè¯æŸå¤±æœ€å°çš„ç›®å½•"""
        if not model_dirs:
            return model_dirs
        
        # æŒ‰å‰ç¼€åˆ†ç»„
        prefix_groups = {}
        
        for dir_path in model_dirs:
            dir_name = os.path.basename(dir_path)
            # æå–å‰ç¼€ï¼ˆå»é™¤æ—¶é—´æˆ³éƒ¨åˆ†ï¼‰
            prefix = self._extract_model_prefix(dir_name)
            
            if prefix not in prefix_groups:
                prefix_groups[prefix] = []
            prefix_groups[prefix].append(dir_path)
        
        # å¯¹æ¯ä¸ªå‰ç¼€ç»„ï¼Œåªä¿ç•™æœ€ä½³éªŒè¯æŸå¤±æœ€å°çš„ç›®å½•
        deduplicated = []
        removed_count = 0
        
        for prefix, dirs in prefix_groups.items():
            if len(dirs) > 1:
                print(f"   ğŸ” å‰ç¼€ '{prefix}' æœ‰ {len(dirs)} ä¸ªç‰ˆæœ¬ï¼Œæ­£åœ¨æ¯”è¾ƒéªŒè¯æŸå¤±...")
                
                # è·å–æ¯ä¸ªç›®å½•çš„éªŒè¯æŸå¤±
                dirs_with_loss = []
                for dir_path in dirs:
                    dir_name = os.path.basename(dir_path)
                    timestamp = self._extract_timestamp(dir_name)
                    
                    # å°è¯•åŠ è½½æŸå¤±æ•°æ®
                    best_val_loss = self._get_best_val_loss(dir_path)
                    
                    dirs_with_loss.append((best_val_loss, timestamp, dir_path, dir_name))
                    
                    if best_val_loss is not None:
                        print(f"      ğŸ“Š {dir_name}: æœ€ä½³éªŒè¯æŸå¤± = {best_val_loss:.4f}")
                    else:
                        print(f"      âŒ {dir_name}: æ— æ³•è·å–éªŒè¯æŸå¤±")
                
                # æ’åºç­–ç•¥ï¼š
                # 1. ä¼˜å…ˆé€‰æ‹©æœ‰éªŒè¯æŸå¤±æ•°æ®çš„ç›®å½•
                # 2. åœ¨æœ‰éªŒè¯æŸå¤±çš„ç›®å½•ä¸­é€‰æ‹©æŸå¤±æœ€å°çš„
                # 3. å¦‚æœæŸå¤±ç›¸åŒï¼Œé€‰æ‹©æ—¶é—´æˆ³æœ€æ–°çš„
                # 4. å¦‚æœéƒ½æ²¡æœ‰æŸå¤±æ•°æ®ï¼Œé€‰æ‹©æ—¶é—´æˆ³æœ€æ–°çš„
                def sort_key(item):
                    best_val_loss, timestamp, dir_path, dir_name = item
                    if best_val_loss is not None:
                        return (0, best_val_loss, -float(timestamp.replace('_', '')))  # æŸå¤±å°çš„ä¼˜å…ˆï¼Œæ—¶é—´æ–°çš„ä¼˜å…ˆ
                    else:
                        return (1, 0, -float(timestamp.replace('_', '')))  # æ²¡æœ‰æŸå¤±æ•°æ®çš„æ’åé¢
                
                dirs_with_loss.sort(key=sort_key)
                
                # ä¿ç•™æ’åºåçš„ç¬¬ä¸€ä¸ªï¼ˆæœ€ä¼˜çš„ï¼‰
                selected_loss, selected_timestamp, selected_dir, selected_name = dirs_with_loss[0]
                deduplicated.append(selected_dir)
                
                # è®°å½•è¢«ç§»é™¤çš„ç›®å½•
                removed_dirs = dirs_with_loss[1:]
                removed_count += len(removed_dirs)
                
                if selected_loss is not None:
                    print(f"   ğŸ† ä¿ç•™æœ€ä½³æ¨¡å‹: {selected_name} (éªŒè¯æŸå¤±: {selected_loss:.4f})")
                else:
                    print(f"   ğŸ“… ä¿ç•™æœ€æ–°æ¨¡å‹: {selected_name} (æ— æŸå¤±æ•°æ®ï¼ŒæŒ‰æ—¶é—´æˆ³é€‰æ‹©)")
                
                for removed_loss, removed_timestamp, removed_dir, removed_name in removed_dirs:
                    if removed_loss is not None:
                        print(f"      â­ï¸ è·³è¿‡: {removed_name} (éªŒè¯æŸå¤±: {removed_loss:.4f})")
                    else:
                        print(f"      â­ï¸ è·³è¿‡: {removed_name} (æ— æŸå¤±æ•°æ®)")
            else:
                deduplicated.append(dirs[0])
        
        if removed_count > 0:
            print(f"   ğŸ“Š å»é‡ç»Ÿè®¡: åŸæœ‰ {len(model_dirs)} ä¸ªç›®å½•ï¼Œå»é‡å {len(deduplicated)} ä¸ªç›®å½•")
        
        return deduplicated
    
    def _get_best_val_loss(self, model_dir):
        """è·å–æ¨¡å‹ç›®å½•çš„æœ€ä½³éªŒè¯æŸå¤±"""
        try:
            # ä¸´æ—¶åˆ›å»ºä¸€ä¸ªæŸå¤±æ•°æ®å­—å…¸
            temp_loss_data = {
                'name': os.path.basename(model_dir),
                'path': model_dir,
                'train_losses': [],
                'val_losses': [],
                'epochs': [],
                'best_epoch': None,
                'best_val_loss': None,
                'final_train_loss': None,
                'final_val_loss': None,
                'source_file': None
            }
            
            # å°è¯•å¤šç§æŸå¤±æ•°æ®æ–‡ä»¶ï¼ˆJSONå’ŒPKLï¼‰
            loss_files = [
                'training_log.json',
                'training.log',
                'loss_history.json',
                'training_history.json',
                'training_results.json',
                'losses.json',
                'log.json',
                'results.json',
                'history.json',
                'training_log.pkl',
                'loss_history.pkl',
                'training_history.pkl'
            ]
            
            # é¦–å…ˆå°è¯•JSONå’ŒPKLæ–‡ä»¶
            for loss_file in loss_files:
                file_path = os.path.join(model_dir, loss_file)
                if os.path.exists(file_path):
                    try:
                        data = self._load_file(file_path)
                        if self._extract_losses(data, temp_loss_data):
                            temp_loss_data['source_file'] = loss_file
                            break
                    except Exception:
                        continue
            
            # å¦‚æœJSON/PKLæ–‡ä»¶éƒ½å¤±è´¥äº†ï¼Œå°è¯•ä»checkpointåŠ è½½
            if not temp_loss_data['source_file']:
                checkpoint_files = ['best_model.pth', 'checkpoint_latest.pth', 'checkpoint.pth', 'model.pth']
                for checkpoint_file in checkpoint_files:
                    file_path = os.path.join(model_dir, checkpoint_file)
                    if os.path.exists(file_path):
                        try:
                            # é¦–å…ˆå°è¯•å®‰å…¨åŠ è½½ï¼ˆweights_only=Trueï¼‰
                            try:
                                checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
                            except Exception:
                                # å¦‚æœå®‰å…¨åŠ è½½å¤±è´¥ï¼Œå°è¯•å®Œæ•´åŠ è½½
                                checkpoint = torch.load(file_path, map_location='cpu', weights_only=False)
                            
                            if self._extract_losses_from_checkpoint(checkpoint, temp_loss_data):
                                temp_loss_data['source_file'] = checkpoint_file
                                break
                        except Exception:
                            continue
            
            # è®¡ç®—æœ€ä½³éªŒè¯æŸå¤±
            if temp_loss_data['val_losses']:
                best_val_loss = min(temp_loss_data['val_losses'])
                return best_val_loss
            elif temp_loss_data['final_val_loss'] is not None:
                return temp_loss_data['final_val_loss']
            else:
                return None
                
        except Exception:
            return None
    
    def _extract_model_prefix(self, dir_name):
        """æå–æ¨¡å‹ç›®å½•çš„å‰ç¼€ï¼ˆå»é™¤æ—¶é—´æˆ³ï¼‰"""
        # ç§»é™¤å¸¸è§çš„æ—¶é—´æˆ³æ ¼å¼
        patterns = [
            r'_\d{8}_\d{6}.*$',    # _20241201_123456
            r'_\d{8}.*$',          # _20241201
            r'_results_\d{8}_\d{6}.*$',  # _results_20241201_123456
            r'_results_\d{8}.*$'   # _results_20241201
        ]
        
        prefix = dir_name
        for pattern in patterns:
            prefix = re.sub(pattern, '', prefix)
        
        return prefix
    
    def _extract_timestamp(self, dir_name):
        """æå–ç›®å½•åä¸­çš„æ—¶é—´æˆ³ç”¨äºæ’åº"""
        # å°è¯•æå–ä¸åŒæ ¼å¼çš„æ—¶é—´æˆ³
        timestamp_patterns = [
            r'(\d{8}_\d{6})',      # 20241201_123456
            r'(\d{8})',            # 20241201
        ]
        
        for pattern in timestamp_patterns:
            match = re.search(pattern, dir_name)
            if match:
                timestamp_str = match.group(1)
                # æ ‡å‡†åŒ–ä¸ºå¯æ¯”è¾ƒçš„æ ¼å¼
                if '_' in timestamp_str:
                    return timestamp_str  # 20241201_123456
                else:
                    return timestamp_str + '_000000'  # 20241201 -> 20241201_000000
        
        # å¦‚æœæ²¡æ‰¾åˆ°æ—¶é—´æˆ³ï¼Œè¿”å›ç›®å½•åæœ¬èº«ä½œä¸ºæ’åºä¾æ®
        return dir_name
    
    def _extract_model_type(self, model_name):
        """æå–æ¨¡å‹ç±»å‹"""
        name = model_name.lower()
        
        # æœ¬åœ°æ¨¡å‹ç±»å‹è¯†åˆ«ï¼ˆå®Œæ•´åŒ¹é…ï¼‰
        local_models = [
            'baseline_cnn', 'enhanced_cnn_resnet50', 'enhanced_cnn_vgg16', 
            'cnn_vgg', 'cnn_resnet50', 'cnn_resnet', 'cnn_vgg16'
        ]
        
        for model in local_models:
            if model in name:
                # ç®€åŒ–åç§°
                if 'baseline_cnn' in model:
                    return 'baseline_cnn'
                elif 'enhanced_cnn_resnet50' in model:
                    return 'cnn_resnet50'
                elif 'enhanced_cnn_vgg16' in model:
                    return 'cnn_vgg16'
                elif 'cnn_vgg' in model:
                    return 'cnn_vgg'
                elif 'cnn_resnet50' in model:
                    return 'cnn_resnet50'
                else:
                    return model
        
        # äº‘ç«¯æ¨¡å‹ç±»å‹è¯†åˆ«
        cloud_models = [
            'vgg_unfrozen', 'vgg', 'resnet50_unfrozen', 'resnet50', 
            'resnet', 'densenet', 'efficientnet', 'mobilenet'
        ]
        
        for model in cloud_models:
            if model in name:
                return model
        
        # é€šç”¨æ¨¡å‹ç±»å‹è¯†åˆ«
        general_models = [
            'transformer', 'bert', 'gpt', 'lstm', 'rnn', 'alexnet', 
            'inception', 'squeezenet', 'shufflenet'
        ]
        
        for model in general_models:
            if model in name:
                return model
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„è¯
        parts = model_name.split('_')
        for part in parts:
            if len(part) > 2 and part.isalpha():
                return part.lower()
        
        return 'model'
    
    def _extract_training_mode(self, model_name):
        """æå–è®­ç»ƒæ¨¡å¼"""
        name = model_name.lower()
        
        # è®­ç»ƒæ¨¡å¼æ¨¡å¼åŒ¹é…
        training_modes = [
            r'bg0[-_](\d+)mw',      # bg0-20mw, bg0_100mw
            r'bg1[-_](\d+)mw',      # bg1-100mw, bg1_400mw
            r'bg(\d+)[-_](\d+)mw',  # bg0-20mw, bg1-100mw
            r'bg(\d+)',             # bg0, bg1
            r'all',                 # all
            r'frozen',              # frozen
            r'unfrozen',            # unfrozen
            r'finetune',            # finetune
            r'pretrained',          # pretrained
            r'scratch'              # from scratch
        ]
        
        for pattern in training_modes:
            match = re.search(pattern, name)
            if match:
                if 'bg0' in match.group(0) and 'mw' in match.group(0):
                    # æå–bg0-XXmwæ ¼å¼
                    mw_match = re.search(r'bg0[-_](\d+)mw', match.group(0))
                    if mw_match:
                        return f"bg0_{mw_match.group(1)}mw"
                elif 'bg1' in match.group(0) and 'mw' in match.group(0):
                    # æå–bg1-XXmwæ ¼å¼
                    mw_match = re.search(r'bg1[-_](\d+)mw', match.group(0))
                    if mw_match:
                        return f"bg1_{mw_match.group(1)}mw"
                elif 'bg' in match.group(0):
                    # æå–bgXæ ¼å¼
                    bg_match = re.search(r'bg(\d+)', match.group(0))
                    if bg_match:
                        return f"bg{bg_match.group(1)}"
                else:
                    return match.group(0)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†è®­ç»ƒæ¨¡å¼ï¼Œå°è¯•æå–å…¶ä»–æœ‰æ„ä¹‰çš„æ ‡è¯†ç¬¦
        parts = model_name.split('_')
        meaningful_parts = []
        
        for part in parts:
            part_lower = part.lower()
            # è·³è¿‡æ—¶é—´æˆ³ã€resultsç­‰æ— å…³éƒ¨åˆ†
            if (not re.match(r'\d{8}', part) and 
                not re.match(r'\d{6}', part) and
                part_lower not in ['results', 'training', 'feature', 'enhanced', 'baseline']):
                if len(part) > 1 and not part.isdigit():
                    meaningful_parts.append(part_lower)
        
        # è¿”å›æœ€åä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ä½œä¸ºè®­ç»ƒæ¨¡å¼
        if meaningful_parts:
            # ä¼˜å…ˆè¿”å›åŒ…å«æ•°å­—æˆ–ç‰¹æ®Šæ ‡è¯†çš„éƒ¨åˆ†
            for part in meaningful_parts:
                if re.search(r'\d', part) or part in ['all', 'frozen', 'unfrozen']:
                    return part
            return meaningful_parts[-1]
        
        return None
    
    def load_loss_data(self, model_dir):
        """ä»æ¨¡å‹ç›®å½•åŠ è½½æŸå¤±æ•°æ®"""
        model_name = os.path.basename(model_dir)
        print(f"\nğŸ“Š åˆ†ææ¨¡å‹: {model_name}")
        
        loss_data = {
            'name': model_name,
            'path': model_dir,
            'train_losses': [],
            'val_losses': [],
            'epochs': [],
            'best_epoch': None,
            'best_val_loss': None,
            'final_train_loss': None,
            'final_val_loss': None,
            'source_file': None
        }
        
        # å°è¯•å¤šç§æŸå¤±æ•°æ®æ–‡ä»¶ï¼ˆJSONå’ŒPKLï¼‰
        loss_files = [
            'training_log.json',
            'training.log',  # æ·»åŠ æ”¯æŒ
            'loss_history.json',
            'training_history.json',
            'training_results.json',
            'losses.json',
            'log.json',
            'results.json',  # æ·»åŠ æ”¯æŒ
            'history.json',  # æ·»åŠ æ”¯æŒ
            'training_log.pkl',
            'loss_history.pkl',
            'training_history.pkl'
        ]
        
        # é¦–å…ˆå°è¯•JSONå’ŒPKLæ–‡ä»¶
        for loss_file in loss_files:
            file_path = os.path.join(model_dir, loss_file)
            if os.path.exists(file_path):
                try:
                    data = self._load_file(file_path)
                    if self._extract_losses(data, loss_data):
                        loss_data['source_file'] = loss_file
                        print(f"   âœ“ ä» {loss_file} åŠ è½½æŸå¤±æ•°æ®")
                        break
                except Exception as e:
                    print(f"   âš ï¸ åŠ è½½ {loss_file} å¤±è´¥: {e}")
                    continue  # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ–‡ä»¶
        
        # å¦‚æœJSON/PKLæ–‡ä»¶éƒ½å¤±è´¥äº†ï¼Œå°è¯•ä»checkpointåŠ è½½
        if not loss_data['source_file']:
            print(f"   ğŸ“ JSON/PKLæ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œå°è¯•ä»æ¨¡å‹æ–‡ä»¶åŠ è½½...")
            checkpoint_files = ['best_model.pth', 'checkpoint_latest.pth', 'checkpoint.pth', 'model.pth']
            for checkpoint_file in checkpoint_files:
                file_path = os.path.join(model_dir, checkpoint_file)
                if os.path.exists(file_path):
                    try:
                        # é¦–å…ˆå°è¯•å®‰å…¨åŠ è½½ï¼ˆweights_only=Trueï¼‰
                        try:
                            checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
                        except Exception as safe_load_error:
                            # å¦‚æœå®‰å…¨åŠ è½½å¤±è´¥ï¼Œå°è¯•å®Œæ•´åŠ è½½ï¼ˆé€‚ç”¨äºåŒ…å«argparse.Namespaceç­‰å¯¹è±¡çš„æ–‡ä»¶ï¼‰
                            print(f"   ğŸ“ å®‰å…¨åŠ è½½å¤±è´¥ï¼Œå°è¯•å®Œæ•´åŠ è½½ {checkpoint_file}...")
                            checkpoint = torch.load(file_path, map_location='cpu', weights_only=False)
                        
                        if self._extract_losses_from_checkpoint(checkpoint, loss_data):
                            loss_data['source_file'] = checkpoint_file
                            print(f"   âœ“ ä» {checkpoint_file} åŠ è½½æŸå¤±æ•°æ®")
                            break
                    except Exception as e:
                        print(f"   âš ï¸ åŠ è½½ {checkpoint_file} å¤±è´¥: {e}")
                        continue  # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ–‡ä»¶
        
        if loss_data['source_file']:
            self._calculate_statistics(loss_data)
            return loss_data
        else:
            print(f"   âŒ æœªæ‰¾åˆ°æŸå¤±æ•°æ®")
            return None
    
    def _load_file(self, file_path):
        """åŠ è½½æ–‡ä»¶ï¼ˆJSONæˆ–PKLï¼‰"""
        file_name = os.path.basename(file_path)
        
        if file_path.endswith('.json') or file_path.endswith('.log'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    
                # å¦‚æœæ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡
                if not content:
                    raise ValueError("æ–‡ä»¶ä¸ºç©º")
                
                # å°è¯•ç›´æ¥è§£æJSON
                try:
                    return json.loads(content)
                except json.JSONDecodeError as e:
                    print(f"   âš ï¸ JSONè§£æå¤±è´¥ ({file_name}): {e}")
                    
                    # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
                    # 1. ç§»é™¤å°¾éšé€—å·
                    content = content.rstrip().rstrip(',')
                    
                    # 2. å°è¯•é€è¡Œè§£æï¼ˆå¤„ç†å¤šä¸ªJSONå¯¹è±¡çš„æƒ…å†µï¼‰
                    lines = content.split('\n')
                    json_objects = []
                    
                    for line in lines:
                        line = line.strip()
                        if line and not line.startswith('#'):  # è·³è¿‡æ³¨é‡Šè¡Œ
                            try:
                                obj = json.loads(line)
                                json_objects.append(obj)
                            except:
                                continue
                    
                    if json_objects:
                        print(f"   ğŸ“ æ‰¾åˆ° {len(json_objects)} ä¸ªJSONå¯¹è±¡")
                        
                        # å¦‚æœæœ‰å¤šä¸ªJSONå¯¹è±¡ï¼Œå°è¯•æ„å»ºè®­ç»ƒå†å²
                        if len(json_objects) > 1:
                            # å°è¯•ä»å¤šä¸ªepochè®°å½•æ„å»ºå†å²
                            train_losses = []
                            val_losses = []
                            epochs = []
                            
                            for obj in json_objects:
                                if isinstance(obj, dict):
                                    # å°è¯•ä¸åŒçš„é”®åç»„åˆ
                                    train_loss_keys = ['train_loss', 'training_loss', 'loss']
                                    val_loss_keys = ['val_loss', 'validation_loss', 'valid_loss']
                                    epoch_keys = ['epoch', 'step', 'iteration']
                                    
                                    train_loss = None
                                    val_loss = None
                                    epoch = None
                                    
                                    for key in train_loss_keys:
                                        if key in obj:
                                            train_loss = float(obj[key])
                                            break
                                    
                                    for key in val_loss_keys:
                                        if key in obj:
                                            val_loss = float(obj[key])
                                            break
                                    
                                    for key in epoch_keys:
                                        if key in obj:
                                            epoch = int(obj[key])
                                            break
                                    
                                    if train_loss is not None and val_loss is not None:
                                        train_losses.append(train_loss)
                                        val_losses.append(val_loss)
                                        epochs.append(epoch if epoch is not None else len(epochs) + 1)
                            
                            if train_losses and val_losses:
                                return {
                                    'train_losses': train_losses,
                                    'val_losses': val_losses,
                                    'epochs': epochs
                                }
                        
                        # å•ä¸ªå¯¹è±¡æˆ–åˆå¹¶ç­–ç•¥
                        if len(json_objects) == 1:
                            return json_objects[0]
                        else:
                            # å°è¯•åˆå¹¶å¤šä¸ªå¯¹è±¡
                            merged = {}
                            for obj in json_objects:
                                if isinstance(obj, dict):
                                    merged.update(obj)
                            return merged if merged else json_objects
                    
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æå–éƒ¨åˆ†æœ‰æ•ˆçš„JSON
                    # æŸ¥æ‰¾å¯èƒ½çš„JSONç‰‡æ®µ
                    import re
                    json_pattern = r'\{[^{}]*\}'
                    matches = re.findall(json_pattern, content)
                    
                    for match in matches:
                        try:
                            return json.loads(match)
                        except:
                            continue
                    
                    # æœ€åå°è¯•ï¼šæŸ¥æ‰¾æ•°ç»„æ ¼å¼
                    array_pattern = r'\[[^\[\]]*\]'
                    matches = re.findall(array_pattern, content)
                    
                    for match in matches:
                        try:
                            return json.loads(match)
                        except:
                            continue
                    
                    raise ValueError(f"æ— æ³•è§£æJSONæ–‡ä»¶: {file_name}")
                    
            except Exception as e:
                raise ValueError(f"åŠ è½½æ–‡ä»¶å¤±è´¥ ({file_name}): {e}")
                
        elif file_path.endswith('.pkl'):
            with open(file_path, 'rb') as f:
                return pickle.load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
    
    def _extract_losses(self, data, loss_data):
        """ä»æ•°æ®ä¸­æå–æŸå¤±å€¼"""
        # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
        if isinstance(data, dict):
            # æ ¼å¼1: {'train_losses': [...], 'val_losses': [...]}
            if 'train_losses' in data and 'val_losses' in data:
                loss_data['train_losses'] = data['train_losses']
                loss_data['val_losses'] = data['val_losses']
                loss_data['epochs'] = list(range(1, len(data['train_losses']) + 1))
                return True
            
            # æ ¼å¼2: {'epoch_1': {'train_loss': ..., 'val_loss': ...}, ...}
            epochs = [k for k in data.keys() if k.startswith('epoch_')]
            if epochs:
                epochs.sort(key=lambda x: int(x.split('_')[1]))
                for epoch in epochs:
                    epoch_data = data[epoch]
                    if 'train_loss' in epoch_data and 'val_loss' in epoch_data:
                        loss_data['train_losses'].append(epoch_data['train_loss'])
                        loss_data['val_losses'].append(epoch_data['val_loss'])
                        loss_data['epochs'].append(int(epoch.split('_')[1]))
                return len(loss_data['train_losses']) > 0
            
            # æ ¼å¼3: {'history': {'loss': [...], 'val_loss': [...]}}
            if 'history' in data:
                history = data['history']
                if 'loss' in history and 'val_loss' in history:
                    loss_data['train_losses'] = history['loss']
                    loss_data['val_losses'] = history['val_loss']
                    loss_data['epochs'] = list(range(1, len(history['loss']) + 1))
                    return True
            
            # æ ¼å¼4: å•å€¼æŸå¤± {'final_train_loss': ..., 'final_val_loss': ...}
            final_loss_combinations = [
                ('final_train_loss', 'final_val_loss'),
                ('train_loss', 'val_loss'),
                ('training_loss', 'validation_loss'),
                ('best_train_loss', 'best_val_loss'),
                ('last_train_loss', 'last_val_loss')
            ]
            
            for train_key, val_key in final_loss_combinations:
                if train_key in data and val_key in data:
                    # ç¡®ä¿æ˜¯æ•°å€¼ç±»å‹
                    try:
                        train_loss = float(data[train_key])
                        val_loss = float(data[val_key])
                        loss_data['final_train_loss'] = train_loss
                        loss_data['final_val_loss'] = val_loss
                        print(f"   âœ“ æ‰¾åˆ°æœ€ç»ˆæŸå¤±: {train_key}={train_loss:.4f}, {val_key}={val_loss:.4f}")
                        return True
                    except (ValueError, TypeError):
                        continue
        
        # æ ¼å¼4: åˆ—è¡¨æ ¼å¼ [{'train_loss': ..., 'val_loss': ...}, ...]
        elif isinstance(data, list):
            for i, epoch_data in enumerate(data):
                if isinstance(epoch_data, dict):
                    if 'train_loss' in epoch_data and 'val_loss' in epoch_data:
                        loss_data['train_losses'].append(epoch_data['train_loss'])
                        loss_data['val_losses'].append(epoch_data['val_loss'])
                        loss_data['epochs'].append(i + 1)
            return len(loss_data['train_losses']) > 0
        
        return False
    
    def _extract_losses_from_checkpoint(self, checkpoint, loss_data):
        """ä»checkpointä¸­æå–æŸå¤±æ•°æ®"""
        print(f"   ğŸ” æ£€æŸ¥checkpointå†…å®¹ï¼Œå¯ç”¨é”®: {list(checkpoint.keys())}")
        
        # å¸¸è§çš„checkpointé”®åï¼ˆæ‰©å±•æ›´å¤šå¯èƒ½çš„ç»„åˆï¼‰
        loss_keys = [
            ('train_losses', 'val_losses'),
            ('train_loss_history', 'val_loss_history'),
            ('training_losses', 'validation_losses'),
            ('loss_history', 'val_loss_history'),
            ('train_loss_list', 'val_loss_list'),
            ('training_loss', 'validation_loss'),
            ('losses_train', 'losses_val'),
            ('train_losses_epoch', 'val_losses_epoch'),
            ('epoch_train_losses', 'epoch_val_losses'),
            ('history_train_loss', 'history_val_loss')
        ]
        
        # å°è¯•ä¸åŒçš„é”®åç»„åˆ
        for train_key, val_key in loss_keys:
            if train_key in checkpoint and val_key in checkpoint:
                train_losses = checkpoint[train_key]
                val_losses = checkpoint[val_key]
                
                # ç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
                if isinstance(train_losses, (list, tuple)) and isinstance(val_losses, (list, tuple)):
                    if len(train_losses) > 0 and len(val_losses) > 0:
                        loss_data['train_losses'] = list(train_losses)
                        loss_data['val_losses'] = list(val_losses)
                        loss_data['epochs'] = list(range(1, len(train_losses) + 1))
                        print(f"   âœ“ æ‰¾åˆ°æŸå¤±å†å²: {train_key}, {val_key}")
                        return True
        
        # å°è¯•ä»åµŒå¥—çš„historyå­—å…¸ä¸­æå–
        if 'history' in checkpoint:
            history = checkpoint['history']
            if isinstance(history, dict):
                # å°è¯•ä¸åŒçš„é”®å
                train_keys = ['loss', 'train_loss', 'training_loss', 'train_losses']
                val_keys = ['val_loss', 'validation_loss', 'val_losses', 'valid_loss']
                
                for train_key in train_keys:
                    for val_key in val_keys:
                        if train_key in history and val_key in history:
                            train_losses = history[train_key]
                            val_losses = history[val_key]
                            
                            if isinstance(train_losses, (list, tuple)) and isinstance(val_losses, (list, tuple)):
                                if len(train_losses) > 0 and len(val_losses) > 0:
                                    loss_data['train_losses'] = list(train_losses)
                                    loss_data['val_losses'] = list(val_losses)
                                    loss_data['epochs'] = list(range(1, len(train_losses) + 1))
                                    print(f"   âœ“ ä»historyä¸­æ‰¾åˆ°æŸå¤±: {train_key}, {val_key}")
                                    return True
        
        # å°è¯•ä»å•ä¸ªå€¼è·å–æœ€ç»ˆæŸå¤±
        single_loss_keys = [
            ('train_loss', 'val_loss'),
            ('training_loss', 'validation_loss'),
            ('final_train_loss', 'final_val_loss'),
            ('best_train_loss', 'best_val_loss'),
            ('last_train_loss', 'last_val_loss')
        ]
        
        for train_key, val_key in single_loss_keys:
            if train_key in checkpoint and val_key in checkpoint:
                loss_data['final_train_loss'] = float(checkpoint[train_key])
                loss_data['final_val_loss'] = float(checkpoint[val_key])
                print(f"   âœ“ æ‰¾åˆ°æœ€ç»ˆæŸå¤±: {train_key}={loss_data['final_train_loss']:.4f}, {val_key}={loss_data['final_val_loss']:.4f}")
                return True
        
        # å°è¯•ä»optimizerçŠ¶æ€æˆ–å…¶ä»–å¯èƒ½çš„ä½ç½®æå–
        if 'optimizer' in checkpoint and isinstance(checkpoint['optimizer'], dict):
            optimizer_state = checkpoint['optimizer']
            if 'state' in optimizer_state and 'param_groups' in optimizer_state:
                # æœ‰æ—¶æŸå¤±ä¿¡æ¯å¯èƒ½å­˜å‚¨åœ¨optimizerä¸­
                pass
        
        print(f"   âš ï¸ æœªåœ¨checkpointä¸­æ‰¾åˆ°æŸå¤±æ•°æ®")
        return False
    
    def _calculate_statistics(self, loss_data):
        """è®¡ç®—æŸå¤±ç»Ÿè®¡ä¿¡æ¯"""
        if loss_data['train_losses'] and loss_data['val_losses']:
            # æ‰¾åˆ°æœ€ä½³éªŒè¯æŸå¤±
            val_losses = loss_data['val_losses']
            best_idx = np.argmin(val_losses)
            loss_data['best_epoch'] = loss_data['epochs'][best_idx]
            loss_data['best_val_loss'] = val_losses[best_idx]
            
            # æœ€ç»ˆæŸå¤±
            loss_data['final_train_loss'] = loss_data['train_losses'][-1]
            loss_data['final_val_loss'] = loss_data['val_losses'][-1]
            
            print(f"   ğŸ“ˆ è®­ç»ƒè½®æ•°: {len(loss_data['train_losses'])}")
            print(f"   ğŸ¯ æœ€ä½³éªŒè¯æŸå¤±: {loss_data['best_val_loss']:.4f} (ç¬¬{loss_data['best_epoch']}è½®)")
            print(f"   ğŸ“Š æœ€ç»ˆè®­ç»ƒæŸå¤±: {loss_data['final_train_loss']:.4f}")
            print(f"   ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {loss_data['final_val_loss']:.4f}")
        elif loss_data['final_train_loss'] is not None:
            print(f"   ğŸ“Š è®­ç»ƒæŸå¤±: {loss_data['final_train_loss']:.4f}")
            print(f"   ğŸ“Š éªŒè¯æŸå¤±: {loss_data['final_val_loss']:.4f}")
    
    def analyze_models(self, model_dirs=None):
        """åˆ†æå¤šä¸ªæ¨¡å‹çš„æŸå¤±"""
        if model_dirs is None:
            model_dirs = self.detect_model_directories()
        
        if not model_dirs:
            print("âŒ æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•")
            return []
        
        print(f"\nğŸ” å¼€å§‹åˆ†æ {len(model_dirs)} ä¸ªæ¨¡å‹...")
        
        for model_dir in model_dirs:
            loss_data = self.load_loss_data(model_dir)
            if loss_data:
                self.models_data.append(loss_data)
        
        print(f"\nâœ… æˆåŠŸåˆ†æ {len(self.models_data)} ä¸ªæ¨¡å‹")
        return self.models_data
    
    def create_loss_comparison_plot(self, output_dir="loss_analysis"):
        """åˆ›å»ºæŸå¤±å¯¹æ¯”å›¾"""
        if not self.models_data:
            print("âŒ æ²¡æœ‰æ¨¡å‹æ•°æ®å¯ä¾›ç»˜å›¾")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        
        # è¿‡æ»¤æœ‰å®Œæ•´æŸå¤±å†å²çš„æ¨¡å‹
        models_with_history = [m for m in self.models_data if m['train_losses']]
        
        if not models_with_history:
            print("âŒ æ²¡æœ‰æ¨¡å‹åŒ…å«å®Œæ•´çš„æŸå¤±å†å²")
            return
        
        # æ™ºèƒ½ç”Ÿæˆæ¨¡å‹æ˜¾ç¤ºåç§°
        def get_display_name(model_name, max_length=40):
            """ç”Ÿæˆé€‚åˆæ˜¾ç¤ºçš„æ¨¡å‹åç§°ï¼šæ¨¡å‹ç±»å‹_è®­ç»ƒæ¨¡å¼"""
            # æå–æ¨¡å‹ç±»å‹å’Œè®­ç»ƒæ¨¡å¼
            model_type = self._extract_model_type(model_name)
            training_mode = self._extract_training_mode(model_name)
            
            # æ„å»ºæ˜¾ç¤ºåç§°
            if training_mode:
                display_name = f"{model_type}_{training_mode}"
            else:
                display_name = model_type
            
            return display_name[:max_length]
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆæ˜¾ç¤ºåç§°
        display_names = {}
        for model in models_with_history:
            display_names[model['name']] = get_display_name(model['name'])
        
        # æ£€æŸ¥é‡å¤åç§°å¹¶æ·»åŠ åºå·
        name_counts = {}
        for model_name, display_name in display_names.items():
            if display_name in name_counts:
                name_counts[display_name] += 1
                display_names[model_name] = f"{display_name}_{name_counts[display_name]}"
            else:
                name_counts[display_name] = 1
        
        # åˆ›å»ºæ›´å¤§çš„å›¾å½¢ä»¥å®¹çº³å®Œæ•´ä¿¡æ¯
        fig, axes = plt.subplots(2, 2, figsize=(20, 14))
        fig.suptitle('æ¨¡å‹è®­ç»ƒæŸå¤±å¯¹æ¯”åˆ†æ', fontsize=18, fontweight='bold')
        
        # 1. è®­ç»ƒæŸå¤±å¯¹æ¯”
        ax1 = axes[0, 0]
        for model in models_with_history:
            ax1.plot(model['epochs'], model['train_losses'], 
                    label=display_names[model['name']], linewidth=2)
        ax1.set_title('è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=14)
        ax1.set_xlabel('è®­ç»ƒè½®æ•°', fontsize=12)
        ax1.set_ylabel('è®­ç»ƒæŸå¤±', fontsize=12)
        # è°ƒæ•´å›¾ä¾‹ä½ç½®å’Œå¤§å°
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # 2. éªŒè¯æŸå¤±å¯¹æ¯”
        ax2 = axes[0, 1]
        for model in models_with_history:
            ax2.plot(model['epochs'], model['val_losses'], 
                    label=display_names[model['name']], linewidth=2)
        ax2.set_title('éªŒè¯æŸå¤±å¯¹æ¯”', fontsize=14)
        ax2.set_xlabel('è®­ç»ƒè½®æ•°', fontsize=12)
        ax2.set_ylabel('éªŒè¯æŸå¤±', fontsize=12)
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        # 3. æœ€ä½³éªŒè¯æŸå¤±æ’å
        ax3 = axes[1, 0]
        models_sorted = sorted(models_with_history, key=lambda x: x['best_val_loss'])
        names = [display_names[m['name']] for m in models_sorted]
        best_losses = [m['best_val_loss'] for m in models_sorted]
        
        bars = ax3.bar(range(len(names)), best_losses, color='skyblue', alpha=0.7)
        ax3.set_title('æœ€ä½³éªŒè¯æŸå¤±æ’å', fontsize=14)
        ax3.set_xlabel('æ¨¡å‹', fontsize=12)
        ax3.set_ylabel('æœ€ä½³éªŒè¯æŸå¤±', fontsize=12)
        ax3.set_xticks(range(len(names)))
        # æ”¹è¿›xè½´æ ‡ç­¾æ˜¾ç¤º
        ax3.set_xticklabels(names, rotation=45, ha='right', fontsize=10)
        
        # æ™ºèƒ½æ·»åŠ æ•°å€¼æ ‡ç­¾ï¼Œé¿å…é‡å 
        self._add_smart_value_labels(ax3, bars, best_losses)
        
        # 4. æ”¶æ•›æ€§åˆ†æï¼ˆæœ€å10è½®çš„æŸå¤±å˜åŒ–ï¼‰
        ax4 = axes[1, 1]
        for model in models_with_history:
            if len(model['val_losses']) >= 10:
                last_10 = model['val_losses'][-10:]
                epochs_last_10 = list(range(len(model['val_losses'])-9, len(model['val_losses'])+1))
                ax4.plot(epochs_last_10, last_10, 
                        label=display_names[model['name']], linewidth=2, marker='o')
        
        ax4.set_title('æ”¶æ•›æ€§åˆ†æï¼ˆæœ€å10è½®ï¼‰', fontsize=14)
        ax4.set_xlabel('è®­ç»ƒè½®æ•°', fontsize=12)
        ax4.set_ylabel('éªŒè¯æŸå¤±', fontsize=12)
        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
        ax4.grid(True, alpha=0.3)
        
        # è°ƒæ•´å­å›¾é—´è·ä»¥é˜²æ­¢é‡å 
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
        # ä¿å­˜å›¾åƒï¼Œä½¿ç”¨æ›´é«˜çš„DPIå’Œæ›´å¥½çš„è¾¹ç•Œè®¾ç½®
        plot_path = os.path.join(output_dir, 'loss_comparison.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
        print(f"ğŸ“Š æŸå¤±å¯¹æ¯”å›¾å·²ä¿å­˜: {plot_path}")
        
        # åŒæ—¶ç”Ÿæˆä¸€ä¸ªå¤§å°ºå¯¸ç‰ˆæœ¬ç”¨äºè¯¦ç»†æŸ¥çœ‹
        plot_path_large = os.path.join(output_dir, 'loss_comparison_large.png')
        fig.set_size_inches(24, 18)
        plt.savefig(plot_path_large, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
        print(f"ğŸ“Š å¤§å°ºå¯¸æŸå¤±å¯¹æ¯”å›¾å·²ä¿å­˜: {plot_path_large}")
        
        plt.show()
    
    def _add_smart_value_labels(self, ax, bars, values, min_distance=0.05):
        """æ™ºèƒ½æ·»åŠ æ•°å€¼æ ‡ç­¾ï¼Œé¿å…é‡å """
        if not bars or not values:
            return
        
        # è®¡ç®—å›¾è¡¨çš„yè½´èŒƒå›´
        y_min, y_max = ax.get_ylim()
        y_range = y_max - y_min
        
        # è®¡ç®—æœ€å°è·ç¦»ï¼ˆç›¸å¯¹äºyè½´èŒƒå›´ï¼‰
        min_distance_abs = min_distance * y_range
        
        # æ”¶é›†æ‰€æœ‰æ ‡ç­¾çš„ä½ç½®ä¿¡æ¯
        label_positions = []
        for bar, value in zip(bars, values):
            x_pos = bar.get_x() + bar.get_width() / 2
            y_pos = bar.get_height()
            label_positions.append({
                'x': x_pos,
                'y': y_pos,
                'value': value,
                'bar': bar,
                'adjusted_y': y_pos  # è°ƒæ•´åçš„yä½ç½®
            })
        
        # æ£€æµ‹å’Œè§£å†³é‡å 
        num_labels = len(label_positions)
        if num_labels <= 1:
            # å•ä¸ªæ ‡ç­¾ï¼Œç›´æ¥æ˜¾ç¤º
            for label_info in label_positions:
                ax.text(label_info['x'], label_info['adjusted_y'],
                       f'{label_info["value"]:.3f}', 
                       ha='center', va='bottom', fontsize=9,
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
            return
        
        # å¤šä¸ªæ ‡ç­¾æ—¶çš„æ™ºèƒ½å¸ƒå±€
        # æ–¹æ¡ˆ1: äº¤æ›¿æ˜¾ç¤ºåœ¨ä¸Šæ–¹å’Œä¸‹æ–¹
        if num_labels <= 6:
            for i, label_info in enumerate(label_positions):
                if i % 2 == 0:
                    # å¶æ•°ç´¢å¼•ï¼šæ˜¾ç¤ºåœ¨æŸ±å­ä¸Šæ–¹
                    y_pos = label_info['y'] + min_distance_abs * 0.5
                    va = 'bottom'
                else:
                    # å¥‡æ•°ç´¢å¼•ï¼šæ˜¾ç¤ºåœ¨æŸ±å­å†…éƒ¨æˆ–ä¸‹æ–¹
                    if label_info['y'] > y_range * 0.3:  # æŸ±å­è¶³å¤Ÿé«˜
                        y_pos = label_info['y'] * 0.5  # æŸ±å­ä¸­é—´
                        va = 'center'
                    else:
                        y_pos = label_info['y'] + min_distance_abs * 0.5
                        va = 'bottom'
                
                ax.text(label_info['x'], y_pos,
                       f'{label_info["value"]:.3f}', 
                       ha='center', va=va, fontsize=9,
                       bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))
        
        # æ–¹æ¡ˆ2: æ ‡ç­¾æ•°é‡è¾ƒå¤šæ—¶ï¼Œä½¿ç”¨æ›´ç´§å‡‘çš„æ˜¾ç¤º
        else:
            # è®¡ç®—æ˜¯å¦éœ€è¦æ—‹è½¬æ ‡ç­¾
            font_size = max(7, 10 - num_labels // 3)  # æ ¹æ®æ•°é‡è°ƒæ•´å­—ä½“å¤§å°
            
            for i, label_info in enumerate(label_positions):
                # ä½¿ç”¨é˜¶æ¢¯å¼å¸ƒå±€
                level = i % 3  # ä¸‰ä¸ªå±‚æ¬¡
                y_offset = min_distance_abs * (0.5 + level * 0.8)
                y_pos = label_info['y'] + y_offset
                
                # ç¡®ä¿ä¸è¶…å‡ºå›¾è¡¨èŒƒå›´
                if y_pos > y_max * 0.95:
                    y_pos = label_info['y'] * 0.7  # æ”¾åœ¨æŸ±å­å†…éƒ¨
                    va = 'center'
                    facecolor = 'yellow'
                else:
                    va = 'bottom'
                    facecolor = 'white'
                
                ax.text(label_info['x'], y_pos,
                       f'{label_info["value"]:.2f}',  # å‡å°‘å°æ•°ä½æ•°
                       ha='center', va=va, fontsize=font_size,
                       bbox=dict(boxstyle='round,pad=0.2', facecolor=facecolor, alpha=0.8))
    
    def create_summary_table(self, output_dir="loss_analysis"):
        """åˆ›å»ºæ±‡æ€»è¡¨æ ¼"""
        if not self.models_data:
            print("âŒ æ²¡æœ‰æ¨¡å‹æ•°æ®å¯ä¾›åˆ†æ")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        
        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        table_data = []
        for model in self.models_data:
            row = {
                'æ¨¡å‹åç§°': model['name'],
                'è®­ç»ƒè½®æ•°': len(model['train_losses']) if model['train_losses'] else 'N/A',
                'æœ€ä½³éªŒè¯æŸå¤±': f"{model['best_val_loss']:.4f}" if model['best_val_loss'] else 'N/A',
                'æœ€ä½³è½®æ•°': model['best_epoch'] if model['best_epoch'] else 'N/A',
                'æœ€ç»ˆè®­ç»ƒæŸå¤±': f"{model['final_train_loss']:.4f}" if model['final_train_loss'] else 'N/A',
                'æœ€ç»ˆéªŒè¯æŸå¤±': f"{model['final_val_loss']:.4f}" if model['final_val_loss'] else 'N/A',
                'æ•°æ®æº': model['source_file'] or 'N/A'
            }
            table_data.append(row)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(table_data)
        
        # æŒ‰æœ€ä½³éªŒè¯æŸå¤±æ’åº
        df_sorted = df.copy()
        df_sorted['æœ€ä½³éªŒè¯æŸå¤±_æ•°å€¼'] = pd.to_numeric(df_sorted['æœ€ä½³éªŒè¯æŸå¤±'], errors='coerce')
        df_sorted = df_sorted.sort_values('æœ€ä½³éªŒè¯æŸå¤±_æ•°å€¼').drop('æœ€ä½³éªŒè¯æŸå¤±_æ•°å€¼', axis=1)
        
        # ä¿å­˜CSV
        csv_path = os.path.join(output_dir, 'loss_summary.csv')
        df_sorted.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“‹ æ±‡æ€»è¡¨æ ¼å·²ä¿å­˜: {csv_path}")
        
        # æ‰“å°è¡¨æ ¼
        print("\nğŸ“Š æ¨¡å‹æŸå¤±æ±‡æ€»è¡¨:")
        print("=" * 120)
        print(df_sorted.to_string(index=False))
        print("=" * 120)
        
        return df_sorted
    
    def generate_report(self, output_dir="loss_analysis"):
        """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
        print("\nğŸ” ç”ŸæˆæŸå¤±åˆ†ææŠ¥å‘Š...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºæŠ¥å‘Š
        report_path = os.path.join(output_dir, 'loss_analysis_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# æ¨¡å‹è®­ç»ƒæŸå¤±åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**åˆ†ææ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**åˆ†ææ¨¡å‹æ•°é‡**: {len(self.models_data)}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            if self.models_data:
                models_with_history = [m for m in self.models_data if m['train_losses']]
                if models_with_history:
                    best_model = min(models_with_history, key=lambda x: x['best_val_loss'])
                    f.write("## æ€»ä½“ç»Ÿè®¡\n\n")
                    f.write(f"- **æœ€ä½³æ¨¡å‹**: {best_model['name']}\n")
                    f.write(f"- **æœ€ä½³éªŒè¯æŸå¤±**: {best_model['best_val_loss']:.4f}\n")
                    f.write(f"- **å¹³å‡è®­ç»ƒè½®æ•°**: {np.mean([len(m['train_losses']) for m in models_with_history]):.1f}\n\n")
            
            # è¯¦ç»†ä¿¡æ¯
            f.write("## æ¨¡å‹è¯¦ç»†ä¿¡æ¯\n\n")
            for i, model in enumerate(self.models_data, 1):
                f.write(f"### {i}. {model['name']}\n\n")
                f.write(f"- **æ•°æ®æº**: {model['source_file']}\n")
                if model['train_losses']:
                    f.write(f"- **è®­ç»ƒè½®æ•°**: {len(model['train_losses'])}\n")
                    f.write(f"- **æœ€ä½³éªŒè¯æŸå¤±**: {model['best_val_loss']:.4f} (ç¬¬{model['best_epoch']}è½®)\n")
                    f.write(f"- **æœ€ç»ˆè®­ç»ƒæŸå¤±**: {model['final_train_loss']:.4f}\n")
                    f.write(f"- **æœ€ç»ˆéªŒè¯æŸå¤±**: {model['final_val_loss']:.4f}\n")
                else:
                    f.write(f"- **è®­ç»ƒæŸå¤±**: {model['final_train_loss']:.4f}\n")
                    f.write(f"- **éªŒè¯æŸå¤±**: {model['final_val_loss']:.4f}\n")
                f.write("\n")
        
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    parser = argparse.ArgumentParser(description='æ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…·')
    parser.add_argument('--base_dir', type=str, default=None,
                       help='æ¨¡å‹ç›®å½•çš„åŸºç¡€è·¯å¾„ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰')
    parser.add_argument('--models', type=str, nargs='+', default=None,
                       help='æŒ‡å®šè¦åˆ†æçš„æ¨¡å‹ç›®å½•åç§°')
    parser.add_argument('--output_dir', type=str, default='loss_analysis',
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: loss_analysisï¼‰')
    parser.add_argument('--no_plot', action='store_true',
                       help='ä¸ç”Ÿæˆå›¾è¡¨')
    
    args = parser.parse_args()
    
    print("=== æ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…· ===")
    print(f"åŸºç¡€ç›®å½•: {args.base_dir or os.getcwd()}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = BatchLossAnalyzer(args.base_dir)
    
    # ç¡®å®šè¦åˆ†æçš„æ¨¡å‹ç›®å½•
    if args.models:
        # ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
        base_dir = args.base_dir or os.getcwd()
        model_dirs = [os.path.join(base_dir, model) for model in args.models]
        model_dirs = [d for d in model_dirs if os.path.isdir(d)]
        print(f"\nğŸ¯ ç”¨æˆ·æŒ‡å®š {len(model_dirs)} ä¸ªæ¨¡å‹ç›®å½•")
    else:
        # è‡ªåŠ¨æ£€æµ‹
        model_dirs = None
    
    # åˆ†ææ¨¡å‹
    analyzer.analyze_models(model_dirs)
    
    if not analyzer.models_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„æ¨¡å‹æ•°æ®")
        return
    
    # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
    analyzer.create_summary_table(args.output_dir)
    
    # ç”Ÿæˆå›¾è¡¨ï¼ˆå¦‚æœæœ‰å®Œæ•´å†å²æ•°æ®ï¼‰
    if not args.no_plot:
        analyzer.create_loss_comparison_plot(args.output_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_report(args.output_dir)
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")

if __name__ == "__main__":
    main() 