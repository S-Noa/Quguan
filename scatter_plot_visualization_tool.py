#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•£ç‚¹å›¾å…¬å¼åŒ–å·¥å…·
æ”¯æŒäº‘ç«¯ResNet50ã€VGGæ¨¡å‹å’Œæœ¬åœ°åŸºçº¿CNNæ¨¡å‹çš„é¢„æµ‹å€¼vsçœŸå®å€¼å¯è§†åŒ–
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import json
from datetime import datetime
from pathlib import Path
import seaborn as sns
from torch.utils.data import DataLoader
from typing import List, Tuple, Optional, Dict, Any, Union

# æ·»åŠ è·¯å¾„
sys.path.append('src')
sys.path.append('cloud_vgg_training_package')

# è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def safe_collate_fn(batch: List[Any]) -> Tuple[torch.Tensor, torch.Tensor, List[Dict[str, Any]]]:
    """
    å®‰å…¨çš„collateå‡½æ•°ï¼Œå¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
    """
    try:
        # åˆ†ç¦»å›¾åƒã€æµ“åº¦å’Œå…ƒæ•°æ®
        images = []
        concentrations = []
        metadata = []
        
        for item in batch:
            if len(item) == 3:
                image, concentration, meta = item
                images.append(image)
                concentrations.append(concentration)
                
                # ç¡®ä¿å…ƒæ•°æ®åŒ…å«å¿…è¦å­—æ®µ
                if isinstance(meta, dict):
                    # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                    if 'detection_bbox' not in meta:
                        meta['detection_bbox'] = [0, 0, 224, 224]  # é»˜è®¤æ•´ä¸ªå›¾åƒ
                    if 'detection_confidence' not in meta:
                        meta['detection_confidence'] = 1.0
                    metadata.append(meta)
                else:
                    # å¦‚æœmetaä¸æ˜¯å­—å…¸ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
                    metadata.append({
                        'detection_bbox': [0, 0, 224, 224],
                        'detection_confidence': 1.0,
                        'bg_type': 'unknown',
                        'power': 'unknown',
                        'distance': 'unknown'
                    })
            else:
                raise ValueError(f"Unexpected item format: {len(item)} elements")
        
        # ä½¿ç”¨é»˜è®¤çš„collateå‡½æ•°å¤„ç†
        from torch.utils.data.dataloader import default_collate
        images_tensor = default_collate(images)
        concentrations_tensor = default_collate(concentrations)
        
        return images_tensor, concentrations_tensor, metadata
        
    except Exception as e:
        print(f"Collateå‡½æ•°é”™è¯¯: {e}")
        print(f"æ‰¹æ¬¡å¤§å°: {len(batch)}")
        if batch:
            print(f"ç¬¬ä¸€ä¸ªé¡¹ç›®ç±»å‹: {type(batch[0])}")
            print(f"ç¬¬ä¸€ä¸ªé¡¹ç›®é•¿åº¦: {len(batch[0]) if hasattr(batch[0], '__len__') else 'N/A'}")
        raise

class ScatterPlotVisualizer:
    """æ•£ç‚¹å›¾å¯è§†åŒ–å™¨"""
    
    def __init__(self, output_dir: Optional[str] = None) -> None:
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„scatter_plot_analysis_æ—¶é—´æˆ³
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"scatter_plot_analysis_{timestamp}"
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“Š è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _load_checkpoint_safely(self, model_path):
        """å®‰å…¨åŠ è½½checkpointï¼Œå…¼å®¹PyTorch 2.6"""
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨weights_only=Trueï¼ˆå®‰å…¨æ¨¡å¼ï¼‰
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)
        except Exception as e:
            # å¦‚æœå®‰å…¨æ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨weights_only=Falseï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
        return checkpoint
    
    def load_model_and_predict(self, model_type: str, model_path: str, dataset_path: str, bg_mode: str = 'all') -> Tuple[np.ndarray, np.ndarray, str]:
        """
        åŠ è½½æ¨¡å‹å¹¶ç”Ÿæˆé¢„æµ‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('baseline_cnn', 'enhanced_cnn', 'cloud_resnet50', 'cloud_vgg', 'adaptive_resnet50')
            model_path: æ¨¡å‹è·¯å¾„
            dataset_path: æ•°æ®é›†è·¯å¾„
            bg_mode: èƒŒæ™¯æ¨¡å¼ ('all', 'bg0', 'bg1', 'bg0_20mw', 'bg0_100mw', 'bg0_400mw', 'bg1_20mw', 'bg1_100mw', 'bg1_400mw')
        """
        if model_type == 'baseline_cnn':
            return self._load_baseline_cnn(model_path, dataset_path, bg_mode)
        elif model_type == 'enhanced_cnn':
            return self._load_enhanced_cnn(model_path, dataset_path, bg_mode)
        elif model_type == 'cloud_resnet50':
            return self._load_cloud_resnet50(model_path, dataset_path, bg_mode)
        elif model_type == 'cloud_vgg':
            return self._load_cloud_vgg(model_path, dataset_path, bg_mode)
        elif model_type == 'adaptive_resnet50':
            return self._load_adaptive_resnet50(model_path, dataset_path, bg_mode)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    def _load_baseline_cnn(self, model_path: str, dataset_path: str, bg_mode: str) -> Tuple[np.ndarray, np.ndarray, str]:
        """åŠ è½½åŸºçº¿CNNæ¨¡å‹"""
        from cnn_model import CNNFeatureExtractor
        from feature_dataset_loader import create_feature_dataloader
        
        # åŠ è½½æ¨¡å‹
        model = CNNFeatureExtractor()
        checkpoint = self._load_checkpoint_safely(model_path)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(self.device)
        model.eval()
        
        # åŠ è½½æ•°æ®
        bg_filter, power_filter = self._parse_bg_mode(bg_mode)
        _, dataset = create_feature_dataloader(
            feature_dataset_path=dataset_path,
            batch_size=32,
            shuffle=False,
            bg_type=bg_filter,
            power_filter=power_filter
        )
        
        # æ‰‹åŠ¨åˆ›å»ºDataLoaderä½¿ç”¨safe_collate_fn
        dataloader = DataLoader(
            dataset,
            batch_size=32,
            shuffle=False,
            collate_fn=safe_collate_fn
        )
        
        # ç”Ÿæˆé¢„æµ‹
        predictions, targets = self._generate_predictions(model, dataloader)
        
        return predictions, targets, "åŸºçº¿CNN"
    
    def _load_enhanced_cnn(self, model_path: str, dataset_path: str, bg_mode: str) -> Tuple[np.ndarray, np.ndarray, str]:
        """åŠ è½½å¢å¼ºCNNæ¨¡å‹"""
        from enhanced_laser_spot_cnn import EnhancedLaserSpotCNN
        from feature_dataset_loader import create_feature_dataloader
        
        # åŠ è½½æ¨¡å‹
        model = EnhancedLaserSpotCNN(num_classes=1)
        checkpoint = self._load_checkpoint_safely(model_path)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(self.device)
        model.eval()
        
        # åŠ è½½æ•°æ®
        bg_filter, power_filter = self._parse_bg_mode(bg_mode)
        _, dataset = create_feature_dataloader(
            feature_dataset_path=dataset_path,
            batch_size=32,
            shuffle=False,
            bg_type=bg_filter,
            power_filter=power_filter
        )
        
        # æ‰‹åŠ¨åˆ›å»ºDataLoaderä½¿ç”¨safe_collate_fn
        dataloader = DataLoader(
            dataset,
            batch_size=32,
            shuffle=False,
            collate_fn=safe_collate_fn
        )
        
        # ç”Ÿæˆé¢„æµ‹
        predictions, targets = self._generate_predictions(model, dataloader)
        
        return predictions, targets, "å¢å¼ºCNN"
    
    def _load_cloud_resnet50(self, model_path: str, dataset_path: str, bg_mode: str) -> Tuple[np.ndarray, np.ndarray, str]:
        """åŠ è½½äº‘ç«¯ResNet50æ¨¡å‹"""
        from feature_dataset_loader import create_feature_dataloader
        
        # å°è¯•å¯¼å…¥äº‘ç«¯ResNet50å›å½’æ¨¡å‹
        try:
            import sys
            sys.path.append('cloud_vgg_training_package/src')
            from resnet_regression import ResNet50Regression
            
            # åˆ›å»ºæ¨¡å‹ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ¶æ„ï¼‰
            model = ResNet50Regression(freeze_backbone=False, dropout_rate=0.5)
            print("âœ… ä½¿ç”¨äº‘ç«¯ResNet50å›å½’æ¨¡å‹æ¶æ„")
            
        except ImportError:
            print("âš ï¸  æ— æ³•å¯¼å…¥äº‘ç«¯ResNet50æ¨¡å‹ï¼Œå°è¯•æ ‡å‡†ResNet50æ¶æ„")
            import torchvision.models as models
            model = models.resnet50(pretrained=False)
            model.fc = nn.Linear(model.fc.in_features, 1)
        
        # åŠ è½½æƒé‡
        checkpoint = self._load_checkpoint_safely(model_path)
        
        # æ™ºèƒ½åŠ è½½æƒé‡
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        
        # å°è¯•åŠ è½½æƒé‡ï¼Œä½¿ç”¨strict=Falseå¤„ç†å¯èƒ½çš„æ¶æ„ä¸åŒ¹é…
        try:
            model.load_state_dict(state_dict, strict=True)
            print("âœ… ä¸¥æ ¼æ¨¡å¼åŠ è½½æƒé‡æˆåŠŸ")
        except RuntimeError as e:
            print(f"âš ï¸  ä¸¥æ ¼æ¨¡å¼å¤±è´¥: {str(e)[:200]}...")
            try:
                model.load_state_dict(state_dict, strict=False)
                print("âœ… å®½æ¾æ¨¡å¼åŠ è½½æƒé‡æˆåŠŸ")
            except RuntimeError as e2:
                print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e2}")
                raise e2
        
        model.to(self.device)
        model.eval()

        # åŠ è½½æ•°æ®
        bg_filter, power_filter = self._parse_bg_mode(bg_mode)
        _, dataset = create_feature_dataloader(
            feature_dataset_path=dataset_path,
            batch_size=32,
            shuffle=False,
            bg_type=bg_filter,
            power_filter=power_filter
        )
        
        # æ‰‹åŠ¨åˆ›å»ºDataLoaderä½¿ç”¨safe_collate_fn
        dataloader = DataLoader(
            dataset,
            batch_size=32,
            shuffle=False,
            collate_fn=safe_collate_fn
        )
        
        # ç”Ÿæˆé¢„æµ‹
        predictions, targets = self._generate_predictions(model, dataloader)
        
        return predictions, targets, "äº‘ç«¯ResNet50"
    
    def _load_cloud_vgg(self, model_path: str, dataset_path: str, bg_mode: str) -> Tuple[np.ndarray, np.ndarray, str]:
        """åŠ è½½äº‘ç«¯VGGæ¨¡å‹"""
        import sys
        sys.path.append('cloud_vgg_training_package/src')
        from feature_dataset_loader import create_feature_dataloader
        from vgg_regression import VGGRegressionCBAM
        
        # ä½¿ç”¨æ­£ç¡®çš„VGG+CBAMæ¨¡å‹æ¶æ„
        model = VGGRegressionCBAM(
            freeze_features=False,
            debug_mode=False
        )
        
        checkpoint = self._load_checkpoint_safely(model_path)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(self.device)
        model.eval()
        
        # åŠ è½½æ•°æ®
        bg_filter, power_filter = self._parse_bg_mode(bg_mode)
        _, dataset = create_feature_dataloader(
            feature_dataset_path=dataset_path,
            batch_size=32,
            shuffle=False,
            bg_type=bg_filter,
            power_filter=power_filter
        )
        
        # æ‰‹åŠ¨åˆ›å»ºDataLoaderä½¿ç”¨safe_collate_fn
        dataloader = DataLoader(
            dataset,
            batch_size=32,
            shuffle=False,
            collate_fn=safe_collate_fn
        )
        
        # ç”Ÿæˆé¢„æµ‹
        predictions, targets = self._generate_predictions(model, dataloader)
        
        return predictions, targets, "äº‘ç«¯VGG"
    
    def _load_adaptive_resnet50(self, model_path: str, dataset_path: str, bg_mode: str) -> Tuple[np.ndarray, np.ndarray, str]:
        """åŠ è½½è‡ªé€‚åº”ResNet50æ¨¡å‹"""
        from adaptive_attention_resnet50 import AdaptiveAttentionResNet50
        from feature_dataset_loader import create_feature_dataloader
        
        # åŠ è½½æ¨¡å‹
        model = AdaptiveAttentionResNet50(num_classes=1)
        checkpoint = self._load_checkpoint_safely(model_path)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(self.device)
        model.eval()
        
        # åŠ è½½æ•°æ®
        bg_filter, power_filter = self._parse_bg_mode(bg_mode)
        _, dataset = create_feature_dataloader(
            feature_dataset_path=dataset_path,
            batch_size=32,
            shuffle=False,
            bg_type=bg_filter,
            power_filter=power_filter
        )
        
        # æ‰‹åŠ¨åˆ›å»ºDataLoaderä½¿ç”¨safe_collate_fn
        dataloader = DataLoader(
            dataset,
            batch_size=32,
            shuffle=False,
            collate_fn=safe_collate_fn
        )
        
        # ç”Ÿæˆé¢„æµ‹
        predictions, targets = self._generate_predictions(model, dataloader)
        
        return predictions, targets, "è‡ªé€‚åº”ResNet50"
    
    def _parse_bg_mode(self, bg_mode):
        """è§£æèƒŒæ™¯æ¨¡å¼"""
        if '_' in bg_mode:
            parts = bg_mode.split('_')
            bg_filter = parts[0]
            power_filter = parts[1]
        else:
            bg_filter = bg_mode if bg_mode != 'all' else None
            power_filter = None
        
        return bg_filter, power_filter
    
    def _generate_predictions(self, model, dataloader):
        """ç”Ÿæˆé¢„æµ‹å€¼"""
        predictions = []
        targets = []
        
        print(f"   æ­£åœ¨ç”Ÿæˆé¢„æµ‹... (å…±{len(dataloader)}ä¸ªæ‰¹æ¬¡)")
        
        with torch.no_grad():
            for batch_idx, (images, batch_targets, metadata) in enumerate(dataloader):
                images = images.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                outputs = model(images)
                
                # å¤„ç†å¯èƒ½çš„tupleè¾“å‡º
                if isinstance(outputs, tuple):
                    outputs = outputs[0]
                
                predictions.extend(outputs.squeeze().cpu().numpy())
                targets.extend(batch_targets.cpu().numpy())
                
                if (batch_idx + 1) % 50 == 0:
                    print(f"   è¿›åº¦: {batch_idx + 1}/{len(dataloader)}")
        
        return np.array(predictions), np.array(targets)
    
    def create_scatter_plot(self, predictions: np.ndarray, targets: np.ndarray, model_name: str, save_name: Optional[str] = None, min_concentration: Optional[float] = None, max_concentration: Optional[float] = None) -> Tuple[float, float, float]:
        """åˆ›å»ºæ•£ç‚¹å›¾"""
        if save_name is None:
            save_name = f"scatter_plot_{model_name.replace(' ', '_').replace('+', '_')}"

        # åº”ç”¨æµ“åº¦èŒƒå›´ç­›é€‰
        mask: np.ndarray[np.bool_] = np.ones_like(targets, dtype=bool)
        if min_concentration is not None:
            mask &= (targets >= min_concentration)
        if max_concentration is not None:
            mask &= (targets <= max_concentration)

        # åº”ç”¨ç­›é€‰
        filtered_predictions: np.ndarray = predictions[mask]
        filtered_targets: np.ndarray = targets[mask]

        # å¦‚æœæ‰€æœ‰æ•°æ®éƒ½è¢«ç­›é€‰æ‰ï¼Œå‘å‡ºè­¦å‘Šå¹¶ä½¿ç”¨åŸå§‹æ•°æ®
        if len(filtered_targets) == 0:
            print(f"âš ï¸ è­¦å‘Š: æµ“åº¦èŒƒå›´ [{min_concentration}, {max_concentration}] mg/L å†…æ²¡æœ‰æ•°æ®ç‚¹ï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ•°æ®")
            filtered_predictions = predictions
            filtered_targets = targets
        elif len(filtered_targets) < len(targets):
            print(f"ğŸ” å·²ç­›é€‰æµ“åº¦èŒƒå›´: [{min_concentration or min(targets):.2f}, {max_concentration or max(targets):.2f}] mg/L")
            print(f"   åŸå§‹æ•°æ®ç‚¹: {len(targets)}, ç­›é€‰åæ•°æ®ç‚¹: {len(filtered_targets)}")

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        r2 = r2_score(filtered_targets, filtered_predictions)
        mae = mean_absolute_error(targets, predictions)
        mse = mean_squared_error(targets, predictions)
        rmse = np.sqrt(mse)
        
        # è®¡ç®—è¯¯å·®
        errors: np.ndarray = filtered_predictions - filtered_targets

        print(f"\nğŸ“Š {model_name} è¯„ä¼°æŒ‡æ ‡ (ç­›é€‰å):")
        print(f"   RÂ² Score: {r2:.4f}")
        print(f"   MAE: {mae:.2f}")
        print(f"   RMSE: {rmse:.2f}")
        print(f"   æ ·æœ¬æ•°: {len(targets)}")
        
        # åˆ›å»ºå›¾å½¢
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ä¸»æ•£ç‚¹å›¾
        ax1.scatter(filtered_targets, filtered_predictions, alpha=0.6, s=20)

        # ç†æƒ³é¢„æµ‹çº¿
        min_val = min(min(filtered_targets), min(filtered_predictions))
        max_val = max(max(filtered_targets), max(filtered_predictions))

        # æ·»åŠ æµ“åº¦èŒƒå›´æ ‡æ³¨
        if min_concentration is not None or max_concentration is not None:
            range_text = f"æµ“åº¦èŒƒå›´: {min_concentration if min_concentration is not None else 'min'} - {max_concentration if max_concentration is not None else 'max'} mg/L"
            ax1.text(0.05, 0.90, range_text, transform=ax1.transAxes, fontsize=9,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))
        ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='ç†æƒ³é¢„æµ‹')
        
        ax1.set_xlabel('çœŸå®æµ“åº¦ (mg/L)')
        ax1.set_ylabel('é¢„æµ‹æµ“åº¦ (mg/L)')
        ax1.set_title(f'{model_name} - é¢„æµ‹ vs çœŸå®å€¼')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ è¯„ä¼°æŒ‡æ ‡æ–‡æœ¬
        textstr = f'RÂ² = {r2:.4f}\nMAE = {mae:.2f}\nRMSE = {rmse:.2f}\nN = {len(targets)}'
        ax1.text(0.05, 0.95, textstr, transform=ax1.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # 2. è¯¯å·®åˆ†å¸ƒ
        ax2.hist(errors, bins=50, alpha=0.7, edgecolor='black')
        ax2.axvline(0, color='red', linestyle='--', linewidth=2, label='é›¶è¯¯å·®')
        ax2.set_xlabel('é¢„æµ‹è¯¯å·® (mg/L)')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ (ç­›é€‰å)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. æ®‹å·®å›¾
        ax3.scatter(filtered_targets, errors, alpha=0.6, s=20)
        ax3.axhline(0, color='red', linestyle='--', linewidth=2)
        ax3.set_xlabel('çœŸå®æµ“åº¦ (mg/L)')
        ax3.set_ylabel('æ®‹å·® (mg/L)')
        ax3.set_title('æ®‹å·®å›¾ (ç­›é€‰å)')
        ax3.grid(True, alpha=0.3)
        
        # 4. æµ“åº¦åˆ†å¸ƒå¯¹æ¯”
        ax4.hist(filtered_targets, bins=30, alpha=0.7, label='çœŸå®å€¼', density=True)
        ax4.hist(filtered_predictions, bins=30, alpha=0.7, label='é¢„æµ‹å€¼', density=True)
        ax4.set_xlabel('æµ“åº¦ (mg/L)')
        ax4.set_ylabel('å¯†åº¦')
        ax4.set_title('æµ“åº¦åˆ†å¸ƒå¯¹æ¯” (ç­›é€‰å)')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        save_path = os.path.join(self.output_dir, f"{save_name}.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}")
        
        # ä¿å­˜æ•°æ®
        data_path = os.path.join(self.output_dir, f"{save_name}_data.json")
        data = {
            'model_name': model_name,
            'metrics': {
                'r2_score': float(r2),
                'mae': float(mae),
                'mse': float(mse),
                'rmse': float(rmse)
            },
            'sample_count': len(targets),
            'predictions': predictions.tolist(),
            'targets': targets.tolist(),
            'errors': errors.tolist()
        }
        
        with open(data_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"   æ•°æ®å·²ä¿å­˜: {data_path}")
        
        return r2, mae, rmse
    
    def create_comparison_plot(self, model_results: List[Tuple[np.ndarray, np.ndarray, str]]) -> None:
        """åˆ›å»ºæ¨¡å‹å¯¹æ¯”å›¾"""
        if len(model_results) < 2:
            print("âš ï¸ éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹æ‰èƒ½åˆ›å»ºå¯¹æ¯”å›¾")
            return
        
        print(f"\nğŸ“ˆ åˆ›å»ºæ¨¡å‹å¯¹æ¯”å›¾...")
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']
        
        # 1. æ‰€æœ‰æ¨¡å‹çš„æ•£ç‚¹å›¾
        for i, (predictions, targets, model_name) in enumerate(model_results):
            color = colors[i % len(colors)]
            ax1.scatter(targets, predictions, alpha=0.6, s=15, 
                       color=color, label=model_name)
        
        # ç†æƒ³é¢„æµ‹çº¿
        all_targets = np.concatenate([targets for _, targets, _ in model_results])
        all_predictions = np.concatenate([predictions for predictions, _, _ in model_results])
        min_val = min(min(all_targets), min(all_predictions))
        max_val = max(max(all_targets), max(all_predictions))
        ax1.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='ç†æƒ³é¢„æµ‹')
        
        ax1.set_xlabel('çœŸå®æµ“åº¦ (mg/L)')
        ax1.set_ylabel('é¢„æµ‹æµ“åº¦ (mg/L)')
        ax1.set_title('æ¨¡å‹å¯¹æ¯” - é¢„æµ‹ vs çœŸå®å€¼')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”
        model_names = []
        r2_scores = []
        mae_scores = []
        rmse_scores = []
        
        for predictions, targets, model_name in model_results:
            model_names.append(model_name)
            r2_scores.append(r2_score(targets, predictions))
            mae_scores.append(mean_absolute_error(targets, predictions))
            rmse_scores.append(np.sqrt(mean_squared_error(targets, predictions)))
        
        x = np.arange(len(model_names))
        width = 0.25
        
        ax2.bar(x - width, r2_scores, width, label='RÂ² Score', alpha=0.8)
        ax2.set_xlabel('æ¨¡å‹')
        ax2.set_ylabel('RÂ² Score')
        ax2.set_title('RÂ² Score å¯¹æ¯”')
        ax2.set_xticks(x)
        ax2.set_xticklabels(model_names, rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # 3. MAEå¯¹æ¯”
        ax3.bar(x, mae_scores, width, label='MAE', alpha=0.8, color='orange')
        ax3.set_xlabel('æ¨¡å‹')
        ax3.set_ylabel('MAE (mg/L)')
        ax3.set_title('å¹³å‡ç»å¯¹è¯¯å·®å¯¹æ¯”')
        ax3.set_xticks(x)
        ax3.set_xticklabels(model_names, rotation=45)
        ax3.grid(True, alpha=0.3)
        
        # 4. RMSEå¯¹æ¯”
        ax4.bar(x, rmse_scores, width, label='RMSE', alpha=0.8, color='red')
        ax4.set_xlabel('æ¨¡å‹')
        ax4.set_ylabel('RMSE (mg/L)')
        ax4.set_title('å‡æ–¹æ ¹è¯¯å·®å¯¹æ¯”')
        ax4.set_xticks(x)
        ax4.set_xticklabels(model_names, rotation=45)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å¯¹æ¯”å›¾
        comparison_path = os.path.join(self.output_dir, "model_comparison.png")
        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜: {comparison_path}")
        
        # ä¿å­˜å¯¹æ¯”æ•°æ®
        comparison_data = {
            'comparison_summary': {
                'model_names': model_names,
                'r2_scores': r2_scores,
                'mae_scores': mae_scores,
                'rmse_scores': rmse_scores
            },
            'best_models': {
                'highest_r2': model_names[np.argmax(r2_scores)],
                'lowest_mae': model_names[np.argmin(mae_scores)],
                'lowest_rmse': model_names[np.argmin(rmse_scores)]
            }
        }
        
        comparison_data_path = os.path.join(self.output_dir, "model_comparison_data.json")
        with open(comparison_data_path, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)
        
        print(f"   å¯¹æ¯”æ•°æ®å·²ä¿å­˜: {comparison_data_path}")
        
        # æ‰“å°æœ€ä½³æ¨¡å‹
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹:")
        print(f"   æœ€é«˜RÂ²: {comparison_data['best_models']['highest_r2']} (RÂ²={max(r2_scores):.4f})")
        print(f"   æœ€ä½MAE: {comparison_data['best_models']['lowest_mae']} (MAE={min(mae_scores):.2f})")
        print(f"   æœ€ä½RMSE: {comparison_data['best_models']['lowest_rmse']} (RMSE={min(rmse_scores):.2f})")

def main() -> None:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ•£ç‚¹å›¾å…¬å¼åŒ–å·¥å…·')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--output_dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--dataset_path', type=str, default=None,
                       help='ç‰¹å¾æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--bg_mode', type=str, default='all',
                       help='èƒŒæ™¯æ¨¡å¼ (bg0, bg1, all, bg0_20mw, etc.)')

    # æµ“åº¦èŒƒå›´ç­›é€‰
    parser.add_argument('--min_concentration', type=float, default=None,
                       help='æœ€å°æµ“åº¦å€¼ (mg/L)')
    parser.add_argument('--max_concentration', type=float, default=None,
                       help='æœ€å¤§æµ“åº¦å€¼ (mg/L)')

    # æ¨¡å‹é…ç½®
    parser.add_argument('--baseline_cnn_model', type=str, default=None,
                       help='åŸºçº¿CNNæ¨¡å‹è·¯å¾„')
    parser.add_argument('--enhanced_cnn_model', type=str, default=None,
                       help='å¢å¼ºCNNæ¨¡å‹è·¯å¾„')
    parser.add_argument('--cloud_resnet50_model', type=str, default=None,
                       help='äº‘ç«¯ResNet50æ¨¡å‹è·¯å¾„')
    parser.add_argument('--cloud_vgg_model', type=str, default=None,
                       help='äº‘ç«¯VGGæ¨¡å‹è·¯å¾„')
    parser.add_argument('--adaptive_resnet50_model', type=str, default=None,
                       help='è‡ªé€‚åº”ResNet50æ¨¡å‹è·¯å¾„')
    
    # å¯è§†åŒ–é€‰é¡¹
    parser.add_argument('--create_comparison', action='store_true',
                       help='åˆ›å»ºæ¨¡å‹å¯¹æ¯”å›¾')
    parser.add_argument('--models', type=str, nargs='+',
                       choices=['baseline_cnn', 'enhanced_cnn', 'cloud_resnet50', 'cloud_vgg', 'adaptive_resnet50'],
                       help='è¦åˆ†æçš„æ¨¡å‹ç±»å‹')
    
    args = parser.parse_args()
    
    print("ğŸ“Š æ•£ç‚¹å›¾å…¬å¼åŒ–å·¥å…·")
    print("=" * 60)
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = ScatterPlotVisualizer(args.output_dir)
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†
    if args.dataset_path is None:
        sys.path.append('src')
        from feature_dataset_loader import detect_feature_datasets
        datasets = detect_feature_datasets()
        if datasets:
            args.dataset_path = datasets[-1]['path']
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æ•°æ®é›†: {args.dataset_path}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°ç‰¹å¾æ•°æ®é›†ï¼Œè¯·ä½¿ç”¨--dataset_pathæŒ‡å®š")
    
    # æ”¶é›†è¦åˆ†æçš„æ¨¡å‹
    models_to_analyze = []
    model_results = []
    
    if args.models:
        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹åˆ—è¡¨
        for model_type in args.models:
            model_path = getattr(args, f"{model_type}_model")
            if model_path and os.path.exists(model_path):
                models_to_analyze.append((model_type, model_path))
            else:
                print(f"âš ï¸ {model_type}æ¨¡å‹è·¯å¾„æœªæŒ‡å®šæˆ–æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    else:
        # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹
        model_configs = [
            ('baseline_cnn', args.baseline_cnn_model),
            ('enhanced_cnn', args.enhanced_cnn_model),
            ('cloud_resnet50', args.cloud_resnet50_model),
            ('cloud_vgg', args.cloud_vgg_model),
            ('adaptive_resnet50', args.adaptive_resnet50_model)
        ]
        
        for model_type, model_path in model_configs:
            if model_path and os.path.exists(model_path):
                models_to_analyze.append((model_type, model_path))
    
    if not models_to_analyze:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")
        return
    
    print(f"ğŸ“‹ å°†åˆ†æä»¥ä¸‹æ¨¡å‹:")
    for model_type, model_path in models_to_analyze:
        print(f"   {model_type}: {model_path}")
    
    # åˆ†ææ¯ä¸ªæ¨¡å‹
    for model_type, model_path in models_to_analyze:
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ” åˆ†ææ¨¡å‹: {model_type}")
            
            predictions, targets, model_name = visualizer.load_model_and_predict(
                model_type, model_path, args.dataset_path, args.bg_mode
            )
            
            r2, mae, rmse = visualizer.create_scatter_plot(
                predictions, targets, model_name,
                min_concentration=args.min_concentration,
                max_concentration=args.max_concentration
            )
            
            model_results.append((predictions, targets, model_name))
            
        except Exception as e:
            print(f"âŒ åˆ†æ{model_type}æ¨¡å‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # åˆ›å»ºæ¨¡å‹å¯¹æ¯”å›¾
    if args.create_comparison and len(model_results) > 1:
        visualizer.create_comparison_plot(model_results)
    
    print(f"\nğŸ‰ æ•£ç‚¹å›¾åˆ†æå®Œæˆ!")
    print(f"   ç»“æœä¿å­˜åœ¨: {visualizer.output_dir}")

if __name__ == "__main__":
    main()