#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ç‰¹å¾æ•°æ®é›†è®­ç»ƒå›å½’æ¨¡å‹
æ”¯æŒCNNã€VGGç­‰æ¨¡å‹ï¼Œä½¿ç”¨YOLOè£å‰ªçš„ç‰¹å¾åŒºåŸŸè¿›è¡Œè®­ç»ƒ
"""

import os
import sys
import argparse
import glob
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
from datetime import datetime
import json
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import time
import logging

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append('src')

# æ·»åŠ æ•°æ®é›†åç§°å·¥å…·
try:
    from dataset_name_utils import generate_training_output_dir, parse_training_mode_from_args, get_dataset_info_string
    DATASET_UTILS_AVAILABLE = True
    print("æ•°æ®é›†åç§°å·¥å…·åŠ è½½æˆåŠŸ")
except ImportError:
    print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥æ•°æ®é›†åç§°å·¥å…·ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿå‘½åæ–¹å¼")
    DATASET_UTILS_AVAILABLE = False

from feature_dataset_loader import create_feature_dataloader, detect_feature_datasets
from advanced_cnn_models import AdvancedLaserSpotCNN, create_advanced_model
from vgg_regression import VGGRegressionCBAM
from resnet_regression import ResNet50Regression

class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=15, min_delta=1.0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, current_loss, model):
        if self.best_loss is None:
            self.best_loss = current_loss
            self.save_checkpoint(model)
        elif current_loss < self.best_loss - self.min_delta:
            self.best_loss = current_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights and self.best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False
    
    def save_checkpoint(self, model):
        self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}


def safe_collate_fn(batch):
    """
    å®‰å…¨çš„collateå‡½æ•°ï¼Œå¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
    """
    try:
        # åˆ†ç¦»å›¾åƒã€æµ“åº¦å’Œå…ƒæ•°æ®
        images = []
        concentrations = []
        metadata = []
        
        for item in batch:
            if len(item) == 3:
                image, concentration, meta = item
                images.append(image)
                concentrations.append(concentration)
                
                # ç¡®ä¿å…ƒæ•°æ®åŒ…å«å¿…è¦å­—æ®µ
                if isinstance(meta, dict):
                    # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                    if 'detection_bbox' not in meta:
                        meta['detection_bbox'] = [0, 0, 224, 224]  # é»˜è®¤æ•´ä¸ªå›¾åƒ
                    if 'detection_confidence' not in meta:
                        meta['detection_confidence'] = 1.0
                    metadata.append(meta)
                else:
                    # å¦‚æœmetaä¸æ˜¯å­—å…¸ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
                    metadata.append({
                        'detection_bbox': [0, 0, 224, 224],
                        'detection_confidence': 1.0,
                        'bg_type': 'unknown',
                        'power': 'unknown',
                        'distance': 'unknown'
                    })
            else:
                raise ValueError(f"Unexpected item format: {len(item)} elements")
        
        # ä½¿ç”¨é»˜è®¤çš„collateå‡½æ•°å¤„ç†
        from torch.utils.data.dataloader import default_collate
        images_tensor = default_collate(images)
        concentrations_tensor = default_collate(concentrations)
        
        return images_tensor, concentrations_tensor, metadata
        
    except Exception as e:
        print(f"Collateå‡½æ•°é”™è¯¯: {e}")
        print(f"æ‰¹æ¬¡å¤§å°: {len(batch)}")
        if batch:
            print(f"ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(batch[0])}")
            if len(batch[0]) > 0:
                print(f"ç¬¬ä¸€ä¸ªå…ƒç´ å†…å®¹: {batch[0]}")
        raise


def setup_logger(log_file=None, group_tag=None):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    if log_file is None:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        log_file = f'feature_training_{group_tag}_{timestamp}.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger('Feature_Training')
    return logger, log_file

class SmoothAdaptiveWeightedLoss(nn.Module):
    """
    å¹³æ»‘è‡ªé€‚åº”æƒé‡æŸå¤±å‡½æ•°
    è§£å†³ç¡¬é˜ˆå€¼é—®é¢˜ï¼Œä½¿ç”¨Sigmoidå®ç°å¹³æ»‘è¿‡æ¸¡
    """
    
    def __init__(self, transition_concentration=800, beta=0.5, scale_factor=0.01):
        super(SmoothAdaptiveWeightedLoss, self).__init__()
        self.transition_concentration = transition_concentration
        self.beta = beta
        self.scale_factor = scale_factor
        print(f"ğŸ¯ åˆå§‹åŒ–å¹³æ»‘è‡ªé€‚åº”æƒé‡æŸå¤±å‡½æ•°:")
        print(f"   è¿‡æ¸¡æµ“åº¦: {transition_concentration} mg/L")
        print(f"   æœ€å°æƒé‡: {beta}")
        print(f"   å¹³æ»‘å› å­: {scale_factor}")
        
    def forward(self, predictions, targets):
        predictions = predictions.squeeze()
        targets = targets.squeeze()
        
        # è®¡ç®—åŸºç¡€MSEè¯¯å·®
        squared_errors = (predictions - targets) ** 2
        
        # è®¡ç®—å¹³æ»‘æƒé‡
        scale = self.scale_factor * self.transition_concentration
        weights = self.beta + (1 - self.beta) * torch.sigmoid(
            scale * (self.transition_concentration - targets)
        )
        
        # åŠ æƒå¹³å‡
        return torch.mean(weights * squared_errors)

class ProgressiveSmoothWeightedLoss(SmoothAdaptiveWeightedLoss):
    """
    æ¸è¿›å¼å¹³æ»‘æƒé‡æŸå¤±å‡½æ•°
    ä»æ ‡å‡†MSEé€æ¸è¿‡æ¸¡åˆ°å¹³æ»‘æƒé‡
    """
    
    def __init__(self, transition_concentration=800, final_beta=0.5, 
                 warmup_epochs=30, scale_factor=0.01):
        super().__init__(transition_concentration, final_beta, scale_factor)
        self.final_beta = final_beta
        self.warmup_epochs = warmup_epochs
        self.current_epoch = 0
        self.current_beta = 1.0
        print(f"ğŸ¯ åˆå§‹åŒ–æ¸è¿›å¼å¹³æ»‘æƒé‡æŸå¤±å‡½æ•°:")
        print(f"   è¿‡æ¸¡æµ“åº¦: {transition_concentration} mg/L")
        print(f"   æœ€ç»ˆæƒé‡: {final_beta}")
        print(f"   é¢„çƒ­è½®æ¬¡: {warmup_epochs}")
        print(f"   å¹³æ»‘å› å­: {scale_factor}")
        
    def update_epoch(self, epoch):
        self.current_epoch = epoch
        if epoch < self.warmup_epochs:
            progress = epoch / self.warmup_epochs
            self.current_beta = 1.0 - progress * (1.0 - self.final_beta)
        else:
            self.current_beta = self.final_beta
        self.beta = self.current_beta

class ConcentrationAwareLoss(nn.Module):
    """
    æµ“åº¦æ„ŸçŸ¥æŸå¤±å‡½æ•° - ä¼ ç»Ÿç¡¬é˜ˆå€¼ç‰ˆæœ¬
    ä¿ç•™ä½œä¸ºå…¼å®¹é€‰é¡¹
    """
    
    def __init__(self, high_concentration_threshold=800, high_concentration_weight=0.5):
        super(ConcentrationAwareLoss, self).__init__()
        self.threshold = high_concentration_threshold
        self.high_weight = high_concentration_weight
        print(f"ğŸ¯ åˆå§‹åŒ–æµ“åº¦æ„ŸçŸ¥æŸå¤±å‡½æ•°:")
        print(f"   é«˜æµ“åº¦é˜ˆå€¼: {high_concentration_threshold} mg/L")
        print(f"   é«˜æµ“åº¦æŸå¤±æƒé‡: {high_concentration_weight}")
        
    def forward(self, predictions, targets):
        """
        å‰å‘ä¼ æ’­
        Args:
            predictions: æ¨¡å‹é¢„æµ‹å€¼ [batch_size] 
            targets: çœŸå®æµ“åº¦å€¼ [batch_size]
        """
        # ç¡®ä¿è¾“å…¥ä¸º1Då¼ é‡
        predictions = predictions.squeeze()
        targets = targets.squeeze()
        
        # åˆ†ç¦»ä½æµ“åº¦å’Œé«˜æµ“åº¦æ ·æœ¬
        low_mask = targets < self.threshold
        high_mask = targets >= self.threshold
        
        total_loss = 0.0
        loss_components = {}
        
        # ä½æµ“åº¦:ä½¿ç”¨æ ‡å‡†MSEæŸå¤±
        if low_mask.sum() > 0:
            low_preds = predictions[low_mask]
            low_targets = targets[low_mask]
            low_loss = torch.nn.functional.mse_loss(low_preds, low_targets)
            total_loss += low_loss
            loss_components['low_concentration'] = low_loss.item()
        else:
            loss_components['low_concentration'] = 0.0
        
        # é«˜æµ“åº¦:ä½¿ç”¨ç›¸å¯¹MSEæŸå¤±(é™æƒå¤„ç†)
        if high_mask.sum() > 0:
            high_preds = predictions[high_mask]
            high_targets = targets[high_mask]
            
            # è®¡ç®—ç›¸å¯¹è¯¯å·®,é¿å…é™¤é›¶
            relative_errors = (high_preds - high_targets) / torch.clamp(high_targets, min=100.0)
            high_loss = torch.mean(relative_errors ** 2) * self.high_weight
            
            total_loss += high_loss
            loss_components['high_concentration'] = high_loss.item()
        else:
            loss_components['high_concentration'] = 0.0
        
        return total_loss

class FeatureDatasetTrainer:
    """ç‰¹å¾æ•°æ®é›†è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®è¾“å‡ºç›®å½•ï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®é›†åç§°å·¥å…·
        try:
            # å¯¼å…¥æ•°æ®é›†åç§°å·¥å…·
            from dataset_name_utils import generate_training_output_dir, parse_training_mode_from_args
            
            # è§£æè®­ç»ƒæ¨¡å¼
            bg_filter, power_filter, training_mode = parse_training_mode_from_args(args)
            
            # æ·»åŠ å†»ç»“çŠ¶æ€æ ‡è¯†
            freeze_suffix = self._get_freeze_suffix(args)
            model_name_with_suffix = f"{args.model_type}{freeze_suffix}"
            
            # ç”ŸæˆåŸºäºæ•°æ®é›†çš„è¾“å‡ºç›®å½•
            dataset_path = getattr(args, 'feature_dataset_path', None)
            if dataset_path:
                self.output_dir = generate_training_output_dir(
                    model_name=model_name_with_suffix,
                    dataset_path=dataset_path,
                    training_mode=training_mode,
                    bg_filter=bg_filter,
                    power_filter=power_filter
                )
            else:
                # å¦‚æœæ²¡æœ‰æ•°æ®é›†è·¯å¾„ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
                raise ImportError("æ•°æ®é›†è·¯å¾„æœªæŒ‡å®š")
                
            os.makedirs(self.output_dir, exist_ok=True)
            print(f"ä½¿ç”¨åŸºäºæ•°æ®é›†çš„è¾“å‡ºç›®å½•: {self.output_dir}")
            
        except (ImportError, Exception) as e:
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
            print(f"ç”ŸæˆåŸºäºæ•°æ®é›†çš„è¾“å‡ºç›®å½•å¤±è´¥: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            mode_suffix = args.bg_mode.replace('_', '-')
            freeze_suffix = self._get_freeze_suffix(args)
            self.output_dir = f"feature_training_{args.model_type}_{mode_suffix}{freeze_suffix}_results_{timestamp}"
            os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.logger, _ = setup_logger(
            os.path.join(self.output_dir, 'training.log'), 
            f'{args.model_type}_{args.bg_mode.replace("_", "-")}'
        )
        
        self.logger.info(f"{args.model_type.upper()}è®­ç»ƒå™¨åˆå§‹åŒ–")
        self.logger.info(f"   è®¾å¤‡: {self.device}")
        self.logger.info(f"   æ¨¡å‹ç±»å‹: {args.model_type}")
        self.logger.info(f"   è®­ç»ƒæ¨¡å¼: {args.bg_mode.upper()}")
        self.logger.info(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æ·»åŠ æ•°æ®é›†ä¿¡æ¯åˆ°æ—¥å¿—
        try:
            from dataset_name_utils import get_dataset_info_string
            dataset_path = getattr(args, 'feature_dataset_path', None)
            if dataset_path:
                bg_filter, power_filter, _ = parse_training_mode_from_args(args)
                dataset_info = get_dataset_info_string(dataset_path, bg_filter, power_filter)
                self.logger.info(f"   {dataset_info}")
        except:
            pass
    
    def _detect_parameter_changes(self):
        """æ™ºèƒ½æ£€æµ‹æ˜¯å¦éœ€è¦é‡ç½®ä¼˜åŒ–å™¨/è°ƒåº¦å™¨å‚æ•°"""
        self.logger.info("=== æ™ºèƒ½å‚æ•°æ£€æµ‹ ===")
        
        if not self.args.resume:
            self.logger.info("   éæ–­ç‚¹ç»­è®­æ¨¡å¼ï¼Œè·³è¿‡å‚æ•°æ£€æµ‹")
            return
        
        # æ£€æŸ¥æ˜¯å¦æ˜¾å¼æŒ‡å®šäº†é‡ç½®å‚æ•°
        if self.args.reset_optimizer_on_resume or self.args.reset_scheduler_on_resume:
            self.logger.info("   å·²æ˜¾å¼æŒ‡å®šå‚æ•°é‡ç½®æ ‡å¿—ï¼Œè·³è¿‡æ™ºèƒ½æ£€æµ‹")
            return
        
        # æ™ºèƒ½æ£€æµ‹ï¼šå¦‚æœæŒ‡å®šäº†å…³é”®è®­ç»ƒå‚æ•°ï¼Œè‡ªåŠ¨å¯ç”¨é‡ç½®
        parser = argparse.ArgumentParser()
        
        # æ·»åŠ é»˜è®¤å‚æ•°ï¼ˆå¤åˆ¶mainå‡½æ•°ä¸­çš„å‚æ•°å®šä¹‰ï¼‰
        parser.add_argument('--learning_rate', type=float, default=5e-5)
        parser.add_argument('--scheduler', type=str, default='step')
        parser.add_argument('--step_size', type=int, default=50)
        parser.add_argument('--gamma', type=float, default=0.5)
        parser.add_argument('--patience', type=int, default=10)
        parser.add_argument('--weight_decay', type=float, default=1e-3)
        parser.add_argument('--optimizer', type=str, default='adam')
        
        # è§£æé»˜è®¤å€¼
        defaults = parser.parse_args([])
        
        # æ£€æŸ¥å…³é”®å‚æ•°æ˜¯å¦è¢«ä¿®æ”¹
        changed_params = []
        critical_params = ['learning_rate', 'scheduler', 'step_size', 'gamma', 'patience', 'weight_decay', 'optimizer']
        
        self.logger.info("   æ£€æŸ¥å…³é”®å‚æ•°å˜åŒ–:")
        for param in critical_params:
            if hasattr(self.args, param) and hasattr(defaults, param):
                current_val = getattr(self.args, param)
                default_val = getattr(defaults, param)
                if current_val != default_val:
                    changed_params.append(param)
                    self.logger.info(f"     âœ“ {param}: {default_val} -> {current_val}")
                else:
                    self.logger.info(f"     - {param}: {current_val} (é»˜è®¤å€¼)")
        
        # å¦‚æœæœ‰å…³é”®å‚æ•°è¢«ä¿®æ”¹ï¼Œè‡ªåŠ¨å¯ç”¨é‡ç½®
        if changed_params:
            self.logger.info(f"ğŸ” æ£€æµ‹åˆ°è®­ç»ƒå‚æ•°å˜æ›´: {', '.join(changed_params)}")
            self.logger.info("ğŸ’¡ è‡ªåŠ¨å¯ç”¨å‚æ•°é‡ç½®æ¨¡å¼ï¼ˆä¿ç•™æ¨¡å‹æƒé‡ï¼Œé‡ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ï¼‰")
            self.args.reset_optimizer_on_resume = True
            self.args.reset_scheduler_on_resume = True
            self._parameter_changes_detected = changed_params
        else:
            self.logger.info("ğŸ”„ ä½¿ç”¨æ£€æŸ¥ç‚¹åŸå§‹å‚æ•°ç»§ç»­è®­ç»ƒ")
            self._parameter_changes_detected = []
    
    def _get_freeze_suffix(self, args):
        """ç”Ÿæˆå†»ç»“çŠ¶æ€åç¼€ï¼Œç”¨äºåŒºåˆ†è¾“å‡ºç›®å½•"""
        freeze_parts = []
        
        if args.model_type == 'vgg':
            if args.freeze_features:
                freeze_parts.append('frozen')
            else:
                freeze_parts.append('unfrozen')
        elif args.model_type == 'resnet50':
            if args.freeze_backbone:
                freeze_parts.append('frozen')
            else:
                freeze_parts.append('unfrozen')
        elif args.model_type == 'cnn':
            # CNNæ¨¡å‹æ²¡æœ‰é¢„è®­ç»ƒæƒé‡ï¼Œä¸éœ€è¦å†»ç»“æ ‡è¯†
            pass
        
        # æ·»åŠ æ¸è¿›å¼è§£å†»æ ‡è¯†
        if args.progressive_unfreeze:
            freeze_parts.append('progressive')
        
        if freeze_parts:
            return '_' + '_'.join(freeze_parts)
        else:
            return ''
    
    def _parse_training_mode(self):
        """è§£æè®­ç»ƒæ¨¡å¼ï¼Œæ”¯æŒ6æ¡£ç»†åˆ†"""
        bg_mode = self.args.bg_mode
        
        # 6æ¡£ç»†åˆ†æ¨¡å¼
        if '_' in bg_mode:
            # è§£æ bg0_20mw, bg1_100mw ç­‰æ ¼å¼
            parts = bg_mode.split('_')
            if len(parts) == 2:
                bg_filter = parts[0]  # bg0 æˆ– bg1
                power_filter = parts[1]  # 20mw, 100mw, 400mw
                
                self.logger.info(f">>> 6æ¡£ç»†åˆ†è®­ç»ƒæ¨¡å¼: {bg_mode}")
                self.logger.info(f"   å…‰ç…§æ¡ä»¶: {bg_filter}")
                self.logger.info(f"   æ¿€å…‰åŠŸç‡: {power_filter}")
                
                # å¿½ç•¥å‘½ä»¤è¡Œçš„power_filterå‚æ•°
                if self.args.power_filter:
                    self.logger.warning(f"   !!! å¿½ç•¥å‘½ä»¤è¡Œpower_filter: {self.args.power_filter}")
                
                return bg_filter, power_filter
            else:
                raise ValueError(f"æ— æ•ˆçš„ç»†åˆ†æ¨¡å¼æ ¼å¼: {bg_mode}")
        
        # ä¼ ç»Ÿ3æ¡£æ¨¡å¼
        else:
            power_filter = self.args.power_filter  # å¯èƒ½ä¸ºNone
            
            if bg_mode == 'all':
                bg_filter = None
                self.logger.info(f">>> ä¼ ç»Ÿè®­ç»ƒæ¨¡å¼: å…¨éƒ¨æ•°æ®")
            else:
                bg_filter = bg_mode  # bg0 æˆ– bg1
                self.logger.info(f">>> ä¼ ç»Ÿè®­ç»ƒæ¨¡å¼: {bg_filter}")
            
            if power_filter:
                self.logger.info(f"   é¢å¤–åŠŸç‡è¿‡æ»¤: {power_filter}")
            
            return bg_filter, power_filter
    
    def load_data(self):
        """åŠ è½½ç‰¹å¾æ•°æ®é›†"""
        self.logger.info("=== ç¬¬1é˜¶æ®µï¼šåŠ è½½ç‰¹å¾æ•°æ®é›† ===")
        
        # è‡ªåŠ¨æ£€æµ‹ç‰¹å¾æ•°æ®é›†ï¼ˆæ”¯æŒç‰ˆæœ¬é€‰æ‹©ï¼‰
        if self.args.feature_dataset_path is None:
            self.logger.info(f"è‡ªåŠ¨æ£€æµ‹ç‰¹å¾æ•°æ®é›† (ç‰ˆæœ¬: {self.args.dataset_version})...")
            feature_datasets = detect_feature_datasets(version_filter=self.args.dataset_version)
            if not feature_datasets:
                # æ˜¾ç¤ºå¯ç”¨ç‰ˆæœ¬
                available_datasets = detect_feature_datasets()
                if available_datasets:
                    self.logger.info("å¯ç”¨çš„æ•°æ®é›†ç‰ˆæœ¬:")
                    for dataset in available_datasets:
                        exclude_info = f", æ’é™¤: {dataset['exclude_patterns']}" if dataset['exclude_patterns'] else ""
                        self.logger.info(f"  - {dataset['name']} ({dataset['version']}) - {dataset['sample_count']} å¼ {exclude_info}")
                
                raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°ç‰ˆæœ¬ {self.args.dataset_version} çš„ç‰¹å¾æ•°æ®é›†ï¼è¯·å…ˆç”Ÿæˆè¯¥ç‰ˆæœ¬æ•°æ®é›†")
            
            # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ•°æ®é›†
            dataset_info = feature_datasets[0]
            self.args.feature_dataset_path = dataset_info['path']
            
            self.logger.info(f"âœ… é€‰æ‹©æ•°æ®é›†: {dataset_info['name']} ({dataset_info['version']})")
            self.logger.info(f"   è·¯å¾„: {dataset_info['path']}")
            self.logger.info(f"   æ ·æœ¬æ•°é‡: {dataset_info['sample_count']}")
            if dataset_info['exclude_patterns']:
                self.logger.info(f"   æ’é™¤æ¨¡å¼: {dataset_info['exclude_patterns']}")
            if dataset_info['yolo_model_used']:
                self.logger.info(f"   ç”Ÿæˆæ¨¡å‹: {os.path.basename(dataset_info['yolo_model_used'])}")
            
            # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
            self.dataset_info = dataset_info
        else:
            self.logger.info(f"ä½¿ç”¨æŒ‡å®šæ•°æ®é›†: {self.args.feature_dataset_path}")
            self.dataset_info = None
        
        # è§£æç»†åˆ†è®­ç»ƒæ¨¡å¼
        bg_filter, power_filter = self._parse_training_mode()
        
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        self.logger.info("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        full_dataloader, full_dataset = create_feature_dataloader(
            feature_dataset_path=self.args.feature_dataset_path,
            batch_size=self.args.batch_size,
            shuffle=False,  # å…ˆä¸æ‰“ä¹±ï¼Œæ–¹ä¾¿åˆ†å‰²
            bg_type=bg_filter,
            power_filter=power_filter,
            image_size=self.args.image_size
        )
        
        self.logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(full_dataset)}")
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        self.logger.info("=== ç¬¬2é˜¶æ®µï¼šæ•°æ®é›†åˆ†å‰² ===")
        total_size = len(full_dataset)
        train_size = int(total_size * self.args.train_ratio)
        val_size = total_size - train_size
        
        self.logger.info(f"æ•°æ®é›†åˆ†å‰²é…ç½®:")
        self.logger.info(f"   æ€»æ ·æœ¬æ•°: {total_size}")
        self.logger.info(f"   è®­ç»ƒæ¯”ä¾‹: {self.args.train_ratio}")
        self.logger.info(f"   è®­ç»ƒé›†å¤§å°: {train_size}")
        self.logger.info(f"   éªŒè¯é›†å¤§å°: {val_size}")
        
        train_dataset, val_dataset = random_split(
            full_dataset, 
            [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        # åˆ›å»ºDataLoader
        self.logger.info("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True,
            collate_fn=safe_collate_fn
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True,
            collate_fn=safe_collate_fn
        )
        
        self.logger.info(f"DataLoaderåˆ›å»ºå®Œæˆ:")
        self.logger.info(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(self.train_loader)}")
        self.logger.info(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(self.val_loader)}")
        
        # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        self.logger.info("=== ç¬¬3é˜¶æ®µï¼šæ•°æ®é›†ç»Ÿè®¡åˆ†æ ===")
        conc_stats = full_dataset.get_concentration_statistics()
        meta_stats = full_dataset.get_metadata_statistics()
        
        self.logger.info(f"æµ“åº¦ç»Ÿè®¡:")
        self.logger.info(f"   èŒƒå›´: {conc_stats['min']:.1f} - {conc_stats['max']:.1f}")
        self.logger.info(f"   å¹³å‡: {conc_stats['mean']:.2f} Â± {conc_stats['std']:.2f}")
        self.logger.info(f"   ç§ç±»: {len(conc_stats['unique_values'])} ç§")
        
        self.logger.info(f"å…ƒæ•°æ®ç»Ÿè®¡:")
        self.logger.info(f"   èƒŒæ™¯åˆ†å¸ƒ: {meta_stats['bg_types']}")
        self.logger.info(f"   åŠŸç‡åˆ†å¸ƒ: {meta_stats['powers']}")
        
        self.logger.info("=== æ•°æ®åŠ è½½é˜¶æ®µå®Œæˆ ===\n")
        return full_dataset
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        self.logger.info(f"=== ç¬¬4é˜¶æ®µï¼šåˆ›å»º{self.args.model_type.upper()}æ¨¡å‹ ===")
        
        if self.args.model_type == 'cnn':
            self.logger.info("åˆå§‹åŒ–AdvancedLaserSpotCNNæ¨¡å‹...")
            self.model = AdvancedLaserSpotCNN(
                num_features=self.args.hidden_dim,
                use_attention=True,
                use_multiscale=True,
                use_laser_attention=False  # ç‰¹å¾æ•°æ®é›†ä¸Šç¦ç”¨æ¿€å…‰å…‰æ–‘ä¸“ç”¨æ³¨æ„åŠ›
            )
        elif self.args.model_type == 'vgg':
            self.logger.info("åˆå§‹åŒ–VGG+CBAMæ¨¡å‹...")
            self.model = VGGRegressionCBAM(
                freeze_features=self.args.freeze_features,  # æ”¹ä¸ºå¯é…ç½®ï¼Œé»˜è®¤è§£å†»
                debug_mode=False  # ç¦ç”¨è°ƒè¯•æ¨¡å¼
            )
        elif self.args.model_type == 'resnet50':
            self.logger.info("åˆå§‹åŒ–ResNet50å›å½’æ¨¡å‹...")
            self.model = ResNet50Regression(
                freeze_backbone=self.args.freeze_backbone,  # æ”¹ä¸ºå¯é…ç½®ï¼Œé»˜è®¤è§£å†»
                dropout_rate=0.5  # Dropoutæ¯”ç‡
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.args.model_type}")
        
        self.model = self.model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        self.logger.info(f"æ¨¡å‹æ¶æ„åˆ†æ:")
        if self.args.model_type == 'cnn':
            self.logger.info(f"   æ¨¡å‹ç±»å‹: é«˜çº§CNN (æ®‹å·®+æ³¨æ„åŠ›+å¤šå°ºåº¦)")
            self.logger.info(f"   æ¿€å…‰å…‰æ–‘ä¸“ç”¨æ³¨æ„åŠ›: å·²ç¦ç”¨ï¼ˆé€‚é…ç‰¹å¾æ•°æ®é›†ï¼‰")
        elif self.args.model_type == 'vgg':
            self.logger.info(f"   æ¨¡å‹ç±»å‹: VGG16+CBAM (é¢„è®­ç»ƒ+æ³¨æ„åŠ›)")
            freeze_status = "å·²å†»ç»“" if self.args.freeze_features else "å·²è§£å†»"
            self.logger.info(f"   é¢„è®­ç»ƒç‰¹å¾: {freeze_status}ï¼ˆImageNetæƒé‡ï¼‰")
        elif self.args.model_type == 'resnet50':
            self.logger.info(f"   æ¨¡å‹ç±»å‹: ResNet50å›å½’ (é¢„è®­ç»ƒä¸»å¹²+å›å½’å¤´)")
            freeze_status = "å·²å†»ç»“" if self.args.freeze_backbone else "å·²è§£å†»"
            self.logger.info(f"   ä¸»å¹²ç½‘ç»œ: {freeze_status}ï¼ˆImageNetæƒé‡ï¼‰")
            self.logger.info(f"   å›å½’å¤´: ä¸‰å±‚å…¨è¿æ¥+Dropout+ReLU")
        self.logger.info(f"   æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.logger.info(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f} MB")
        
        # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
        self.logger.info("æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        test_input = torch.randn(1, 3, 224, 224).to(self.device)
        with torch.no_grad():
            test_output = self.model(test_input)
        self.logger.info(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        # å¤„ç†tupleè¾“å‡ºçš„æƒ…å†µ
        if isinstance(test_output, tuple):
            self.logger.info(f"   è¾“å‡ºå½¢çŠ¶: {test_output[0].shape} (ä¸»è¾“å‡º)")
            if len(test_output) > 1:
                self.logger.info(f"   æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {test_output[1].shape}")
        else:
            self.logger.info(f"   è¾“å‡ºå½¢çŠ¶: {test_output.shape}")
        
        self.logger.info("=== æ¨¡å‹åˆ›å»ºå®Œæˆ ===\n")
    
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        self.logger.info("=== ç¬¬5é˜¶æ®µï¼šé…ç½®è®­ç»ƒç»„ä»¶ ===")
        
        # æŸå¤±å‡½æ•° - å¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨æµ“åº¦æ„ŸçŸ¥æŸå¤±å‡½æ•°
        use_concentration_aware = not getattr(self.args, 'disable_concentration_aware_loss', False)
        
        if use_concentration_aware:
            threshold = getattr(self.args, 'high_concentration_threshold', 800)
            weight = getattr(self.args, 'high_concentration_weight', 0.5)
            scale = getattr(self.args, 'smooth_scale_factor', 0.01)
            
            # é€‰æ‹©æŸå¤±å‡½æ•°ç±»å‹
            if getattr(self.args, 'progressive_weighting', False):
                self.criterion = ProgressiveSmoothWeightedLoss(
                    transition_concentration=threshold,
                    final_beta=weight,
                    warmup_epochs=getattr(self.args, 'warmup_epochs', 30),
                    scale_factor=scale
                )
                self.logger.info(f"æŸå¤±å‡½æ•°: ProgressiveSmoothWeightedLoss (æ¸è¿›å¼å¹³æ»‘æƒé‡)")
                self.logger.info(f"   â° é¢„çƒ­è½®æ¬¡: {getattr(self.args, 'warmup_epochs', 30)}")
            elif getattr(self.args, 'use_legacy_loss', False):
                # ä½¿ç”¨åŸæœ‰çš„ç¡¬é˜ˆå€¼æŸå¤±å‡½æ•°
                self.criterion = ConcentrationAwareLoss(
                    high_concentration_threshold=threshold,
                    high_concentration_weight=weight
                )
                self.logger.info(f"æŸå¤±å‡½æ•°: ConcentrationAwareLoss (ä¼ ç»Ÿç¡¬é˜ˆå€¼ç‰ˆæœ¬)")
            else:
                # é»˜è®¤ä½¿ç”¨å¹³æ»‘æƒé‡æŸå¤±å‡½æ•°
                self.criterion = SmoothAdaptiveWeightedLoss(
                    transition_concentration=threshold,
                    beta=weight,
                    scale_factor=scale
                )
                self.logger.info(f"æŸå¤±å‡½æ•°: SmoothAdaptiveWeightedLoss (å¹³æ»‘æƒé‡)")
            
            self.logger.info(f"   ğŸ¯ é«˜æµ“åº¦ä¼˜åŒ–: å·²å¯ç”¨æµ“åº¦æ„ŸçŸ¥æŸå¤±å‡½æ•°")
            self.logger.info(f"   ğŸ“Š è¿‡æ¸¡æµ“åº¦: {threshold} mg/L")
            self.logger.info(f"   ğŸ“Š æœ€å°æƒé‡: {weight} (é™æƒ{(1-weight)*100:.0f}%)")
            self.logger.info(f"   ğŸ“Š å¹³æ»‘å› å­: {scale}")
            self.logger.info(f"   ğŸ“ˆ é¢„æœŸæ•ˆæœ: é«˜æµ“åº¦åŒºåŸŸæŸå¤±é™æƒ,ç¼“è§£ç‰¹å¾è¶‹åŒé—®é¢˜")
        else:
            self.criterion = nn.MSELoss()
            self.logger.info(f"æŸå¤±å‡½æ•°: {type(self.criterion).__name__} (æ ‡å‡†MSE)")
            self.logger.info("   âš ï¸  æµ“åº¦æ„ŸçŸ¥ä¼˜åŒ–: å·²ç¦ç”¨,ä½¿ç”¨æ ‡å‡†MSEæŸå¤±")
        
        # ä¼˜åŒ–å™¨
        if self.args.optimizer == 'adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.args.learning_rate,
                weight_decay=self.args.weight_decay
            )
        elif self.args.optimizer == 'sgd':
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=self.args.learning_rate,
                momentum=0.9,
                weight_decay=self.args.weight_decay
            )
        
        self.logger.info(f"ä¼˜åŒ–å™¨é…ç½®:")
        self.logger.info(f"   ç±»å‹: {type(self.optimizer).__name__}")
        self.logger.info(f"   å­¦ä¹ ç‡: {self.args.learning_rate}")
        self.logger.info(f"   æƒé‡è¡°å‡: {self.args.weight_decay}")
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.args.scheduler == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.args.step_size,
                gamma=self.args.gamma
            )
            self.logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: StepLR (æ­¥é•¿={self.args.step_size}, è¡°å‡={self.args.gamma})")
        elif self.args.scheduler == 'plateau':
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=self.args.gamma,
                patience=self.args.patience,
                min_lr=1e-7
            )
            self.logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: ReduceLROnPlateau (è€å¿ƒ={self.args.patience}, è¡°å‡={self.args.gamma})")
        elif self.args.scheduler == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.args.epochs
            )
            self.logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: CosineAnnealingLR (T_max={self.args.epochs})")
        else:
            self.scheduler = None
            self.logger.info("å­¦ä¹ ç‡è°ƒåº¦å™¨: æ— ")
        
        self.logger.info("=== è®­ç»ƒç»„ä»¶é…ç½®å®Œæˆ ===\n")
    
    def _check_disk_space(self, required_mb=500):
        """æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³"""
        import shutil
        try:
            total, used, free = shutil.disk_usage(self.output_dir)
            free_mb = free // (1024 * 1024)
            
            if free_mb < required_mb:
                self.logger.warning(f"âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³: å‰©ä½™ {free_mb} MB, éœ€è¦ {required_mb} MB")
                return False
            
            self.logger.debug(f"ç£ç›˜ç©ºé—´å……è¶³: å‰©ä½™ {free_mb} MB")
            return True
        except Exception as e:
            self.logger.warning(f"æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {e}")
            return True  # é»˜è®¤å…è®¸ä¿å­˜
    
    def _cleanup_old_checkpoints(self, keep_latest=3):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°çš„å‡ ä¸ª"""
        try:
            checkpoint_pattern = os.path.join(self.output_dir, 'checkpoint_epoch_*.pth')
            checkpoints = glob.glob(checkpoint_pattern)
            
            if len(checkpoints) <= keep_latest:
                return
            
            # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„
            checkpoints.sort(key=os.path.getmtime)
            to_delete = checkpoints[:-keep_latest]
            
            deleted_count = 0
            for checkpoint in to_delete:
                try:
                    os.remove(checkpoint)
                    deleted_count += 1
                except Exception as e:
                    self.logger.warning(f"æ— æ³•åˆ é™¤æ£€æŸ¥ç‚¹ {checkpoint}: {e}")
            
            if deleted_count > 0:
                self.logger.info(f"   æ¸…ç†äº† {deleted_count} ä¸ªæ—§æ£€æŸ¥ç‚¹")
                
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥ç‚¹æ¸…ç†å¤±è´¥: {e}")
    
    def save_checkpoint(self, epoch, train_losses, val_losses, val_maes, val_r2s, best_val_loss, best_epoch):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆå«ç£ç›˜ç©ºé—´ç®¡ç†ï¼‰"""
        try:
            # æ£€æŸ¥ç£ç›˜ç©ºé—´
            if not self._check_disk_space(required_mb=500):
                self.logger.warning("   ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå°è¯•æ¸…ç†æ—§æ£€æŸ¥ç‚¹...")
                self._cleanup_old_checkpoints(keep_latest=2)
                
                if not self._check_disk_space(required_mb=200):
                    self.logger.error("   ç£ç›˜ç©ºé—´ä»ç„¶ä¸è¶³ï¼Œè·³è¿‡æ£€æŸ¥ç‚¹ä¿å­˜")
                    return
            
            # å‡†å¤‡æ£€æŸ¥ç‚¹æ•°æ®ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                # åªä¿ç•™æœ€è¿‘20ä¸ªæŸå¤±å€¼ï¼Œå‡å°‘æ–‡ä»¶å¤§å°
                'train_losses': [float(x) for x in train_losses[-20:]],
                'val_losses': [float(x) for x in val_losses[-20:]],
                'val_maes': [float(x) for x in val_maes[-20:]],
                'val_r2s': [float(x) for x in val_r2s[-20:]],
                'best_val_loss': float(best_val_loss),
                'best_epoch': int(best_epoch),
                'args': self.args
            }
            
            # åˆ†æ­¥ä¿å­˜ï¼Œé¿å…å†…å­˜å³°å€¼
            checkpoint_path = os.path.join(self.output_dir, f'checkpoint_epoch_{epoch+1}.pth')
            
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¿è¯åŸå­æ€§å†™å…¥
            temp_path = checkpoint_path + '.tmp'
            torch.save(checkpoint, temp_path)
            
            # åŸå­æ€§é‡å‘½å
            os.rename(temp_path, checkpoint_path)
            self.logger.info(f"   æ£€æŸ¥ç‚¹å·²ä¿å­˜: checkpoint_epoch_{epoch+1}.pth")
            
            # æ›´æ–°æœ€æ–°æ£€æŸ¥ç‚¹ï¼ˆè½¯è¿æ¥æˆ–å¤åˆ¶ï¼‰
            latest_path = os.path.join(self.output_dir, 'checkpoint_latest.pth')
            try:
                if os.path.exists(latest_path):
                    os.remove(latest_path)
                
                # åœ¨Windowsä¸Šä½¿ç”¨å¤åˆ¶è€Œä¸æ˜¯è½¯é“¾æ¥
                import shutil
                shutil.copy2(checkpoint_path, latest_path)
                
            except Exception as e:
                self.logger.warning(f"æ›´æ–°æœ€æ–°æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            
            # å®šæœŸæ¸…ç†æ—§æ£€æŸ¥ç‚¹
            if (epoch + 1) % 5 == 0:
                self._cleanup_old_checkpoints(keep_latest=3)
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            # å¦‚æœæ˜¯ç£ç›˜ç©ºé—´é—®é¢˜ï¼Œå°è¯•ç´§æ€¥æ¸…ç†
            if "No space left on device" in str(e) or "PytorchStreamWriter failed" in str(e):
                self.logger.warning("æ£€æµ‹åˆ°ç£ç›˜ç©ºé—´é—®é¢˜ï¼Œæ‰§è¡Œç´§æ€¥æ¸…ç†...")
                self._cleanup_old_checkpoints(keep_latest=1)
                
                # å°è¯•åªä¿å­˜æ¨¡å‹æƒé‡
                try:
                    model_only_path = os.path.join(self.output_dir, f'model_epoch_{epoch+1}.pth')
                    torch.save(self.model.state_dict(), model_only_path)
                    self.logger.info(f"   ä»…ä¿å­˜æ¨¡å‹æƒé‡: model_epoch_{epoch+1}.pth")
                except Exception as e2:
                    self.logger.error(f"ç´§æ€¥ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")
            else:
                raise
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œæ”¯æŒæ¶æ„å…¼å®¹æ€§å¤„ç†"""
        if os.path.isdir(checkpoint_path):
            # å¦‚æœæ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
            latest_checkpoint = os.path.join(checkpoint_path, 'checkpoint_latest.pth')
            if os.path.exists(latest_checkpoint):
                checkpoint_path = latest_checkpoint
            else:
                # æŸ¥æ‰¾æœ€æ–°çš„epochæ£€æŸ¥ç‚¹
                checkpoints = [f for f in os.listdir(checkpoint_path) if f.startswith('checkpoint_epoch_') and f.endswith('.pth')]
                if checkpoints:
                    checkpoints.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))
                    checkpoint_path = os.path.join(checkpoint_path, checkpoints[-1])
                else:
                    raise FileNotFoundError(f"åœ¨ç›®å½• {checkpoint_path} ä¸­æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
        
        self.logger.info(f"æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        # PyTorch 2.6å…¼å®¹æ€§ä¿®å¤: å¤„ç†weights_onlyé»˜è®¤å€¼å˜æ›´
        try:
            # å°è¯•ä½¿ç”¨æ–°çš„å®‰å…¨åŠ è½½æ¨¡å¼
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        except Exception as e:
            if "Weights only load failed" in str(e) or "argparse.Namespace" in str(e):
                # å¯¹äºåŒ…å«argparse.Namespaceçš„æ£€æŸ¥ç‚¹ï¼Œæ·»åŠ å®‰å…¨å…¨å±€å˜é‡
                import argparse
                torch.serialization.add_safe_globals([argparse.Namespace])
                checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                self.logger.info("   âœ“ ä½¿ç”¨å®‰å…¨å…¨å±€å˜é‡æ¨¡å¼åŠ è½½")
            else:
                # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                raise e
        
        # æ™ºèƒ½åŠ è½½æ¨¡å‹çŠ¶æ€ï¼Œå¤„ç†æ¶æ„ä¸åŒ¹é…
        try:
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.logger.info("   âœ“ å®Œå…¨åŒ¹é…åŠ è½½")
        except RuntimeError as e:
            if "size mismatch" in str(e):
                self.logger.warning("   âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹æ¶æ„ä¸åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŠ è½½...")
                self._load_compatible_state_dict(checkpoint['model_state_dict'])
            else:
                raise e
        
                # æ™ºèƒ½åŠ è½½ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€
        self._smart_load_optimizer_scheduler(checkpoint)
        
        # å…¼å®¹å¤„ç†ï¼šæ£€æŸ¥è®­ç»ƒå†å²å­—æ®µæ˜¯å¦å­˜åœ¨
        checkpoint_type = "å®Œæ•´æ£€æŸ¥ç‚¹" if 'train_losses' in checkpoint else "æœ€ä½³æ¨¡å‹æ–‡ä»¶"
        self.logger.info(f"   æ£€æŸ¥ç‚¹ç±»å‹: {checkpoint_type}")
        
        # ä¸ºç¼ºå°‘çš„è®­ç»ƒå†å²å­—æ®µæä¾›é»˜è®¤å€¼
        for field, default in [
            ('train_losses', []),
            ('val_losses', []),
            ('val_maes', []),
            ('val_r2s', []),
            ('best_val_loss', float('inf')),
            ('best_epoch', 0)
        ]:
            if field not in checkpoint:
                checkpoint[field] = default
                self.logger.warning(f"   âš ï¸ æ£€æŸ¥ç‚¹ä¸­ç¼ºå°‘'{field}'å­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼: {default}")
        
        self.logger.info(f"æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ:")
        self.logger.info(f"   æ¢å¤åˆ°epoch: {checkpoint['epoch'] + 1}")
        self.logger.info(f"   å†å²æœ€ä½³éªŒè¯æŸå¤±: {checkpoint['best_val_loss']}")
        self.logger.info(f"   å†å²æœ€ä½³epoch: {checkpoint['best_epoch'] + 1}")
        
        return checkpoint
    
    def _smart_load_optimizer_scheduler(self, checkpoint):
        """æ™ºèƒ½åŠ è½½ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€"""
        self.logger.info("=== æ™ºèƒ½å‚æ•°åŠ è½½ç­–ç•¥ ===")
        
        # æƒ…å†µ1: æ˜¾å¼è¦æ±‚é‡ç½®ä¼˜åŒ–å™¨
        if self.args.reset_optimizer_on_resume:
            self.logger.info("âœ¨ æ˜¾å¼é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆä½¿ç”¨æ–°å­¦ä¹ ç‡ç­‰å‚æ•°ï¼‰")
            self._reset_optimizer_to_new_params()
        else:
            # æƒ…å†µ2: å°è¯•åŠ è½½æ£€æŸ¥ç‚¹ä¸­çš„ä¼˜åŒ–å™¨çŠ¶æ€
            try:
                if checkpoint.get('optimizer_state_dict'):
                    old_lr = self.optimizer.param_groups[0]['lr']
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    restored_lr = self.optimizer.param_groups[0]['lr']
                    
                    self.logger.info(f"âœ“ ä¼˜åŒ–å™¨çŠ¶æ€å·²ä»æ£€æŸ¥ç‚¹æ¢å¤")
                    self.logger.info(f"   å­¦ä¹ ç‡: {old_lr:.2e} -> {restored_lr:.2e}")
                    
                    # æ˜¾ç¤ºå·²æ¢å¤çš„å‚æ•°
                    self._log_optimizer_state()
                else:
                    self.logger.warning("âš ï¸ æ£€æŸ¥ç‚¹ä¸­æ— ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä½¿ç”¨æ–°å‚æ•°")
                    self._reset_optimizer_to_new_params()
            except Exception as e:
                self.logger.warning(f"âš ï¸ ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ–°å‚æ•°: {str(e)}")
                self._reset_optimizer_to_new_params()

        # æƒ…å†µ1: æ˜¾å¼è¦æ±‚é‡ç½®è°ƒåº¦å™¨  
        if self.args.reset_scheduler_on_resume:
            self.logger.info("âœ¨ æ˜¾å¼é‡ç½®è°ƒåº¦å™¨çŠ¶æ€ï¼ˆä½¿ç”¨æ–°è°ƒåº¦å™¨é…ç½®ï¼‰")
            self._reset_scheduler_to_new_params()
        else:
            # æƒ…å†µ2: å°è¯•åŠ è½½æ£€æŸ¥ç‚¹ä¸­çš„è°ƒåº¦å™¨çŠ¶æ€
            try:
                if self.scheduler and checkpoint.get('scheduler_state_dict'):
                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    self.logger.info("âœ“ è°ƒåº¦å™¨çŠ¶æ€å·²ä»æ£€æŸ¥ç‚¹æ¢å¤")
                    self._log_scheduler_state()
                else:
                    if self.scheduler:
                        self.logger.warning("âš ï¸ æ£€æŸ¥ç‚¹ä¸­æ— è°ƒåº¦å™¨çŠ¶æ€ï¼Œä½¿ç”¨æ–°é…ç½®")
                        self._reset_scheduler_to_new_params()
            except Exception as e:
                self.logger.warning(f"âš ï¸ è°ƒåº¦å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ–°é…ç½®: {str(e)}")
                if self.scheduler:
                    self._reset_scheduler_to_new_params()
        
        self.logger.info("=== å‚æ•°åŠ è½½å®Œæˆ ===")

    def _reset_optimizer_to_new_params(self):
        """é‡ç½®ä¼˜åŒ–å™¨åˆ°æ–°å‚æ•°"""
        if hasattr(self, '_parameter_changes_detected') and self._parameter_changes_detected:
            self.logger.info(f"   åº”ç”¨æ–°å‚æ•°: {', '.join(self._parameter_changes_detected)}")
        
        # é‡æ–°è®¾ç½®å­¦ä¹ ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.args.learning_rate
            if hasattr(self.args, 'weight_decay'):
                param_group['weight_decay'] = self.args.weight_decay
        
        self.logger.info(f"   å­¦ä¹ ç‡è®¾ç½®ä¸º: {self.args.learning_rate:.2e}")
        self.logger.info(f"   æƒé‡è¡°å‡è®¾ç½®ä¸º: {self.args.weight_decay:.2e}")

    def _reset_scheduler_to_new_params(self):
        """é‡ç½®è°ƒåº¦å™¨åˆ°æ–°å‚æ•°"""
        # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨ä»¥ä½¿ç”¨æ–°å‚æ•°
        if self.args.scheduler == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.args.step_size,
                gamma=self.args.gamma
            )
            self.logger.info(f"   StepLRé‡ç½®: step_size={self.args.step_size}, gamma={self.args.gamma}")
        elif self.args.scheduler == 'plateau':
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=self.args.gamma,
                patience=self.args.patience,
                min_lr=1e-7
            )
            self.logger.info(f"   ReduceLROnPlateaué‡ç½®: patience={self.args.patience}, factor={self.args.gamma}")
        elif self.args.scheduler == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.args.epochs
            )
            self.logger.info(f"   CosineAnnealingLRé‡ç½®: T_max={self.args.epochs}")
    
    def _log_optimizer_state(self):
        """è®°å½•ä¼˜åŒ–å™¨çŠ¶æ€"""
        for i, param_group in enumerate(self.optimizer.param_groups):
            self.logger.info(f"   å‚æ•°ç»„{i}: lr={param_group['lr']:.2e}, weight_decay={param_group.get('weight_decay', 0):.2e}")
    
    def _log_scheduler_state(self):
        """è®°å½•è°ƒåº¦å™¨çŠ¶æ€"""
        if self.scheduler:
            scheduler_type = type(self.scheduler).__name__
            self.logger.info(f"   è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")
            if hasattr(self.scheduler, 'last_epoch'):
                self.logger.info(f"   å½“å‰epoch: {self.scheduler.last_epoch}")
            if hasattr(self.scheduler, 'patience'):
                self.logger.info(f"   patience: {self.scheduler.patience}")
    
    def _initialize_progressive_unfreeze(self):
        """åˆå§‹åŒ–æ¸è¿›å¼è§£å†»ç­–ç•¥"""
        self.logger.info("=== åˆå§‹åŒ–æ¸è¿›å¼è§£å†»ç­–ç•¥ ===")
        
        # æ£€æµ‹æ•°æ®é›†ç±»å‹ï¼Œé’ˆå¯¹bg1_400mwè°ƒæ•´ç­–ç•¥
        is_bg1_400mw = hasattr(self.args, 'bg_mode') and 'bg1_400mw' in str(self.args.bg_mode)
        
        # å®šä¹‰è§£å†»é˜¶æ®µ
        total_epochs = self.args.epochs
        
        if is_bg1_400mw:
            # bg1_400mwä¸“ç”¨ç­–ç•¥ï¼šè·³è¿‡ç¬¬ä¸€é˜¶æ®µï¼Œç›´æ¥ä»Layer4è§£å†»å¼€å§‹
            self.unfreeze_stages = {
                'stage1_end': 0,                                 # è·³è¿‡ç¬¬ä¸€é˜¶æ®µ
                'stage2_end': int(total_epochs * 0.4),           # å‰40%è½®æ¬¡  
                'stage3_end': int(total_epochs * 0.7),           # å‰70%è½®æ¬¡
                'stage4_end': total_epochs                       # å…¨éƒ¨è½®æ¬¡
            }
            self.logger.info("ğŸ¯ æ£€æµ‹åˆ°bg1_400mwæ•°æ®é›†ï¼Œä½¿ç”¨ä¸“ç”¨æ¸è¿›å¼è§£å†»ç­–ç•¥ï¼ˆè·³è¿‡å›å½’å¤´é˜¶æ®µï¼‰")
        else:
            # æ ‡å‡†ç­–ç•¥
            self.unfreeze_stages = {
                'stage1_end': int(total_epochs * 0.2),  # å‰20%è½®æ¬¡
                'stage2_end': int(total_epochs * 0.4),  # å‰40%è½®æ¬¡  
                'stage3_end': int(total_epochs * 0.6),  # å‰60%è½®æ¬¡
                'stage4_end': total_epochs              # å…¨éƒ¨è½®æ¬¡
            }
            self.logger.info("ğŸ“‹ ä½¿ç”¨æ ‡å‡†æ¸è¿›å¼è§£å†»ç­–ç•¥")
        
        self.logger.info(f"æ¸è¿›å¼è§£å†»è®¡åˆ’:")
        if is_bg1_400mw:
            self.logger.info(f"   é˜¶æ®µ1: è·³è¿‡ï¼ˆé¿å…å›å½’å¤´ä¸æ”¶æ•›ï¼‰")
            self.logger.info(f"   é˜¶æ®µ2 (è½®æ¬¡1-{self.unfreeze_stages['stage2_end']}): è§£å†»Layer4")
            self.logger.info(f"   é˜¶æ®µ3 (è½®æ¬¡{self.unfreeze_stages['stage2_end']+1}-{self.unfreeze_stages['stage3_end']}): è§£å†»Layer3-4")
            self.logger.info(f"   é˜¶æ®µ4 (è½®æ¬¡{self.unfreeze_stages['stage3_end']+1}-{self.unfreeze_stages['stage4_end']}): å…¨éƒ¨è§£å†»")
        else:
            self.logger.info(f"   é˜¶æ®µ1 (è½®æ¬¡1-{self.unfreeze_stages['stage1_end']}): åªè®­ç»ƒå›å½’å¤´")
            self.logger.info(f"   é˜¶æ®µ2 (è½®æ¬¡{self.unfreeze_stages['stage1_end']+1}-{self.unfreeze_stages['stage2_end']}): è§£å†»Layer4")
            self.logger.info(f"   é˜¶æ®µ3 (è½®æ¬¡{self.unfreeze_stages['stage2_end']+1}-{self.unfreeze_stages['stage3_end']}): è§£å†»Layer3-4")
            self.logger.info(f"   é˜¶æ®µ4 (è½®æ¬¡{self.unfreeze_stages['stage3_end']+1}-{self.unfreeze_stages['stage4_end']}): å…¨éƒ¨è§£å†»")
        
        # è®°å½•åˆå§‹å­¦ä¹ ç‡å’Œæ•°æ®é›†ç±»å‹
        self.initial_lr = self.optimizer.param_groups[0]['lr']
        self.current_stage = 0
        self.is_bg1_400mw = is_bg1_400mw
        
        # æ ¹æ®ç­–ç•¥è®¾ç½®åˆå§‹çŠ¶æ€
        if is_bg1_400mw:
            # bg1_400mwç›´æ¥ä»é˜¶æ®µ2å¼€å§‹ï¼Œè§£å†»Layer4
            self._unfreeze_layer4()
            self.current_stage = 2  # ç›´æ¥è®¾ä¸ºé˜¶æ®µ2
            self.logger.info("ğŸš€ bg1_400mwç­–ç•¥ï¼šç›´æ¥ä»é˜¶æ®µ2å¼€å§‹ï¼ˆLayer4è§£å†»ï¼‰")
        else:
            # æ ‡å‡†ç­–ç•¥ï¼šç¡®ä¿å¼€å§‹æ—¶åªæœ‰å›å½’å¤´å¯è®­ç»ƒ
            self._freeze_all_backbone()
            
        self.logger.info("=== æ¸è¿›å¼è§£å†»åˆå§‹åŒ–å®Œæˆ ===")
    
    def _freeze_all_backbone(self):
        """å†»ç»“æ‰€æœ‰backboneå±‚"""
        if hasattr(self.model, 'backbone'):
            for param in self.model.backbone.parameters():
                param.requires_grad = False
        self._log_trainable_params("å†»ç»“æ‰€æœ‰backboneå")
    
    def _apply_progressive_unfreeze(self, epoch):
        """åº”ç”¨æ¸è¿›å¼è§£å†»ç­–ç•¥"""
        new_stage = self._get_current_stage(epoch)
        
        if new_stage != self.current_stage:
            self.logger.info(f"=== æ¸è¿›å¼è§£å†»ï¼šè¿›å…¥é˜¶æ®µ{new_stage} ===")
            
            if new_stage == 1:
                # é˜¶æ®µ1ï¼šåªè®­ç»ƒå›å½’å¤´ï¼ˆå·²ç»è®¾ç½®ï¼‰
                self._freeze_all_backbone()
                self._adjust_learning_rate(1.0)
                self.logger.info("   ç­–ç•¥ï¼šåªè®­ç»ƒå›å½’å¤´")
                
            elif new_stage == 2:
                # é˜¶æ®µ2ï¼šè§£å†»Layer4
                self._unfreeze_layer4()
                self._adjust_learning_rate(0.5)
                self.logger.info("   ç­–ç•¥ï¼šè§£å†»Layer4ï¼Œå­¦ä¹ ç‡å‡åŠ")
                
            elif new_stage == 3:
                # é˜¶æ®µ3ï¼šè§£å†»Layer3-4
                self._unfreeze_layer3_4()
                self._adjust_learning_rate(0.3)
                self.logger.info("   ç­–ç•¥ï¼šè§£å†»Layer3-4ï¼Œå­¦ä¹ ç‡é™è‡³30%")
                
            elif new_stage == 4:
                # é˜¶æ®µ4ï¼šå…¨éƒ¨è§£å†»
                self._unfreeze_all_layers()
                self._adjust_learning_rate(0.1)
                self.logger.info("   ç­–ç•¥ï¼šå…¨éƒ¨è§£å†»ï¼Œå­¦ä¹ ç‡é™è‡³10%")
            
            self.current_stage = new_stage
            self.logger.info("=== æ¸è¿›å¼è§£å†»é˜¶æ®µåˆ‡æ¢å®Œæˆ ===")
    
    def _get_current_stage(self, epoch):
        """è·å–å½“å‰åº”è¯¥å¤„äºçš„é˜¶æ®µ"""
        # bg1_400mwè·³è¿‡é˜¶æ®µ1
        if hasattr(self, 'is_bg1_400mw') and self.is_bg1_400mw:
            if epoch < self.unfreeze_stages['stage2_end']:
                return 2  # ç›´æ¥ä»é˜¶æ®µ2å¼€å§‹
            elif epoch < self.unfreeze_stages['stage3_end']:
                return 3
            else:
                return 4
        else:
            # æ ‡å‡†ç­–ç•¥
            if epoch < self.unfreeze_stages['stage1_end']:
                return 1
            elif epoch < self.unfreeze_stages['stage2_end']:
                return 2
            elif epoch < self.unfreeze_stages['stage3_end']:
                return 3
            else:
                return 4
    
    def _unfreeze_layer4(self):
        """è§£å†»ResNet50çš„Layer4"""
        if hasattr(self.model, 'backbone') and hasattr(self.model.backbone, 'layer4'):
            for param in self.model.backbone.layer4.parameters():
                param.requires_grad = True
        self._log_trainable_params("è§£å†»Layer4å")
    
    def _unfreeze_layer3_4(self):
        """è§£å†»ResNet50çš„Layer3å’ŒLayer4"""
        if hasattr(self.model, 'backbone'):
            if hasattr(self.model.backbone, 'layer3'):
                for param in self.model.backbone.layer3.parameters():
                    param.requires_grad = True
            if hasattr(self.model.backbone, 'layer4'):
                for param in self.model.backbone.layer4.parameters():
                    param.requires_grad = True
        self._log_trainable_params("è§£å†»Layer3-4å")
    
    def _unfreeze_all_layers(self):
        """è§£å†»æ‰€æœ‰å±‚"""
        if hasattr(self.model, 'backbone'):
            for param in self.model.backbone.parameters():
                param.requires_grad = True
        self._log_trainable_params("å…¨éƒ¨è§£å†»å")
    
    def _adjust_learning_rate(self, factor):
        """è°ƒæ•´å­¦ä¹ ç‡"""
        # é’ˆå¯¹bg1_400mwä½¿ç”¨æ›´æ¸©å’Œçš„å­¦ä¹ ç‡è°ƒæ•´
        if hasattr(self, 'is_bg1_400mw') and self.is_bg1_400mw:
            # bg1_400mwä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ ç‡è¡°å‡
            adjusted_factor = max(factor, 0.3)  # æœ€ä½ä¸ä½äº30%
            new_lr = self.initial_lr * adjusted_factor
        else:
            new_lr = self.initial_lr * factor
            
        old_lr = self.optimizer.param_groups[0]['lr']
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = new_lr
        
        dataset_info = " (bg1_400mwä¼˜åŒ–)" if hasattr(self, 'is_bg1_400mw') and self.is_bg1_400mw else ""
        self.logger.info(f"   å­¦ä¹ ç‡è°ƒæ•´: {old_lr:.6f} -> {new_lr:.6f} (å› å­: {factor}){dataset_info}")
    
    def _log_trainable_params(self, context=""):
        """è®°å½•å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        frozen_params = total_params - trainable_params
        
        self.logger.info(f"   {context}å‚æ•°ç»Ÿè®¡:")
        self.logger.info(f"     æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"     å¯è®­ç»ƒ: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
        self.logger.info(f"     å†»ç»“: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)")
    
    def _load_compatible_state_dict(self, checkpoint_state_dict):
        """å…¼å®¹æ€§åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸"""
        model_state_dict = self.model.state_dict()
        loaded_keys = []
        skipped_keys = []
        
        for key, value in checkpoint_state_dict.items():
            if key in model_state_dict:
                if model_state_dict[key].shape == value.shape:
                    model_state_dict[key] = value
                    loaded_keys.append(key)
                else:
                    skipped_keys.append(f"{key} (å½¢çŠ¶ä¸åŒ¹é…: {value.shape} -> {model_state_dict[key].shape})")
            else:
                skipped_keys.append(f"{key} (å½“å‰æ¨¡å‹ä¸­ä¸å­˜åœ¨)")
        
        # åŠ è½½å…¼å®¹çš„å‚æ•°
        self.model.load_state_dict(model_state_dict)
        
        self.logger.info(f"   å…¼å®¹æ€§åŠ è½½ç»Ÿè®¡:")
        self.logger.info(f"     æˆåŠŸåŠ è½½: {len(loaded_keys)} ä¸ªå‚æ•°")
        self.logger.info(f"     è·³è¿‡å‚æ•°: {len(skipped_keys)} ä¸ª")
        
        if skipped_keys:
            self.logger.warning("   è·³è¿‡çš„å‚æ•°:")
            for key in skipped_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                self.logger.warning(f"     - {key}")
            if len(skipped_keys) > 5:
                self.logger.warning(f"     ... åŠå…¶ä»– {len(skipped_keys) - 5} ä¸ªå‚æ•°")
        
        # è®¡ç®—åŠ è½½æ¯”ä¾‹
        load_ratio = len(loaded_keys) / len(checkpoint_state_dict) * 100
        self.logger.info(f"   å‚æ•°åŠ è½½æ¯”ä¾‹: {load_ratio:.1f}%")
        
        if load_ratio < 50:
            self.logger.error("   âŒ åŠ è½½æ¯”ä¾‹è¿‡ä½ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
            raise RuntimeError("æ£€æŸ¥ç‚¹å…¼å®¹æ€§å¤ªå·®ï¼Œå»ºè®®ä»å¤´å¼€å§‹è®­ç»ƒ")
        elif load_ratio < 80:
            self.logger.warning("   âš ï¸ åŠ è½½æ¯”ä¾‹è¾ƒä½ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡")
        else:
            self.logger.info("   âœ“ åŠ è½½æ¯”ä¾‹è‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        # æ›´æ–°æ¸è¿›å¼æƒé‡æŸå¤±å‡½æ•°çš„epoch
        if hasattr(self.criterion, 'update_epoch'):
            self.criterion.update_epoch(epoch)
        
        total_loss = 0.0
        num_batches = 0
        
        # è¿›åº¦ç›‘æ§
        total_batches = len(self.train_loader)
        log_interval = max(1, total_batches // 10)  # æ¯10%è¾“å‡ºä¸€æ¬¡
        
        for batch_idx, (images, targets, metadata) in enumerate(self.train_loader):
            images = images.to(self.device)
            targets = targets.to(self.device).float()
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(images)
            # å¤„ç†æ¨¡å‹è¾“å‡ºï¼ˆå¯èƒ½æ˜¯tupleï¼‰
            if isinstance(outputs, tuple):
                outputs = outputs[0]  # å–ä¸»è¾“å‡º
            
            # é’ˆå¯¹ä¸åŒæ¨¡å‹è°ƒæ•´ç›®æ ‡å€¼å½¢çŠ¶ä»¥é¿å…å¹¿æ’­è­¦å‘Š
            if self.args.model_type == 'resnet50':
                # ResNet50è¾“å‡ºæ ‡é‡ï¼Œç›®æ ‡å€¼ä¹Ÿåº”ä¸ºæ ‡é‡
                loss = self.criterion(outputs.squeeze(), targets.squeeze())
            else:
                # å…¶ä»–æ¨¡å‹ä¿æŒåŸæœ‰å¤„ç†æ–¹å¼
                loss = self.criterion(outputs.squeeze(), targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # è¯¦ç»†è¿›åº¦è¾“å‡º
            if batch_idx % log_interval == 0:
                progress = (batch_idx + 1) / total_batches * 100
                avg_loss = total_loss / num_batches
                self.logger.info(f"   è®­ç»ƒè¿›åº¦: {batch_idx+1}/{total_batches} ({progress:.1f}%) - "
                               f"å½“å‰æŸå¤±: {loss.item():.6f}, å¹³å‡æŸå¤±: {avg_loss:.6f}")
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        self.logger.info("   å¼€å§‹éªŒè¯...")
        self.model.eval()
        total_loss = 0.0
        predictions = []
        targets = []
        
        val_batches = len(self.val_loader)
        
        with torch.no_grad():
            for batch_idx, (images, batch_targets, metadata) in enumerate(self.val_loader):
                images = images.to(self.device)
                batch_targets = batch_targets.to(self.device).float()
                
                outputs = self.model(images)
                # å¤„ç†æ¨¡å‹è¾“å‡ºï¼ˆå¯èƒ½æ˜¯tupleï¼‰
                if isinstance(outputs, tuple):
                    outputs = outputs[0]  # å–ä¸»è¾“å‡º
                
                # é’ˆå¯¹ä¸åŒæ¨¡å‹è°ƒæ•´ç›®æ ‡å€¼å½¢çŠ¶ä»¥é¿å…å¹¿æ’­è­¦å‘Š
                if self.args.model_type == 'resnet50':
                    # ResNet50è¾“å‡ºæ ‡é‡ï¼Œç›®æ ‡å€¼ä¹Ÿåº”ä¸ºæ ‡é‡
                    loss = self.criterion(outputs.squeeze(), batch_targets.squeeze())
                else:
                    # å…¶ä»–æ¨¡å‹ä¿æŒåŸæœ‰å¤„ç†æ–¹å¼
                    loss = self.criterion(outputs.squeeze(), batch_targets)
                
                total_loss += loss.item()
                
                # å®‰å…¨å¤„ç†é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ï¼Œé¿å…0ç»´æ•°ç»„é—®é¢˜
                pred_numpy = outputs.squeeze().cpu().numpy()
                target_numpy = batch_targets.cpu().numpy()
                
                # ç¡®ä¿æ˜¯1ç»´æ•°ç»„ï¼Œå³ä½¿batch_size=1
                if pred_numpy.ndim == 0:
                    pred_numpy = np.array([pred_numpy])
                if target_numpy.ndim == 0:
                    target_numpy = np.array([target_numpy])
                    
                predictions.extend(pred_numpy)
                targets.extend(target_numpy)
                
                # éªŒè¯è¿›åº¦
                if batch_idx % max(1, val_batches // 5) == 0:
                    progress = (batch_idx + 1) / val_batches * 100
                    self.logger.info(f"   éªŒè¯è¿›åº¦: {batch_idx+1}/{val_batches} ({progress:.1f}%)")
        
        avg_loss = total_loss / len(self.val_loader)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        predictions = np.array(predictions)
        targets = np.array(targets)
        
        mse = mean_squared_error(targets, predictions)
        mae = mean_absolute_error(targets, predictions)
        r2 = r2_score(targets, predictions)
        
        self.logger.info(f"   éªŒè¯å®Œæˆ: æŸå¤±={avg_loss:.6f}, MAE={mae:.4f}, RÂ²={r2:.4f}")
        
        return avg_loss, mse, mae, r2, predictions, targets
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        self.logger.info("=== ç¬¬6é˜¶æ®µï¼šå¼€å§‹æ¨¡å‹è®­ç»ƒ ===")
        self.logger.info(f"è®­ç»ƒé…ç½®:")
        self.logger.info(f"   æ€»è½®æ¬¡: {self.args.epochs}")
        self.logger.info(f"   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
        self.logger.info(f"   è®­ç»ƒæ‰¹æ¬¡: {len(self.train_loader)}")
        self.logger.info(f"   éªŒè¯æ‰¹æ¬¡: {len(self.val_loader)}")
        
        # åˆå§‹åŒ–æ¸è¿›å¼è§£å†»
        if self.args.progressive_unfreeze and self.args.model_type == 'resnet50':
            self._initialize_progressive_unfreeze()
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        val_maes = []
        val_r2s = []
        
        best_val_loss = float('inf')
        best_epoch = 0
        start_epoch = 0
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        if self.args.resume:
            checkpoint = self.load_checkpoint(self.args.resume)
            start_epoch = checkpoint['epoch'] + 1
            train_losses = checkpoint['train_losses']
            val_losses = checkpoint['val_losses']
            val_maes = checkpoint['val_maes']
            val_r2s = checkpoint['val_r2s']
            best_val_loss = checkpoint['best_val_loss']
            best_epoch = checkpoint['best_epoch']
            self.logger.info(f"ä»ç¬¬ {start_epoch + 1} è½®æ¬¡ç»§ç»­è®­ç»ƒ")
        
        training_start_time = time.time()
        
        for epoch in range(start_epoch, self.args.epochs):
            epoch_start_time = time.time()
            self.logger.info(f"\n--- è½®æ¬¡ {epoch+1}/{self.args.epochs} ---")
            
            # æ‰§è¡Œæ¸è¿›å¼è§£å†»ç­–ç•¥
            if self.args.progressive_unfreeze and self.args.model_type == 'resnet50':
                self._apply_progressive_unfreeze(epoch)
            
            # è®­ç»ƒ
            self.logger.info("è®­ç»ƒé˜¶æ®µ:")
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            self.logger.info("éªŒè¯é˜¶æ®µ:")
            val_loss, val_mse, val_mae, val_r2, predictions, targets = self.validate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                old_lr = self.optimizer.param_groups[0]['lr']
                
                # æ ¹æ®è°ƒåº¦å™¨ç±»å‹ä½¿ç”¨ä¸åŒçš„stepæ–¹æ³•
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)  # ReduceLROnPlateauéœ€è¦ä¼ å…¥æŸå¤±å€¼
                else:
                    self.scheduler.step()  # å…¶ä»–è°ƒåº¦å™¨ä¸éœ€è¦å‚æ•°
                    
                new_lr = self.optimizer.param_groups[0]['lr']
                if old_lr != new_lr:
                    self.logger.info(f"   å­¦ä¹ ç‡æ›´æ–°: {old_lr:.6f} -> {new_lr:.6f}")
                
                # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å°
                if new_lr < 1e-7:
                    self.logger.warning(f"   âš ï¸ å­¦ä¹ ç‡è¿‡å° ({new_lr:.2e})ï¼Œè®­ç»ƒå¯èƒ½åœæ»")
            
            # è®°å½•å†å²
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            val_maes.append(val_mae)
            val_r2s.append(val_r2)
            
            # è½®æ¬¡æ€»ç»“
            epoch_time = time.time() - epoch_start_time
            current_lr = self.optimizer.param_groups[0]['lr']
            
            self.logger.info(f"è½®æ¬¡ {epoch+1} æ€»ç»“:")
            self.logger.info(f"   è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            self.logger.info(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
            self.logger.info(f"   éªŒè¯MAE: {val_mae:.4f}")
            self.logger.info(f"   éªŒè¯RÂ²: {val_r2:.4f}")
            self.logger.info(f"   å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")
            self.logger.info(f"   è½®æ¬¡è€—æ—¶: {epoch_time:.1f}s")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_epoch = epoch
                
                # ä¿å­˜å®Œæ•´çš„æ£€æŸ¥ç‚¹æ ¼å¼ï¼Œç¡®ä¿å…¼å®¹æ€§
                best_model_checkpoint = {
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                    'epoch': epoch,
                    'best_val_loss': best_val_loss,
                    'best_epoch': best_epoch,
                    'args': self.args
                }
                
                torch.save(best_model_checkpoint, 
                          os.path.join(self.output_dir, 'best_model.pth'))
                self.logger.info(f"   âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.6f})")
            else:
                improvement = val_loss - best_val_loss
                self.logger.info(f"   å½“å‰æ¨¡å‹æ¯”æœ€ä½³æ¨¡å‹å·® {improvement:.6f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.args.save_checkpoint_every == 0:
                self.save_checkpoint(epoch, train_losses, val_losses, val_maes, val_r2s, best_val_loss, best_epoch)
        
        total_time = time.time() - training_start_time
        self.logger.info(f"\n=== è®­ç»ƒå®Œæˆ ===")
        self.logger.info(f"è®­ç»ƒæ€»ç»“:")
        self.logger.info(f"   æœ€ä½³è½®æ¬¡: {best_epoch+1}")
        self.logger.info(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        self.logger.info(f"   æœ€ç»ˆéªŒè¯RÂ²: {val_r2s[-1]:.4f}")
        self.logger.info(f"   æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
        self.logger.info(f"   å¹³å‡æ¯è½®æ—¶é—´: {total_time/self.args.epochs:.1f}s")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history = {
            'train_losses': [float(x) for x in train_losses],  # è½¬æ¢numpy float32ä¸ºPython float
            'val_losses': [float(x) for x in val_losses],
            'val_maes': [float(x) for x in val_maes],
            'val_r2s': [float(x) for x in val_r2s],
            'best_epoch': int(best_epoch),
            'best_val_loss': float(best_val_loss),
            'total_training_time': float(total_time)
        }
        
        with open(os.path.join(self.output_dir, 'training_history.json'), 'w') as f:
            json.dump(history, f, indent=2)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(history)
        
        return history
    
    def plot_training_curves(self, history):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        epochs = range(1, len(history['train_losses']) + 1)
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(epochs, history['train_losses'], 'b-', label='è®­ç»ƒæŸå¤±')
        axes[0, 0].plot(epochs, history['val_losses'], 'r-', label='éªŒè¯æŸå¤±')
        axes[0, 0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        axes[0, 0].set_xlabel('è½®æ¬¡')
        axes[0, 0].set_ylabel('æŸå¤±')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # MAEæ›²çº¿
        axes[0, 1].plot(epochs, history['val_maes'], 'g-', label='éªŒè¯MAE')
        axes[0, 1].set_title('éªŒè¯å¹³å‡ç»å¯¹è¯¯å·®')
        axes[0, 1].set_xlabel('è½®æ¬¡')
        axes[0, 1].set_ylabel('MAE')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # RÂ²æ›²çº¿
        axes[1, 0].plot(epochs, history['val_r2s'], 'm-', label='éªŒè¯RÂ²')
        axes[1, 0].set_title('éªŒè¯å†³å®šç³»æ•°')
        axes[1, 0].set_xlabel('è½®æ¬¡')
        axes[1, 0].set_ylabel('RÂ²')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # æœ€åä¸€æ¬¡éªŒè¯çš„é¢„æµ‹vsçœŸå®å€¼
        # é‡æ–°åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        best_model_path = os.path.join(self.output_dir, 'best_model.pth')
        if os.path.exists(best_model_path):
            try:
                # PyTorch 2.6å…¼å®¹æ€§ä¿®å¤: å¤„ç†weights_onlyé»˜è®¤å€¼å˜æ›´
                try:
                    # å°è¯•ä½¿ç”¨æ–°çš„å®‰å…¨åŠ è½½æ¨¡å¼
                    best_checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
                except Exception as e:
                    if "Weights only load failed" in str(e) or "argparse.Namespace" in str(e):
                        # å¯¹äºåŒ…å«argparse.Namespaceçš„æ£€æŸ¥ç‚¹ï¼Œæ·»åŠ å®‰å…¨å…¨å±€å˜é‡
                        import argparse
                        torch.serialization.add_safe_globals([argparse.Namespace])
                        best_checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=True)
                        self.logger.info("   âœ“ ä½¿ç”¨å®‰å…¨å…¨å±€å˜é‡æ¨¡å¼åŠ è½½æœ€ä½³æ¨¡å‹")
                    else:
                        # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                        raise e
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ ¼å¼ï¼ˆåŒ…å«'model_state_dict'é”®ï¼‰
                if isinstance(best_checkpoint, dict) and 'model_state_dict' in best_checkpoint:
                    self.model.load_state_dict(best_checkpoint['model_state_dict'])
                else:
                    # æ—§æ ¼å¼ï¼Œç›´æ¥æ˜¯state_dict
                    self.model.load_state_dict(best_checkpoint)
                _, _, _, _, final_predictions, final_targets = self.validate()
            except Exception as e:
                self.logger.warning(f"åŠ è½½æœ€ä½³æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€: {e}")
                _, _, _, _, final_predictions, final_targets = self.validate()
        else:
            self.logger.warning("æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€")
            _, _, _, _, final_predictions, final_targets = self.validate()
        
        axes[1, 1].scatter(final_targets, final_predictions, alpha=0.6)
        axes[1, 1].plot([min(final_targets), max(final_targets)], 
                        [min(final_targets), max(final_targets)], 'r--', label='ç†æƒ³é¢„æµ‹')
        axes[1, 1].set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')
        axes[1, 1].set_xlabel('çœŸå®æµ“åº¦')
        axes[1, 1].set_ylabel('é¢„æµ‹æµ“åº¦')
        axes[1, 1].legend()
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'training_curves.png'), dpi=300)
        plt.close()
        
        self.logger.info("è®­ç»ƒæ›²çº¿å·²ä¿å­˜: training_curves.png")
    
    def save_training_info(self, dataset, history):
        """ä¿å­˜è®­ç»ƒä¿¡æ¯"""
        info = {
            'training_time': datetime.now().isoformat(),
            'feature_dataset_path': self.args.feature_dataset_path,
            'model_type': self.args.model_type,
            'training_args': vars(self.args),
            'dataset_info': {
                'total_samples': len(dataset),
                'concentration_stats': dataset.get_concentration_statistics(),
                'metadata_stats': dataset.get_metadata_statistics()
            },
            'training_results': {
                'best_epoch': history['best_epoch'],
                'best_val_loss': history['best_val_loss'],
                'final_val_mae': history['val_maes'][-1],
                'final_val_r2': history['val_r2s'][-1]
            }
        }
        
        info_path = os.path.join(self.output_dir, 'training_info.json')
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        self.logger.info("è®­ç»ƒä¿¡æ¯å·²ä¿å­˜: training_info.json")

    def train_epoch_with_dual_eval(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼ŒåŒæ—¶è·Ÿè¸ªå¹³æ»‘æƒé‡æŸå¤±å’Œæ ‡å‡†MSE"""
        self.model.train()
        
        # æ›´æ–°æ¸è¿›å¼æƒé‡
        if hasattr(self.criterion, 'update_epoch'):
            self.criterion.update_epoch(epoch)
        
        total_smooth_loss = 0.0
        total_standard_loss = 0.0
        num_batches = 0
        
        # æ ‡å‡†MSEç”¨äºæ€§èƒ½è¯„ä¼°
        standard_mse = nn.MSELoss()
        
        # è¿›åº¦ç›‘æ§
        total_batches = len(self.train_loader)
        log_interval = max(1, total_batches // 10)  # æ¯10%è¾“å‡ºä¸€æ¬¡
        
        for batch_idx, (images, targets, metadata) in enumerate(self.train_loader):
            images = images.to(self.device)
            targets = targets.to(self.device).float()
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(images)
            # å¤„ç†æ¨¡å‹è¾“å‡ºï¼ˆå¯èƒ½æ˜¯tupleï¼‰
            if isinstance(outputs, tuple):
                outputs = outputs[0]  # å–ä¸»è¾“å‡º
            
            # ç”¨äºè®­ç»ƒçš„å¹³æ»‘æƒé‡æŸå¤±
            smooth_loss = self.criterion(outputs.squeeze(), targets.squeeze())
            
            # ç”¨äºè¯„ä¼°çš„æ ‡å‡†MSEï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
            with torch.no_grad():
                standard_loss = standard_mse(outputs.squeeze(), targets.squeeze())
            
            smooth_loss.backward()
            self.optimizer.step()
            
            total_smooth_loss += smooth_loss.item()
            total_standard_loss += standard_loss.item()
            num_batches += 1
            
            # è¿›åº¦è¾“å‡º
            if batch_idx % log_interval == 0:
                progress = 100.0 * batch_idx / total_batches
                self.logger.info(f"   æ‰¹æ¬¡ {batch_idx:3d}/{total_batches} ({progress:5.1f}%) | "
                               f"å¹³æ»‘æŸå¤±: {smooth_loss.item():.4f} | "
                               f"æ ‡å‡†MSE: {standard_loss.item():.4f}")
        
        avg_smooth_loss = total_smooth_loss / num_batches
        avg_standard_loss = total_standard_loss / num_batches
        
        return avg_smooth_loss, avg_standard_loss

    def validate_with_dual_eval(self):
        """éªŒè¯æ¨¡å‹ï¼Œè¿”å›å¹³æ»‘æƒé‡æŸå¤±å’Œæ ‡å‡†MSE"""
        self.model.eval()
        total_smooth_loss = 0.0
        total_standard_loss = 0.0
        predictions = []
        targets = []
        
        standard_mse = nn.MSELoss()
        
        with torch.no_grad():
            for images, batch_targets, metadata in self.val_loader:
                images = images.to(self.device)
                batch_targets = batch_targets.to(self.device).float()
                
                outputs = self.model(images)
                if isinstance(outputs, tuple):
                    outputs = outputs[0]
                
                # å¹³æ»‘æƒé‡æŸå¤±
                smooth_loss = self.criterion(outputs.squeeze(), batch_targets.squeeze())
                
                # æ ‡å‡†MSE
                standard_loss = standard_mse(outputs.squeeze(), batch_targets.squeeze())
                
                total_smooth_loss += smooth_loss.item()
                total_standard_loss += standard_loss.item()
                
                predictions.extend(outputs.cpu().numpy())
                targets.extend(batch_targets.cpu().numpy())
        
        avg_smooth_loss = total_smooth_loss / len(self.val_loader)
        avg_standard_loss = total_standard_loss / len(self.val_loader)
        
        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
        predictions = np.array(predictions).flatten()
        targets = np.array(targets).flatten()
        
        mae = mean_absolute_error(targets, predictions)
        r2 = r2_score(targets, predictions)
        
        return (avg_smooth_loss, avg_standard_loss, mae, r2, predictions, targets)


def main():
    parser = argparse.ArgumentParser(description='ä½¿ç”¨ç‰¹å¾æ•°æ®é›†è®­ç»ƒå›å½’æ¨¡å‹')
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--feature_dataset_path', type=str, default=None,
                       help='ç‰¹å¾æ•°æ®é›†è·¯å¾„ (é»˜è®¤è‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--dataset_version', type=str, default='latest',
                       choices=['v1', 'v2', 'v3', 'v4', 'latest'],
                       help='ç‰¹å¾æ•°æ®é›†ç‰ˆæœ¬ (é»˜è®¤latest)')
    parser.add_argument('--bg_mode', type=str, default='all', 
                       choices=['bg0', 'bg1', 'all', 'bg0_20mw', 'bg0_100mw', 'bg0_400mw', 
                               'bg1_20mw', 'bg1_100mw', 'bg1_400mw'],
                       help='è®­ç»ƒæ¨¡å¼: bg0/bg1/all(ä¼ ç»Ÿ3æ¡£) æˆ– bg0_20mwç­‰(æ–°6æ¡£ç»†åˆ†)')
    parser.add_argument('--power_filter', type=str, default=None,
                       help='è¿‡æ»¤ç‰¹å®šåŠŸç‡ (ä¾‹å¦‚: 20mw, 100mw, 400mw) - ä¸bg_modeç»†åˆ†å†²çªæ—¶å¿½ç•¥')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.8)')
    parser.add_argument('--image_size', type=int, default=224,
                       help='è¾“å…¥å›¾åƒå°ºå¯¸ (é»˜è®¤224)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_type', type=str, default='cnn', choices=['cnn', 'vgg', 'resnet50'],
                       help='æ¨¡å‹ç±»å‹: cnn(é«˜çº§CNN), vgg(VGG+CBAM), resnet50(ResNet50å›å½’) (é»˜è®¤: cnn)')
    parser.add_argument('--hidden_dim', type=int, default=256,
                       help='éšè—å±‚ç»´åº¦ (é»˜è®¤: 512)')
    
    # å†»ç»“æƒé‡å‚æ•° - æ–°å¢
    parser.add_argument('--freeze_backbone', action='store_true', default=False,
                       help='æ˜¯å¦å†»ç»“é¢„è®­ç»ƒä¸»å¹²ç½‘ç»œ (é»˜è®¤: False, å³è§£å†»)')
    parser.add_argument('--freeze_features', action='store_true', default=False,
                       help='æ˜¯å¦å†»ç»“VGGé¢„è®­ç»ƒç‰¹å¾ (é»˜è®¤: False, å³è§£å†»)')
    parser.add_argument('--progressive_unfreeze', action='store_true', default=False,
                       help='æ˜¯å¦ä½¿ç”¨æ¸è¿›å¼è§£å†»ç­–ç•¥')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ¬¡ (é»˜è®¤: 100)')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 32)')
    parser.add_argument('--learning_rate', type=float, default=5e-5,
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 0.001)')
    parser.add_argument('--weight_decay', type=float, default=1e-3,
                       help='æƒé‡è¡°å‡ (é»˜è®¤: 1e-4)')
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'],
                       help='ä¼˜åŒ–å™¨ (é»˜è®¤: adam)')
    parser.add_argument('--scheduler', type=str, default='step', choices=['step', 'plateau', 'cosine', 'none'],
                       help='å­¦ä¹ ç‡è°ƒåº¦å™¨ (é»˜è®¤: step)')
    parser.add_argument('--step_size', type=int, default=50,
                       help='StepLRæ­¥é•¿ (é»˜è®¤: 50)')
    parser.add_argument('--gamma', type=float, default=0.5,
                       help='StepLRè¡°å‡ç‡ (é»˜è®¤: 0.5)')
    parser.add_argument('--patience', type=int, default=10,
                       help='ReduceLROnPlateauè€å¿ƒå€¼ (é»˜è®¤: 10)')
    
    # æ–­ç‚¹ç»­è®­å‚æ•°
    parser.add_argument('--resume', type=str, default=None,
                       help='ä»æŒ‡å®šæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆè¾“å‡ºç›®å½•è·¯å¾„ï¼‰')
    parser.add_argument('--reset_optimizer_on_resume', action='store_true', default=False,
                       help='åŠ è½½æ£€æŸ¥ç‚¹æ—¶é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä½¿ç”¨æ–°çš„å­¦ä¹ ç‡ç­‰å‚æ•°')
    parser.add_argument('--reset_scheduler_on_resume', action='store_true', default=False,
                       help='åŠ è½½æ£€æŸ¥ç‚¹æ—¶é‡ç½®è°ƒåº¦å™¨çŠ¶æ€')
    parser.add_argument('--early_stopping_patience', type=int, default=15,
                       help='æ—©åœè€å¿ƒå€¼ (é»˜è®¤: 15)')
    parser.add_argument('--save_checkpoint_every', type=int, default=10,
                       help='æ¯Nä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ (é»˜è®¤: 10)')
    
    # é«˜æµ“åº¦ä¼˜åŒ–å‚æ•°
    parser.add_argument('--disable_concentration_aware_loss', action='store_true',
                       help='ç¦ç”¨æµ“åº¦æ„ŸçŸ¥æŸå¤±,ä½¿ç”¨æ ‡å‡†MSE')
    parser.add_argument('--high_concentration_threshold', type=float, default=800,
                       help='é«˜æµ“åº¦é˜ˆå€¼ (mg/L), é»˜è®¤800')
    parser.add_argument('--high_concentration_weight', type=float, default=0.5,
                       help='é«˜æµ“åº¦æŸå¤±æƒé‡, é»˜è®¤0.5 (50%% é™æƒ)')
    
    # å¹³æ»‘æƒé‡æŸå¤±å‡½æ•°å‚æ•°
    parser.add_argument('--smooth_scale_factor', type=float, default=0.01,
                       help='å¹³æ»‘æƒé‡è¿‡æ¸¡æ–œç‡ (é»˜è®¤: 0.01)')
    parser.add_argument('--progressive_weighting', action='store_true',
                       help='å¯ç”¨æ¸è¿›å¼æƒé‡è°ƒæ•´')
    parser.add_argument('--warmup_epochs', type=int, default=30,
                       help='æ¸è¿›å¼æƒé‡é¢„çƒ­è½®æ¬¡ (é»˜è®¤: 30)')
    parser.add_argument('--use_legacy_loss', action='store_true',
                       help='ä½¿ç”¨ä¼ ç»Ÿç¡¬é˜ˆå€¼æŸå¤±å‡½æ•° (ConcentrationAwareLoss)')
    
    args = parser.parse_args()
    
    print("ä½¿ç”¨ç‰¹å¾æ•°æ®é›†è®­ç»ƒå›å½’æ¨¡å‹")
    print("=" * 50)
    print(f"è®­ç»ƒæ¨¡å¼: {args.bg_mode.upper()}")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = FeatureDatasetTrainer(args)
        
        # æ™ºèƒ½æ£€æµ‹å‚æ•°å˜åŒ–ï¼ˆå¿…é¡»åœ¨åŠ è½½æ•°æ®å‰æ‰§è¡Œï¼‰
        trainer._detect_parameter_changes()
        
        # åŠ è½½æ•°æ®
        dataset = trainer.load_data()
        
        # åˆ›å»ºæ¨¡å‹
        trainer.create_model()
        
        # è®¾ç½®è®­ç»ƒ
        trainer.setup_training()
        
        # è®­ç»ƒæ¨¡å‹
        history = trainer.train()
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        trainer.save_training_info(dataset, history)
        
        print(f"\nè®­ç»ƒå®Œæˆï¼")
        print(f"ç»“æœä¿å­˜åœ¨: {trainer.output_dir}")
        print(f"æœ€ä½³æ¨¡å‹: {trainer.output_dir}/best_model.pth")
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main() 