#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾æ•°æ®é›†åŠ è½½å™¨
ä¸“é—¨ç”¨äºåŠ è½½YOLOè£å‰ªåçš„ç‰¹å¾æ•°æ®é›†
"""

import os
import json
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import glob
from typing import List, Tuple, Optional, Dict
import re

class FeatureImageDataset(Dataset):
    """ç‰¹å¾å›¾åƒæ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, 
                 feature_dataset_path: str,
                 transform=None,
                 bg_type: Optional[str] = None,
                 power_filter: Optional[str] = None):
        """
        åˆå§‹åŒ–ç‰¹å¾æ•°æ®é›†
        
        Args:
            feature_dataset_path: ç‰¹å¾æ•°æ®é›†è·¯å¾„
            transform: å›¾åƒå˜æ¢
            bg_type: è¿‡æ»¤ç‰¹å®šèƒŒæ™¯ç±»å‹ ('bg0', 'bg1')
            power_filter: è¿‡æ»¤ç‰¹å®šåŠŸç‡ ('20mw', '100mw', '400mw')
        """
        self.feature_dataset_path = feature_dataset_path
        self.transform = transform
        self.bg_type = bg_type
        self.power_filter = power_filter
        
        # ç‰¹å¾å›¾åƒå’Œä¿¡æ¯æ–‡ä»¶è·¯å¾„
        self.images_dir = os.path.join(feature_dataset_path, 'images')
        self.info_dir = os.path.join(feature_dataset_path, 'original_info')
        
        # æ£€æŸ¥è·¯å¾„
        if not os.path.exists(self.images_dir):
            raise FileNotFoundError(f"ç‰¹å¾å›¾åƒç›®å½•ä¸å­˜åœ¨: {self.images_dir}")
        if not os.path.exists(self.info_dir):
            raise FileNotFoundError(f"ä¿¡æ¯æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {self.info_dir}")
        
        # åŠ è½½æ•°æ®
        self.image_files = []
        self.concentrations = []
        self.metadata = []
        
        self._load_dataset()
        
        print(f"ç‰¹å¾æ•°æ®é›†åŠ è½½å®Œæˆ")
        print(f"   æ•°æ®é›†è·¯å¾„: {feature_dataset_path}")
        print(f"   æœ‰æ•ˆæ ·æœ¬: {len(self.image_files)} å¼ ")
        if bg_type:
            print(f"   èƒŒæ™¯è¿‡æ»¤: {bg_type}")
        if power_filter:
            print(f"   åŠŸç‡è¿‡æ»¤: {power_filter}")
        if len(self.concentrations) > 0:
            print(f"   æµ“åº¦èŒƒå›´: {min(self.concentrations):.1f} - {max(self.concentrations):.1f}")
            print(f"   å¹³å‡æµ“åº¦: {np.mean(self.concentrations):.2f}")
    
    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        print(f"\n=== å¼€å§‹åŠ è½½ç‰¹å¾æ•°æ®é›† ===")
        print(f"å›¾åƒç›®å½•: {self.images_dir}")
        print(f"ä¿¡æ¯ç›®å½•: {self.info_dir}")
        
        # è·å–æ‰€æœ‰ç‰¹å¾å›¾åƒæ–‡ä»¶
        print("æ­¥éª¤1/4: æ‰«æç‰¹å¾å›¾åƒæ–‡ä»¶...")
        image_pattern = os.path.join(self.images_dir, "feature_*.jpg")
        image_files = glob.glob(image_pattern)
        
        print(f"æ‰¾åˆ° {len(image_files)} ä¸ªç‰¹å¾å›¾åƒæ–‡ä»¶")
        
        # æ·»åŠ è¿‡æ»¤ä¿¡æ¯
        filter_info = []
        if self.bg_type:
            filter_info.append(f"èƒŒæ™¯ç±»å‹={self.bg_type}")
        if self.power_filter:
            filter_info.append(f"åŠŸç‡={self.power_filter}")
        
        if filter_info:
            print(f"åº”ç”¨è¿‡æ»¤æ¡ä»¶: {', '.join(filter_info)}")
        else:
            print("æ— è¿‡æ»¤æ¡ä»¶ï¼ŒåŠ è½½å…¨éƒ¨æ•°æ®")
        
        print("\næ­¥éª¤2/4: éªŒè¯å›¾åƒå’Œä¿¡æ¯æ–‡ä»¶...")
        valid_count = 0
        invalid_count = 0
        filtered_count = 0
        
        # è®¡ç®—è¿›åº¦è¾“å‡ºé—´éš”
        total_files = len(image_files)
        progress_interval = max(1, total_files // 20)  # æ¯5%è¾“å‡ºä¸€æ¬¡è¿›åº¦
        
        for i, image_file in enumerate(image_files):
            # è¿›åº¦è¾“å‡º
            if i % progress_interval == 0 or i == total_files - 1:
                progress = (i + 1) / total_files * 100
                print(f"  è¿›åº¦: {i+1}/{total_files} ({progress:.1f}%) - æœ‰æ•ˆ:{valid_count}, è¿‡æ»¤:{filtered_count}, æ— æ•ˆ:{invalid_count}")
            
            try:
                # æ„å»ºå¯¹åº”çš„ä¿¡æ¯æ–‡ä»¶è·¯å¾„
                image_name = os.path.basename(image_file)
                info_name = os.path.splitext(image_name)[0] + '.json'
                info_file = os.path.join(self.info_dir, info_name)
                
                if not os.path.exists(info_file):
                    if invalid_count < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯ï¼Œé¿å…åˆ·å±
                        print(f"  è­¦å‘Š: ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {info_file}")
                    invalid_count += 1
                    continue
                
                # åŠ è½½ä¿¡æ¯æ–‡ä»¶
                with open(info_file, 'r', encoding='utf-8') as f:
                    info_data = json.load(f)
                
                # æ£€æŸ¥è¿‡æ»¤æ¡ä»¶
                if self.bg_type and info_data.get('bg_type') != self.bg_type:
                    filtered_count += 1
                    continue
                
                if self.power_filter and info_data.get('power') != self.power_filter:
                    filtered_count += 1
                    continue
                
                # éªŒè¯å›¾åƒæ–‡ä»¶
                if not self._validate_image(image_file):
                    invalid_count += 1
                    continue
                
                # æ·»åŠ åˆ°æ•°æ®é›†
                self.image_files.append(image_file)
                # å¤„ç†å¯èƒ½ç¼ºå¤±çš„æµ“åº¦ä¿¡æ¯
                concentration = info_data.get('concentration')
                bg_type = info_data.get('bg_type')
                power = info_data.get('power')
                
                # å¦‚æœæ²¡æœ‰æµ“åº¦ä¿¡æ¯ï¼Œå°è¯•ä»æ–‡ä»¶åè§£æ
                if concentration is None:
                    # ä»æ–‡ä»¶åè§£æä¿¡æ¯
                    # æ–‡ä»¶åæ ¼å¼: feature_å…¥å°„è§’åº¦-æ‚¬æµ®ç‰©æµ“åº¦-ç›¸æœºé«˜åº¦-æ°´ä½“æµé€Ÿ-èƒŒæ™¯è¡¥å…‰ä¸å¦-æ¿€å…‰å…‰å¼º.jpg
                    filename = os.path.basename(image_file)
                    parts = filename.replace('feature_', '').replace('.jpg', '').split('-')
                    if len(parts) >= 6:
                        try:
                            concentration = float(parts[1])  # æ‚¬æµ®ç‰©æµ“åº¦æ˜¯ç¬¬äºŒä¸ªéƒ¨åˆ†
                            # æ›´æ–°å…ƒæ•°æ®
                            info_data['concentration'] = concentration
                            # å¦‚æœèƒŒæ™¯ç±»å‹ç¼ºå¤±ï¼Œä¹Ÿä»æ–‡ä»¶åè§£æ
                            if bg_type is None:
                                bg_type = parts[4]  # èƒŒæ™¯è¡¥å…‰ä¸å¦æ˜¯ç¬¬äº”ä¸ªéƒ¨åˆ†
                                info_data['bg_type'] = bg_type
                            # å¦‚æœåŠŸç‡ç¼ºå¤±ï¼Œä¹Ÿä»æ–‡ä»¶åè§£æ
                            if power is None:
                                power = parts[5]  # æ¿€å…‰å…‰å¼ºæ˜¯ç¬¬å…­ä¸ªéƒ¨åˆ†
                                info_data['power'] = power
                        except ValueError:
                            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼0.0
                            concentration = 0.0
                    else:
                        # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨é»˜è®¤å€¼0.0
                        concentration = 0.0
                else:
                    concentration = float(concentration)
                
                self.concentrations.append(concentration)
                self.metadata.append(info_data)
                valid_count += 1
                
            except Exception as e:
                if invalid_count < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    print(f"  åŠ è½½æ–‡ä»¶å¤±è´¥: {image_file} - {e}")
                invalid_count += 1
        
        print(f"\næ­¥éª¤3/4: æ•°æ®é›†åŠ è½½ç»Ÿè®¡")
        print(f"  æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"  æœ‰æ•ˆæ–‡ä»¶: {valid_count}")
        print(f"  è¿‡æ»¤æ–‡ä»¶: {filtered_count}")
        print(f"  æ— æ•ˆæ–‡ä»¶: {invalid_count}")
        print(f"  æœ€ç»ˆæ•°æ®é›†å¤§å°: {valid_count} å¼ å›¾åƒ")
        
        if len(self.image_files) == 0:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾å›¾åƒæ•°æ®ï¼")
        
        print("\næ­¥éª¤4/4: è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
        if len(self.concentrations) > 0:
            concentrations = np.array(self.concentrations)
            print(f"  æµ“åº¦èŒƒå›´: {concentrations.min():.1f} - {concentrations.max():.1f}")
            print(f"  å¹³å‡æµ“åº¦: {concentrations.mean():.2f} Â± {concentrations.std():.2f}")
            print(f"  æµ“åº¦ç§ç±»: {len(set(self.concentrations))} ç§")
        
        print("=== æ•°æ®é›†åŠ è½½å®Œæˆ ===\n")
    
    def _validate_image(self, image_path: str) -> bool:
        """éªŒè¯å›¾åƒæ–‡ä»¶å®Œæ•´æ€§"""
        try:
            with Image.open(image_path) as img:
                img.verify()
            return True
        except Exception:
            return False
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        """è·å–æ•°æ®é¡¹"""
        image_path = self.image_files[idx]
        concentration = self.concentrations[idx]
        metadata = self.metadata[idx]
        
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            
            # åº”ç”¨å˜æ¢
            if self.transform:
                image = self.transform(image)
            
            return image, concentration, metadata
            
        except Exception as e:
            print(f"è¯»å–å›¾åƒå¤±è´¥: {image_path} - {e}")
            # è¿”å›ä¸€ä¸ªå ä½å›¾åƒ
            if self.transform:
                placeholder = self.transform(Image.new('RGB', (224, 224), color='black'))
            else:
                placeholder = Image.new('RGB', (224, 224), color='black')
            return placeholder, concentration, metadata
    
    def get_concentration_statistics(self):
        """è·å–æµ“åº¦ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.concentrations) == 0:
            return {}
        
        concentrations = np.array(self.concentrations)
        return {
            'count': len(concentrations),
            'min': float(concentrations.min()),
            'max': float(concentrations.max()),
            'mean': float(concentrations.mean()),
            'std': float(concentrations.std()),
            'unique_values': sorted(list(set(concentrations)))
        }
    
    def get_metadata_statistics(self):
        """è·å–å…ƒæ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.metadata) == 0:
            return {}
        
        stats = {}
        
        # ç»Ÿè®¡èƒŒæ™¯ç±»å‹
        bg_types = [meta.get('bg_type', 'unknown') for meta in self.metadata]
        stats['bg_types'] = {bg: bg_types.count(bg) for bg in set(bg_types)}
        
        # ç»Ÿè®¡åŠŸç‡ç±»å‹
        powers = [meta.get('power', 'unknown') for meta in self.metadata]
        stats['powers'] = {power: powers.count(power) for power in set(powers)}
        
        # ç»Ÿè®¡è·ç¦»
        distances = [meta.get('distance', 'unknown') for meta in self.metadata]
        stats['distances'] = {dist: distances.count(dist) for dist in set(distances)}
        
        # ç»Ÿè®¡æ£€æµ‹ç½®ä¿¡åº¦
        confidences = [meta.get('detection_confidence', 0.0) for meta in self.metadata]
        stats['detection_confidence'] = {
            'min': min(confidences),
            'max': max(confidences),
            'mean': np.mean(confidences),
            'std': np.std(confidences)
        }
        
        return stats


def detect_feature_datasets(base_path: str = ".", version_filter: str = None) -> List[Dict]:
    """
    æ£€æµ‹ç‰¹å¾æ•°æ®é›†ï¼ˆæ”¯æŒç‰ˆæœ¬è¿‡æ»¤ï¼‰
    
    Args:
        base_path: æœç´¢åŸºç¡€è·¯å¾„
        version_filter: ç‰ˆæœ¬è¿‡æ»¤å™¨ ('v1', 'v2', 'latest', Noneè¡¨ç¤ºå…¨éƒ¨)
        
    Returns:
        æ•°æ®é›†ä¿¡æ¯åˆ—è¡¨ï¼ŒæŒ‰ç‰ˆæœ¬å’Œæ—¶é—´æ’åº
    """
    print(f"ğŸ” æ£€æµ‹ç‰¹å¾æ•°æ®é›†...")
    print(f"   æœç´¢è·¯å¾„: {base_path}")
    if version_filter:
        print(f"   ç‰ˆæœ¬è¿‡æ»¤: {version_filter}")
    
    datasets = []
    
    # æœç´¢ç‰¹å¾æ•°æ®é›†ç›®å½•
    for item in os.listdir(base_path):
        item_path = os.path.join(base_path, item)
        if os.path.isdir(item_path):
            # æ£€æŸ¥æ˜¯å¦ä¸ºç‰¹å¾æ•°æ®é›†ç›®å½•
            is_feature_dataset = False
            dataset_version = 'v1'  # é»˜è®¤ç‰ˆæœ¬
            
            # é€šè¿‡ç›®å½•ååˆ¤æ–­
            if item.startswith('feature_dataset'):
                is_feature_dataset = True
                
                # æå–ç‰ˆæœ¬ä¿¡æ¯
                if 'v2' in item.lower():
                    dataset_version = 'v2'
                elif 'v3' in item.lower():
                    dataset_version = 'v3'
                elif 'v4' in item.lower():
                    dataset_version = 'v4'
                else:
                    # æ—§ç‰ˆæœ¬å‘½åæ–¹å¼
                    dataset_version = 'v1'
            
            # æ£€æŸ¥ç›®å½•ç»“æ„
            if is_feature_dataset:
                images_dir = os.path.join(item_path, 'images')
                info_dir = os.path.join(item_path, 'original_info')
                config_file = os.path.join(item_path, 'config.json')
                
                if os.path.exists(images_dir) and os.path.exists(info_dir):
                    # è·å–æ•°æ®é›†è¯¦ç»†ä¿¡æ¯
                    dataset_info = {
                        'path': item_path,
                        'name': item,
                        'version': dataset_version,
                        'images_dir': images_dir,
                        'info_dir': info_dir,
                        'config_file': config_file if os.path.exists(config_file) else None,
                        'creation_time': None,
                        'sample_count': 0,
                        'exclude_patterns': [],
                        'yolo_model_used': None
                    }
                    
                    # è¯»å–é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if dataset_info['config_file']:
                        try:
                            with open(dataset_info['config_file'], 'r', encoding='utf-8') as f:
                                config = json.load(f)
                                dataset_info['creation_time'] = config.get('creation_time')
                                dataset_info['exclude_patterns'] = config.get('exclude_patterns', [])
                                dataset_info['yolo_model_used'] = config.get('yolo_model_used')
                                # ä»é…ç½®æ–‡ä»¶è·å–å‡†ç¡®çš„ç‰ˆæœ¬ä¿¡æ¯
                                if 'version' in config:
                                    dataset_info['version'] = config['version']
                        except Exception as e:
                            print(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {config_file} - {e}")
                    
                    # ç»Ÿè®¡æ ·æœ¬æ•°é‡
                    try:
                        image_files = [f for f in os.listdir(images_dir) 
                                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                        dataset_info['sample_count'] = len(image_files)
                    except Exception:
                        dataset_info['sample_count'] = 0
                    
                    # æå–æ—¶é—´æˆ³ï¼ˆå¦‚æœé…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ï¼‰
                    if not dataset_info['creation_time']:
                        timestamp_match = re.search(r'(\d{8}_\d{6})', item)
                        if timestamp_match:
                            dataset_info['creation_time'] = timestamp_match.group(1)
                    
                    datasets.append(dataset_info)
    
    # åº”ç”¨ç‰ˆæœ¬è¿‡æ»¤
    if version_filter:
        if version_filter == 'latest':
            # è·å–æœ€æ–°ç‰ˆæœ¬
            if datasets:
                # æŒ‰ç‰ˆæœ¬æ’åºï¼ˆv4 > v3 > v2 > v1ï¼‰
                version_order = {'v4': 4, 'v3': 3, 'v2': 2, 'v1': 1}
                datasets_by_version = {}
                
                for dataset in datasets:
                    version = dataset['version']
                    if version not in datasets_by_version:
                        datasets_by_version[version] = []
                    datasets_by_version[version].append(dataset)
                
                # æ‰¾åˆ°æœ€é«˜ç‰ˆæœ¬
                max_version = max(datasets_by_version.keys(), 
                                key=lambda v: version_order.get(v, 0))
                latest_datasets = datasets_by_version[max_version]
                
                # åœ¨åŒç‰ˆæœ¬ä¸­é€‰æ‹©æœ€æ–°çš„
                if latest_datasets:
                    latest_datasets.sort(key=lambda d: d.get('creation_time', ''), reverse=True)
                    datasets = [latest_datasets[0]]
                else:
                    datasets = []
        else:
            # è¿‡æ»¤ç‰¹å®šç‰ˆæœ¬
            datasets = [d for d in datasets if d['version'] == version_filter]
    
    # æ’åºï¼šç‰ˆæœ¬ > æ—¶é—´
    version_order = {'v4': 4, 'v3': 3, 'v2': 2, 'v1': 1}
    datasets.sort(key=lambda d: (
        version_order.get(d['version'], 0),
        d.get('creation_time', '')
    ), reverse=True)
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(datasets)} ä¸ªç‰¹å¾æ•°æ®é›†:")
    for i, dataset in enumerate(datasets, 1):
        exclude_info = f", æ’é™¤: {dataset['exclude_patterns']}" if dataset['exclude_patterns'] else ""
        model_info = f", æ¨¡å‹: {os.path.basename(dataset['yolo_model_used'])}" if dataset['yolo_model_used'] else ""
        print(f"   {i}. {dataset['name']} ({dataset['version']}) - {dataset['sample_count']} å¼ {exclude_info}{model_info}")
    
    return datasets


def create_feature_dataloader(feature_dataset_path: str = None,
                             batch_size: int = 32,
                             shuffle: bool = True,
                             bg_type: Optional[str] = None,
                             power_filter: Optional[str] = None,
                             image_size: int = 224,
                             dataset_version: str = 'latest') -> Tuple[DataLoader, FeatureImageDataset]:
    """
    åˆ›å»ºç‰¹å¾æ•°æ®é›†åŠ è½½å™¨ï¼ˆæ”¯æŒç‰ˆæœ¬é€‰æ‹©ï¼‰
    
    Args:
        feature_dataset_path: ç‰¹å¾æ•°æ®é›†è·¯å¾„ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        bg_type: è¿‡æ»¤ç‰¹å®šèƒŒæ™¯ç±»å‹ ('bg0', 'bg1')
        power_filter: è¿‡æ»¤ç‰¹å®šåŠŸç‡ ('20mw', '100mw', '400mw')
        image_size: å›¾åƒå°ºå¯¸
        dataset_version: æ•°æ®é›†ç‰ˆæœ¬ ('v1', 'v2', 'v3', 'v4', 'latest')
        
    Returns:
        (DataLoader, Dataset)
    """
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†
    if feature_dataset_path is None:
        print(f"è‡ªåŠ¨æ£€æµ‹ç‰¹å¾æ•°æ®é›† (ç‰ˆæœ¬: {dataset_version})...")
        
        datasets = detect_feature_datasets(version_filter=dataset_version)
        if not datasets:
            available_datasets = detect_feature_datasets()
            if available_datasets:
                print("å¯ç”¨çš„æ•°æ®é›†ç‰ˆæœ¬:")
                for dataset in available_datasets:
                    print(f"  - {dataset['name']} ({dataset['version']})")
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç‰ˆæœ¬ {dataset_version} çš„ç‰¹å¾æ•°æ®é›†ï¼")
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªï¼ˆæœ€æ–°çš„ï¼‰æ•°æ®é›†
        feature_dataset_path = datasets[0]['path']
        dataset_info = datasets[0]
        
        print(f"âœ… é€‰æ‹©æ•°æ®é›†: {dataset_info['name']} ({dataset_info['version']})")
        print(f"   æ ·æœ¬æ•°é‡: {dataset_info['sample_count']}")
        if dataset_info['exclude_patterns']:
            print(f"   æ’é™¤æ¨¡å¼: {dataset_info['exclude_patterns']}")
        if dataset_info['yolo_model_used']:
            print(f"   ç”Ÿæˆæ¨¡å‹: {os.path.basename(dataset_info['yolo_model_used'])}")
    
    # å›¾åƒå˜æ¢
    if image_size != 224:
        print(f"è®¾ç½®å›¾åƒå°ºå¯¸: {image_size}x{image_size}")
    
    transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = FeatureImageDataset(
        feature_dataset_path=feature_dataset_path,
        transform=transform,
        bg_type=bg_type,
        power_filter=power_filter
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # Windowsç³»ç»Ÿä½¿ç”¨num_workers=0é¿å…å¤šè¿›ç¨‹é—®é¢˜
    num_workers = 0 if os.name == 'nt' else 4
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True
    )
    
    print(f"âœ… ç‰¹å¾æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    print(f"   æ•°æ®é›†è·¯å¾„: {feature_dataset_path}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   æ ·æœ¬æ€»æ•°: {len(dataset)}")
    print(f"   æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
    
    return dataloader, dataset


def main(test_dataset_path: str = None):
    """æµ‹è¯•ç‰¹å¾æ•°æ®é›†åŠ è½½å™¨
    
    Args:
        test_dataset_path: å¯é€‰çš„æµ‹è¯•æ•°æ®é›†è·¯å¾„
    """
    print("æµ‹è¯•ç‰¹å¾æ•°æ®é›†åŠ è½½å™¨")
    
    if test_dataset_path:
        # ä½¿ç”¨æŒ‡å®šçš„æµ‹è¯•æ•°æ®é›†
        feature_dataset_path = test_dataset_path
        print(f"ä½¿ç”¨æµ‹è¯•æ•°æ®é›†: {test_dataset_path}")
    else:
        # æ£€æµ‹ç‰¹å¾æ•°æ®é›†
        feature_datasets = detect_feature_datasets()
        
        if not feature_datasets:
            print("æœªæ‰¾åˆ°ç‰¹å¾æ•°æ®é›†")
            print("è¯·å…ˆè¿è¡Œ create_feature_dataset.py ç”Ÿæˆç‰¹å¾æ•°æ®é›†")
            return
        
        # ä½¿ç”¨æœ€æ–°çš„ç‰¹å¾æ•°æ®é›†
        latest_dataset = feature_datasets[-1]
        feature_dataset_path = latest_dataset['path']
        print(f"ä½¿ç”¨æ•°æ®é›†: {latest_dataset['name']}")
    
    try:
        # åˆ›å»ºDataLoader
        dataloader, dataset = create_feature_dataloader(
            feature_dataset_path=feature_dataset_path,
            batch_size=16,
            bg_type=None  # ä¸è¿‡æ»¤
        )
        
        print(f"\næ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")
        
        # æµ“åº¦ç»Ÿè®¡
        conc_stats = dataset.get_concentration_statistics()
        print(f"   æµ“åº¦èŒƒå›´: {conc_stats['min']:.1f} - {conc_stats['max']:.1f}")
        print(f"   å¹³å‡æµ“åº¦: {conc_stats['mean']:.2f} Â± {conc_stats['std']:.2f}")
        print(f"   æµ“åº¦ç§ç±»: {len(conc_stats['unique_values'])} ç§")
        
        # å…ƒæ•°æ®ç»Ÿè®¡
        meta_stats = dataset.get_metadata_statistics()
        print(f"   èƒŒæ™¯åˆ†å¸ƒ: {meta_stats['bg_types']}")
        print(f"   åŠŸç‡åˆ†å¸ƒ: {meta_stats['powers']}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {meta_stats['detection_confidence']['mean']:.3f}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        print(f"\næµ‹è¯•æ•°æ®åŠ è½½...")
        for i, (images, concentrations, metadata) in enumerate(dataloader):
            print(f"   æ‰¹æ¬¡ {i+1}: {images.shape}, æµ“åº¦èŒƒå›´: {concentrations.min():.1f}-{concentrations.max():.1f}")
            if i >= 2:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
                break
        
        print(f"ç‰¹å¾æ•°æ®é›†åŠ è½½å™¨æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        main(sys.argv[1])
    else:
        main()