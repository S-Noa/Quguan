#!/usr/bin/env python3
"""
å¿«é€Ÿæ•°æ®é›†åŠ è½½å™¨ - æä¾›å¤šç§å›¾åƒéªŒè¯ç­–ç•¥
åŒ…å«ï¼šç¼“å­˜éªŒè¯ã€è·³è¿‡éªŒè¯ã€é‡‡æ ·éªŒè¯ã€å¹¶è¡ŒéªŒè¯ç­‰æ–¹æ¡ˆ
"""

import os
import re
import json
import time
import pickle
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
from pathlib import Path


class FastImageDataset(Dataset):
    def __init__(self, root_dir, transform=None, bg_type=None, 
                 validation_mode='cache', cache_file=None, 
                 sample_validation_ratio=0.1, max_workers=4):
        """
        å¿«é€Ÿå›¾åƒæ•°æ®é›†åŠ è½½å™¨
        
        å‚æ•°:
            root_dir (string): å›¾åƒæ–‡ä»¶å¤¹çš„æ ¹ç›®å½•
            transform (callable, optional): å¯é€‰çš„å›¾åƒè½¬æ¢
            bg_type (str, optional): 'bg0'æˆ–'bg1'ï¼ŒåªåŠ è½½å¯¹åº”è¡¥å…‰ç±»å‹çš„å›¾ç‰‡
            validation_mode (str): éªŒè¯æ¨¡å¼
                - 'none': è·³è¿‡æ‰€æœ‰éªŒè¯ (æœ€å¿«)
                - 'cache': ä½¿ç”¨ç¼“å­˜éªŒè¯ç»“æœ (æ¨è)
                - 'sample': éšæœºé‡‡æ ·éªŒè¯ (å¹³è¡¡æ–¹æ¡ˆ)
                - 'parallel': å¹¶è¡ŒéªŒè¯ (é«˜CPUåˆ©ç”¨ç‡)
                - 'full': å®Œæ•´éªŒè¯ (æœ€å®‰å…¨ä½†æœ€æ…¢)
            cache_file (str): ç¼“å­˜æ–‡ä»¶è·¯å¾„
            sample_validation_ratio (float): é‡‡æ ·éªŒè¯æ¯”ä¾‹ (0.0-1.0)
            max_workers (int): å¹¶è¡ŒéªŒè¯çš„çº¿ç¨‹æ•°
        """
        self.root_dir = root_dir
        self.transform = transform
        self.bg_type = bg_type
        self.validation_mode = validation_mode
        self.sample_validation_ratio = sample_validation_ratio
        self.max_workers = max_workers
        
        # è®¾ç½®ç¼“å­˜æ–‡ä»¶è·¯å¾„
        if cache_file is None:
            root_hash = hashlib.md5(root_dir.encode()).hexdigest()[:8]
            bg_suffix = f"_{bg_type}" if bg_type else "_all"
            cache_file = f"dataset_cache_{root_hash}{bg_suffix}.json"
        self.cache_file = cache_file
        
        print(f"ğŸš€ åˆå§‹åŒ–å¿«é€Ÿæ•°æ®é›† (æ¨¡å¼: {validation_mode})")
        print(f"ğŸ“ æ•°æ®è·¯å¾„: {root_dir}")
        print(f"ğŸ·ï¸ æ•°æ®ç±»å‹: {bg_type or 'all'}")
        
        start_time = time.time()
        
        # æ ¹æ®éªŒè¯æ¨¡å¼é€‰æ‹©ä¸åŒçš„åˆå§‹åŒ–ç­–ç•¥
        if validation_mode == 'none':
            self._init_no_validation()
        elif validation_mode == 'cache':
            self._init_with_cache()
        elif validation_mode == 'sample':
            self._init_sample_validation()
        elif validation_mode == 'parallel':
            self._init_parallel_validation()
        elif validation_mode == 'full':
            self._init_full_validation()
        else:
            raise ValueError(f"æœªçŸ¥çš„éªŒè¯æ¨¡å¼: {validation_mode}")
        
        init_time = time.time() - start_time
        print(f"â±ï¸ æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f}s")
        
        if self.concentrations:
            self._print_statistics()
        else:
            print(f"âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶!")
    
    def _find_candidate_files(self):
        """æŸ¥æ‰¾å€™é€‰å›¾åƒæ–‡ä»¶"""
        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')
        concentration_pattern = r'\d+Â°-(\d+)-'
        
        candidate_files = []
        candidate_concentrations = []
        
        for root, _, files in os.walk(self.root_dir):
            for file in files:
                if file.lower().endswith(valid_extensions):
                    if self.bg_type is not None and self.bg_type not in file:
                        continue
                    
                    file_path = os.path.join(root, file)
                    match = re.search(concentration_pattern, file)
                    if match:
                        concentration = float(match.group(1))
                        candidate_files.append(file_path)
                        candidate_concentrations.append(concentration)
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(candidate_files)} ä¸ªå€™é€‰æ–‡ä»¶")
        return candidate_files, candidate_concentrations
    
    def _init_no_validation(self):
        """è·³è¿‡éªŒè¯æ¨¡å¼ - æœ€å¿«ä½†æœ‰é£é™©"""
        print("âš¡ è·³è¿‡å›¾åƒéªŒè¯ (é£é™©æ¨¡å¼)")
        
        candidate_files, candidate_concentrations = self._find_candidate_files()
        self.image_files = candidate_files
        self.concentrations = candidate_concentrations
        
        print("âš ï¸ è­¦å‘Š: æœªéªŒè¯å›¾åƒå®Œæ•´æ€§ï¼Œå¯èƒ½åœ¨è®­ç»ƒæ—¶é‡åˆ°æŸåæ–‡ä»¶")
    
    def _init_with_cache(self):
        """ç¼“å­˜éªŒè¯æ¨¡å¼ - æ¨èæ–¹æ¡ˆ"""
        print("ğŸ’¾ ä½¿ç”¨ç¼“å­˜éªŒè¯æ¨¡å¼")
        
        candidate_files, candidate_concentrations = self._find_candidate_files()
        
        # å°è¯•åŠ è½½ç¼“å­˜
        cache_data = self._load_cache()
        if cache_data:
            print(f"ğŸ“¥ åŠ è½½ç¼“å­˜æ–‡ä»¶: {self.cache_file}")
            cached_valid_files = set(cache_data.get('valid_files', []))
            cached_invalid_files = set(cache_data.get('invalid_files', []))
            
            # ç­›é€‰æœ‰æ•ˆæ–‡ä»¶
            self.image_files = []
            self.concentrations = []
            need_validation = []
            need_validation_conc = []
            
            for file_path, conc in zip(candidate_files, candidate_concentrations):
                if file_path in cached_valid_files:
                    # ç¼“å­˜ä¸­æ ‡è®°ä¸ºæœ‰æ•ˆ
                    if os.path.exists(file_path):  # å¿«é€Ÿæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        self.image_files.append(file_path)
                        self.concentrations.append(conc)
                elif file_path in cached_invalid_files:
                    # ç¼“å­˜ä¸­æ ‡è®°ä¸ºæ— æ•ˆï¼Œè·³è¿‡
                    continue
                else:
                    # æ–°æ–‡ä»¶ï¼Œéœ€è¦éªŒè¯
                    need_validation.append(file_path)
                    need_validation_conc.append(conc)
            
            print(f"âœ… ä»ç¼“å­˜ä¸­è·å¾— {len(self.image_files)} ä¸ªå·²éªŒè¯æ–‡ä»¶")
            
            # éªŒè¯æ–°æ–‡ä»¶
            if need_validation:
                print(f"ğŸ” éªŒè¯ {len(need_validation)} ä¸ªæ–°æ–‡ä»¶...")
                valid_new, invalid_new = self._validate_files(need_validation, need_validation_conc)
                self.image_files.extend([f for f, _ in valid_new])
                self.concentrations.extend([c for _, c in valid_new])
                
                # æ›´æ–°ç¼“å­˜
                self._update_cache(valid_new, invalid_new)
        else:
            print("ğŸ“ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿›è¡Œå®Œæ•´éªŒè¯...")
            valid_files, invalid_files = self._validate_files(candidate_files, candidate_concentrations)
            self.image_files = [f for f, _ in valid_files]
            self.concentrations = [c for _, c in valid_files]
            
            # åˆ›å»ºç¼“å­˜
            self._create_cache(valid_files, invalid_files)
    
    def _init_sample_validation(self):
        """é‡‡æ ·éªŒè¯æ¨¡å¼ - å¹³è¡¡æ–¹æ¡ˆ"""
        print(f"ğŸ¯ é‡‡æ ·éªŒè¯æ¨¡å¼ (éªŒè¯æ¯”ä¾‹: {self.sample_validation_ratio*100:.1f}%)")
        
        candidate_files, candidate_concentrations = self._find_candidate_files()
        
        if self.sample_validation_ratio >= 1.0:
            # å®Œæ•´éªŒè¯
            valid_files, _ = self._validate_files(candidate_files, candidate_concentrations)
            self.image_files = [f for f, _ in valid_files]
            self.concentrations = [c for _, c in valid_files]
        else:
            # é‡‡æ ·éªŒè¯
            import random
            sample_size = max(1, int(len(candidate_files) * self.sample_validation_ratio))
            sample_indices = random.sample(range(len(candidate_files)), sample_size)
            
            sample_files = [candidate_files[i] for i in sample_indices]
            sample_concs = [candidate_concentrations[i] for i in sample_indices]
            
            print(f"ğŸ” éªŒè¯ {len(sample_files)} ä¸ªé‡‡æ ·æ–‡ä»¶...")
            valid_samples, invalid_samples = self._validate_files(sample_files, sample_concs)
            
            # è®¡ç®—æŸåç‡
            corruption_rate = len(invalid_samples) / len(sample_files) if sample_files else 0
            print(f"ğŸ“Š é‡‡æ ·æŸåç‡: {corruption_rate*100:.2f}%")
            
            if corruption_rate < 0.05:  # æŸåç‡ä½äº5%
                print("âœ… æŸåç‡è¾ƒä½ï¼Œè·³è¿‡å…¶ä½™éªŒè¯")
                self.image_files = candidate_files
                self.concentrations = candidate_concentrations
            else:
                print(f"âš ï¸ æŸåç‡è¾ƒé«˜ ({corruption_rate*100:.2f}%)ï¼Œè¿›è¡Œå®Œæ•´éªŒè¯...")
                valid_files, _ = self._validate_files(candidate_files, candidate_concentrations)
                self.image_files = [f for f, _ in valid_files]
                self.concentrations = [c for _, c in valid_files]
    
    def _init_parallel_validation(self):
        """å¹¶è¡ŒéªŒè¯æ¨¡å¼ - é«˜CPUåˆ©ç”¨ç‡"""
        print(f"ğŸ”„ å¹¶è¡ŒéªŒè¯æ¨¡å¼ (çº¿ç¨‹æ•°: {self.max_workers})")
        
        candidate_files, candidate_concentrations = self._find_candidate_files()
        valid_files, _ = self._validate_files_parallel(candidate_files, candidate_concentrations)
        
        self.image_files = [f for f, _ in valid_files]
        self.concentrations = [c for _, c in valid_files]
    
    def _init_full_validation(self):
        """å®Œæ•´éªŒè¯æ¨¡å¼ - æœ€å®‰å…¨ä½†æœ€æ…¢"""
        print("ğŸ›¡ï¸ å®Œæ•´éªŒè¯æ¨¡å¼")
        
        candidate_files, candidate_concentrations = self._find_candidate_files()
        valid_files, invalid_files = self._validate_files(candidate_files, candidate_concentrations)
        
        self.image_files = [f for f, _ in valid_files]
        self.concentrations = [c for _, c in valid_files]
        
        print(f"ğŸ—‘ï¸ æŸåæ–‡ä»¶æ•°: {len(invalid_files)}")
    
    def _validate_single_file(self, file_path):
        """éªŒè¯å•ä¸ªæ–‡ä»¶"""
        try:
            with Image.open(file_path) as img:
                img.verify()
            return True
        except (OSError, IOError, Image.UnidentifiedImageError):
            return False
    
    def _validate_files(self, files, concentrations):
        """éªŒè¯æ–‡ä»¶åˆ—è¡¨"""
        valid_files = []
        invalid_files = []
        
        for file_path, conc in zip(files, concentrations):
            if self._validate_single_file(file_path):
                valid_files.append((file_path, conc))
            else:
                invalid_files.append((file_path, conc))
        
        return valid_files, invalid_files
    
    def _validate_files_parallel(self, files, concentrations):
        """å¹¶è¡ŒéªŒè¯æ–‡ä»¶åˆ—è¡¨"""
        valid_files = []
        invalid_files = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤éªŒè¯ä»»åŠ¡
            future_to_file = {
                executor.submit(self._validate_single_file, file_path): (file_path, conc)
                for file_path, conc in zip(files, concentrations)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_file):
                file_path, conc = future_to_file[future]
                try:
                    is_valid = future.result()
                    if is_valid:
                        valid_files.append((file_path, conc))
                    else:
                        invalid_files.append((file_path, conc))
                except Exception as e:
                    print(f"éªŒè¯é”™è¯¯ {file_path}: {e}")
                    invalid_files.append((file_path, conc))
        
        return valid_files, invalid_files
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        return None
    
    def _create_cache(self, valid_files, invalid_files):
        """åˆ›å»ºç¼“å­˜æ–‡ä»¶"""
        cache_data = {
            'valid_files': [f for f, _ in valid_files],
            'invalid_files': [f for f, _ in invalid_files],
            'creation_time': time.time(),
            'root_dir': self.root_dir,
            'bg_type': self.bg_type
        }
        
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {self.cache_file}")
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _update_cache(self, new_valid_files, new_invalid_files):
        """æ›´æ–°ç¼“å­˜æ–‡ä»¶"""
        cache_data = self._load_cache() or {'valid_files': [], 'invalid_files': []}
        
        cache_data['valid_files'].extend([f for f, _ in new_valid_files])
        cache_data['invalid_files'].extend([f for f, _ in new_invalid_files])
        cache_data['update_time'] = time.time()
        
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ç¼“å­˜å·²æ›´æ–°")
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ›´æ–°å¤±è´¥: {e}")
    
    def _print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        min_conc = min(self.concentrations)
        max_conc = max(self.concentrations)
        avg_conc = sum(self.concentrations) / len(self.concentrations)
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æœ‰æ•ˆå›¾åƒ: {len(self.image_files)} å¼ ")
        print(f"   æµ“åº¦èŒƒå›´: {min_conc} - {max_conc}")
        print(f"   å¹³å‡æµ“åº¦: {avg_conc:.2f}")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        concentration = self.concentrations[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"è¿è¡Œæ—¶å›¾åƒè¯»å–é”™è¯¯: {img_path} - {e}")
            # åˆ›å»ºé»‘è‰²å›¾åƒä½œä¸ºåº”æ€¥æªæ–½
            image = Image.new('RGB', (224, 224), color='black')
        
        if self.transform:
            image = self.transform(image)
        
        return image, concentration


# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºä¸€ä¸ªåˆ«å
class OptimizedImageDataset(FastImageDataset):
    """ä¼˜åŒ–å›¾åƒæ•°æ®é›† - FastImageDatasetçš„åˆ«å"""
    pass


def get_recommended_dataset(root_dir, transform=None, bg_type=None):
    """
    è·å–æ¨èçš„æ•°æ®é›†é…ç½®
    
    æ ¹æ®æ•°æ®é›†å¤§å°è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„éªŒè¯ç­–ç•¥
    """
    # å¿«é€Ÿç»Ÿè®¡æ–‡ä»¶æ•°é‡
    file_count = 0
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                if bg_type is None or bg_type in file:
                    file_count += 1
    
    print(f"ğŸ“Š é¢„ä¼°æ–‡ä»¶æ•°: {file_count}")
    
    # æ ¹æ®æ–‡ä»¶æ•°é‡é€‰æ‹©ç­–ç•¥
    if file_count < 1000:
        print("ğŸ’¡ æ¨è: å®Œæ•´éªŒè¯æ¨¡å¼ (æ–‡ä»¶æ•°è¾ƒå°‘)")
        return FastImageDataset(root_dir, transform, bg_type, validation_mode='full')
    elif file_count < 10000:
        print("ğŸ’¡ æ¨è: ç¼“å­˜éªŒè¯æ¨¡å¼ (å¹³è¡¡æ€§èƒ½)")
        return FastImageDataset(root_dir, transform, bg_type, validation_mode='cache')
    else:
        print("ğŸ’¡ æ¨è: é‡‡æ ·éªŒè¯æ¨¡å¼ (å¤§æ•°æ®é›†)")
        return FastImageDataset(root_dir, transform, bg_type, validation_mode='sample', 
                               sample_validation_ratio=0.05) 