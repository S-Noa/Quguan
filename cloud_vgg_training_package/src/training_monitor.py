#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒè¿‡ç¨‹ç›‘æ§æ¨¡å—
åŒ…å«Grad-CAMå¯è§†åŒ–å’Œæ°´å°åŒºåŸŸå…³æ³¨åº¦æ£€æŸ¥
"""

import torch
import torch.nn as nn
import numpy as np
import cv2
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import os
from PIL import Image
import time
from font_utils import auto_setup_font, get_labels, CHINESE_SUPPORTED

class TrainingMonitor:
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å™¨"""
    
    def __init__(self, model_type='cnn', save_dir='monitoring_results', device='cuda'):
        """
        åˆå§‹åŒ–ç›‘æ§å™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('cnn' æˆ– 'vgg')
            save_dir: ç›‘æ§ç»“æœä¿å­˜ç›®å½•
            device: è®¡ç®—è®¾å¤‡
        """
        self.model_type = model_type
        self.save_dir = save_dir
        self.device = device
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # è®¾ç½®å­—ä½“
        self.font_available = CHINESE_SUPPORTED
        self.labels = get_labels(use_chinese=self.font_available)
        
        # æ°´å°åŒºåŸŸå®šä¹‰ï¼ˆå·¦ä¸‹è§’ï¼‰
        self.watermark_regions = {
            'small': (0.75, 0.85, 0.95, 0.95),   # æ°´å°åŒºåŸŸ-å°
            'medium': (0.70, 0.80, 1.0, 1.0),    # æ°´å°åŒºåŸŸ-ä¸­  
            'large': (0.65, 0.75, 1.0, 1.0)      # æ°´å°åŒºåŸŸ-å¤§
        }
        
        # ä¸­å¿ƒåŒºåŸŸå®šä¹‰ï¼ˆå…‰æ–‘ä¸»è¦åŒºåŸŸï¼‰
        self.center_region = (0.35, 0.35, 0.65, 0.65)  # ä¸­å¿ƒåŒºåŸŸ (x1, y1, x2, y2)
        
        # ç›‘æ§å†å²
        self.attention_history = []
        self.epoch_gradcams = []
        
        print(f"âœ… è®­ç»ƒç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¨¡å‹ç±»å‹: {model_type}")
        print(f"   ä¿å­˜ç›®å½•: {save_dir}")
        print(f"   å­—ä½“æ”¯æŒ: {'ä¸­æ–‡' if self.font_available else 'è‹±æ–‡'}")
        
    def get_target_layer(self, model):
        """è·å–ç›®æ ‡å±‚ç”¨äºGrad-CAM"""
        if self.model_type == 'cnn':
            # CNNæ¨¡å‹ï¼šä½¿ç”¨æœ€åä¸€ä¸ªå·ç§¯å±‚
            target_layer = None
            for name, module in model.named_modules():
                if isinstance(module, nn.Conv2d):
                    target_layer = module
            return target_layer
        elif self.model_type == 'vgg':
            # VGGæ¨¡å‹ï¼šä½¿ç”¨CBAMå±‚
            target_layer = None
            for name, module in model.features.named_modules():
                if 'cbam_5' in name or 'cbam' in name:
                    target_layer = module
                    break
            if target_layer is None:
                # å¦‚æœæ²¡æœ‰CBAMå±‚ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªå·ç§¯å±‚
                for name, module in model.features.named_modules():
                    if isinstance(module, nn.Conv2d):
                        target_layer = module
            return target_layer
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def generate_gradcam(self, model, input_tensor, target_layer):
        """ç”ŸæˆGrad-CAMçƒ­åŠ›å›¾"""
        # ä¿å­˜å½“å‰æ¨¡å‹çš„è®­ç»ƒçŠ¶æ€
        original_training = model.training
        
        # åˆ›å»ºGrad-CAMå¯¹è±¡
        grad_cam = GradCAM(model, target_layer)
        
        # ç”Ÿæˆçƒ­åŠ›å›¾
        try:
            cam = grad_cam(input_tensor)
            return cam
        except Exception as e:
            print(f"âš ï¸ Grad-CAMç”Ÿæˆå¤±è´¥: {e}")
            return None
        finally:
            # æ¸…ç†hooks
            grad_cam.remove_hooks()
            # æ¢å¤æ¨¡å‹åŸå§‹çŠ¶æ€
            model.train(original_training)
    
    def analyze_attention_regions(self, cam, img_size=(224, 224)):
        """åˆ†ææ³¨æ„åŠ›åŒºåŸŸåˆ†å¸ƒ"""
        if cam is None:
            return None
        
        h, w = img_size
        cam_resized = cv2.resize(cam, (w, h))
        
        # è®¡ç®—ä¸åŒåŒºåŸŸçš„å¹³å‡æ³¨æ„åŠ›
        regions_attention = {}
        
        # æ°´å°åŒºåŸŸ
        for region_name, (x1_ratio, y1_ratio, x2_ratio, y2_ratio) in self.watermark_regions.items():
            x1, y1 = int(x1_ratio * w), int(y1_ratio * h)
            x2, y2 = int(x2_ratio * w), int(y2_ratio * h)
            region_attention = cam_resized[y1:y2, x1:x2].mean()
            regions_attention[f'watermark_{region_name}'] = region_attention
        
        # ä¸­å¿ƒåŒºåŸŸ
        x1_ratio, y1_ratio, x2_ratio, y2_ratio = self.center_region
        x1, y1 = int(x1_ratio * w), int(y1_ratio * h)
        x2, y2 = int(x2_ratio * w), int(y2_ratio * h)
        center_attention = cam_resized[y1:y2, x1:x2].mean()
        regions_attention['center'] = center_attention
        
        # è®¡ç®—æ³¨æ„åŠ›æ¯”ä¾‹
        attention_ratios = {}
        for region_name in ['small', 'medium', 'large']:
            watermark_key = f'watermark_{region_name}'
            if center_attention > 0:
                ratio = regions_attention[watermark_key] / center_attention
                attention_ratios[f'{region_name}_ratio'] = ratio
            else:
                attention_ratios[f'{region_name}_ratio'] = 0.0
        
        return {
            'regions_attention': regions_attention,
            'attention_ratios': attention_ratios,
            'cam_resized': cam_resized
        }
    
    def check_watermark_attention(self, analysis_result, threshold=0.3):
        """æ£€æŸ¥æ°´å°åŒºåŸŸå…³æ³¨åº¦æ˜¯å¦è¿‡é«˜"""
        if analysis_result is None:
            return False, "åˆ†æç»“æœä¸ºç©º"
        
        attention_ratios = analysis_result['attention_ratios']
        warnings = []
        
        for region_name in ['small', 'medium', 'large']:
            ratio_key = f'{region_name}_ratio'
            ratio = attention_ratios.get(ratio_key, 0.0)
            
            if ratio > threshold:
                warnings.append(f"æ°´å°åŒºåŸŸ({region_name})å…³æ³¨åº¦è¿‡é«˜: {ratio:.3f} > {threshold}")
        
        has_warning = len(warnings) > 0
        warning_msg = "; ".join(warnings) if warnings else "æ°´å°å…³æ³¨åº¦æ­£å¸¸"
        
        return has_warning, warning_msg
    
    def visualize_gradcam_with_regions(self, model, sample_image, epoch, save_name=None):
        """ç”Ÿæˆå¸¦åŒºåŸŸæ ‡è®°çš„Grad-CAMå¯è§†åŒ–"""
        if save_name is None:
            save_name = f'{self.model_type}_gradcam_epoch_{epoch}'
        
        # ğŸ”§ ä¿®å¤å›¾åƒå¤„ç†é€»è¾‘
        if isinstance(sample_image, torch.Tensor):
            # å¦‚æœè¾“å…¥æ˜¯tensor
            if sample_image.dim() == 4:  # æ‰¹æ¬¡ç»´åº¦
                input_tensor = sample_image.to(self.device)
                img_for_display = sample_image[0]  # å–ç¬¬ä¸€ä¸ª
            elif sample_image.dim() == 3:  # å•å¼ å›¾åƒ
                input_tensor = sample_image.unsqueeze(0).to(self.device)
                img_for_display = sample_image
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„tensorç»´åº¦: {sample_image.dim()}")
            
            # å‡†å¤‡æ˜¾ç¤ºç”¨çš„å›¾åƒ
            if img_for_display.dim() == 3:
                # ä» (C, H, W) è½¬æ¢ä¸º (H, W, C)
                img_np = img_for_display.permute(1, 2, 0).cpu().numpy()
            else:
                img_np = img_for_display.cpu().numpy()
        else:
            # å¦‚æœè¾“å…¥æ˜¯PILå›¾åƒæˆ–numpyæ•°ç»„
            if isinstance(sample_image, Image.Image):
                img_np = np.array(sample_image)
            else:
                img_np = sample_image
            
            # è½¬æ¢ä¸ºtensorç”¨äºæ¨¡å‹è¾“å…¥
            if len(img_np.shape) == 3 and img_np.shape[2] == 3:
                img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).float() / 255.0
            else:
                raise ValueError("ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼")
            
            input_tensor = img_tensor.unsqueeze(0).to(self.device)
        
        # ç”ŸæˆGrad-CAM
        target_layer = self.get_target_layer(model)
        if target_layer is None:
            print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°åˆé€‚çš„ç›®æ ‡å±‚ï¼Œè·³è¿‡Grad-CAMç”Ÿæˆ")
            return None
        
        cam = self.generate_gradcam(model, input_tensor, target_layer)
        if cam is None:
            print(f"âš ï¸ è·³è¿‡epoch {epoch}çš„Grad-CAMå¯è§†åŒ–")
            return None
        
        # åˆ†ææ³¨æ„åŠ›åŒºåŸŸ
        analysis_result = self.analyze_attention_regions(cam)
        if analysis_result is None:
            print(f"âš ï¸ è·³è¿‡epoch {epoch}çš„æ³¨æ„åŠ›åˆ†æ")
            return None
        
        # æ£€æŸ¥æ°´å°å…³æ³¨åº¦
        has_warning, warning_msg = self.check_watermark_attention(analysis_result)
        
        # ğŸ”§ ä¿®å¤å›¾åƒæ˜¾ç¤ºé€»è¾‘
        # ç¡®ä¿å›¾åƒæ˜¯æ­£ç¡®çš„æ ¼å¼ (H, W, C) ä¸”å€¼åœ¨ [0, 255]
        if img_np.max() <= 1.0:
            # å¦‚æœå›¾åƒå·²å½’ä¸€åŒ–ï¼Œéœ€è¦åå½’ä¸€åŒ–
            if img_np.shape[2] == 3:  # RGBå›¾åƒ
                # ImageNetæ ‡å‡†åå½’ä¸€åŒ–
                mean = np.array([0.485, 0.456, 0.406])
                std = np.array([0.229, 0.224, 0.225])
                img_np = img_np * std + mean
                img_np = np.clip(img_np, 0, 1)
            img_np = (img_np * 255).astype(np.uint8)
        else:
            img_np = img_np.astype(np.uint8)
        
        # è°ƒæ•´çƒ­åŠ›å›¾å¤§å°
        h, w = img_np.shape[:2]
        cam_resized = cv2.resize(cam, (w, h))
        
        # ğŸ¨ åˆ›å»ºå¯è§†åŒ– - ä½¿ç”¨ä¿®å¤åçš„å­—ä½“
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        title = f'Epoch {epoch} Grad-CAM ç›‘æ§åˆ†æ' if self.font_available else f'Epoch {epoch} Grad-CAM Analysis'
        fig.suptitle(title, fontsize=16)
        
        # åŸå§‹å›¾åƒ
        axes[0, 0].imshow(img_np)
        axes[0, 0].set_title(self.labels['original_image'])
        axes[0, 0].axis('off')
        
        # çƒ­åŠ›å›¾
        heatmap = cm.jet(cam_resized)[:, :, :3]
        axes[0, 1].imshow(heatmap)
        axes[0, 1].set_title(self.labels['gradcam_heatmap'])
        axes[0, 1].axis('off')
        
        # å åŠ å›¾åƒ
        overlay = cv2.addWeighted(img_np, 0.6, (heatmap * 255).astype(np.uint8), 0.4, 0)
        axes[1, 0].imshow(overlay)
        axes[1, 0].set_title(self.labels['overlay_visualization'])
        axes[1, 0].axis('off')
        
        # æ·»åŠ åŒºåŸŸæ ‡è®°
        self._add_region_markers(axes[1, 0], w, h)
        
        # æ³¨æ„åŠ›åˆ†æç»“æœ
        analysis_text = self._format_analysis_text(analysis_result, has_warning, warning_msg)
        axes[1, 1].text(0.05, 0.95, analysis_text, transform=axes[1, 1].transAxes,
                       fontsize=10, verticalalignment='top', fontfamily='monospace')
        axes[1, 1].axis('off')
        
        # ä¿å­˜å›¾åƒ
        save_path = os.path.join(self.save_dir, f'{save_name}.png')
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # è®°å½•å†å²
        self.attention_history.append({
            'epoch': epoch,
            'analysis': analysis_result,
            'has_warning': has_warning,
            'warning_msg': warning_msg
        })
        
        print(f"âœ“ Epoch {epoch} Grad-CAMåˆ†æå®Œæˆ: {save_path}")
        if has_warning:
            print(f"âš ï¸ è­¦å‘Š: {warning_msg}")
        else:
            print(f"âœ“ æ°´å°å…³æ³¨åº¦æ£€æŸ¥é€šè¿‡")
        
        return analysis_result
    
    def _add_region_markers(self, ax, w, h):
        """åœ¨å›¾åƒä¸Šæ·»åŠ åŒºåŸŸæ ‡è®°"""
        # æ°´å°åŒºåŸŸæ ‡è®°ï¼ˆçº¢è‰²ï¼‰
        x1_ratio, y1_ratio, x2_ratio, y2_ratio = self.watermark_regions['medium']
        x1, y1 = int(x1_ratio * w), int(y1_ratio * h)
        x2, y2 = int(x2_ratio * w), int(y2_ratio * h)
        
        watermark_rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, 
                                     linewidth=2, edgecolor='red', facecolor='none', alpha=0.8)
        ax.add_patch(watermark_rect)
        watermark_label = 'æ°´å°åŒºåŸŸ' if self.font_available else 'Watermark'
        ax.text(x1, y1-5, watermark_label, color='red', fontsize=10, weight='bold')
        
        # ä¸­å¿ƒåŒºåŸŸæ ‡è®°ï¼ˆç»¿è‰²ï¼‰
        x1_ratio, y1_ratio, x2_ratio, y2_ratio = self.center_region
        x1, y1 = int(x1_ratio * w), int(y1_ratio * h)
        x2, y2 = int(x2_ratio * w), int(y2_ratio * h)
        
        center_rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, 
                                  linewidth=2, edgecolor='green', facecolor='none', alpha=0.8)
        ax.add_patch(center_rect)
        center_label = 'ä¸­å¿ƒåŒºåŸŸ' if self.font_available else 'Center Region'
        ax.text(x1, y1-5, center_label, color='green', fontsize=10, weight='bold')
    
    def _format_analysis_text(self, analysis_result, has_warning, warning_msg):
        """æ ¼å¼åŒ–åˆ†æç»“æœæ–‡æœ¬"""
        regions_attention = analysis_result['regions_attention']
        attention_ratios = analysis_result['attention_ratios']
        
        text = "æ³¨æ„åŠ›åŒºåŸŸåˆ†æ:\n\n" if self.font_available else "Attention Analysis:\n\n"
        
        # åŒºåŸŸæ³¨æ„åŠ›å€¼
        text += "åŒºåŸŸæ³¨æ„åŠ›å¼ºåº¦:\n" if self.font_available else "Region Attention:\n"
        text += f"ä¸­å¿ƒåŒºåŸŸ: {regions_attention['center']:.3f}\n"
        text += f"æ°´å°åŒºåŸŸ(å°): {regions_attention['watermark_small']:.3f}\n"
        text += f"æ°´å°åŒºåŸŸ(ä¸­): {regions_attention['watermark_medium']:.3f}\n"
        text += f"æ°´å°åŒºåŸŸ(å¤§): {regions_attention['watermark_large']:.3f}\n\n"
        
        # æ³¨æ„åŠ›æ¯”ä¾‹
        text += "æ°´å°/ä¸­å¿ƒæ³¨æ„åŠ›æ¯”ä¾‹:\n" if self.font_available else "Watermark/Center Ratios:\n"
        text += f"å°åŒºåŸŸæ¯”ä¾‹: {attention_ratios['small_ratio']:.3f}\n"
        text += f"ä¸­åŒºåŸŸæ¯”ä¾‹: {attention_ratios['medium_ratio']:.3f}\n"
        text += f"å¤§åŒºåŸŸæ¯”ä¾‹: {attention_ratios['large_ratio']:.3f}\n\n"
        
        # è­¦å‘ŠçŠ¶æ€
        status = "âš ï¸ è­¦å‘Š" if has_warning else "âœ“ æ­£å¸¸"
        text += f"çŠ¶æ€: {status}\n"
        text += f"è¯¦æƒ…: {warning_msg}\n\n"
        
        # å»ºè®®
        if has_warning:
            text += "å»ºè®®:\n" if self.font_available else "Suggestions:\n"
            text += "â€¢ æ£€æŸ¥æ•°æ®å¢å¼ºç­–ç•¥\n"
            text += "â€¢ è€ƒè™‘æ·»åŠ æ³¨æ„åŠ›æ­£åˆ™åŒ–\n"
            text += "â€¢ ç›‘æ§åç»­è®­ç»ƒè¿‡ç¨‹\n"
        else:
            text += "æ¨¡å‹å…³æ³¨åŒºåŸŸæ­£ç¡®" if self.font_available else "Model attention is correct"
        
        return text
    
    def generate_monitoring_report(self, save_name='training_monitoring_report'):
        """ç”Ÿæˆè®­ç»ƒç›‘æ§æŠ¥å‘Š"""
        if not self.attention_history:
            print("âš ï¸ æ²¡æœ‰ç›‘æ§æ•°æ®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return
        
        # æå–æ•°æ®
        epochs = [item['epoch'] for item in self.attention_history]
        center_attentions = [item['analysis']['regions_attention']['center'] for item in self.attention_history]
        watermark_ratios = [item['analysis']['attention_ratios']['medium_ratio'] for item in self.attention_history]
        warnings = [item['has_warning'] for item in self.attention_history]
        
        # åˆ›å»ºæŠ¥å‘Šå›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('è®­ç»ƒç›‘æ§æŠ¥å‘Š' if self.font_available else 'Training Monitoring Report', fontsize=16)
        
        # ä¸­å¿ƒåŒºåŸŸæ³¨æ„åŠ›è¶‹åŠ¿
        axes[0, 0].plot(epochs, center_attentions, 'b-o', linewidth=2, markersize=4)
        axes[0, 0].set_title('ä¸­å¿ƒåŒºåŸŸæ³¨æ„åŠ›è¶‹åŠ¿' if self.font_available else 'Center Region Attention Trend')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('æ³¨æ„åŠ›å¼ºåº¦' if self.font_available else 'Attention Intensity')
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ°´å°/ä¸­å¿ƒæ³¨æ„åŠ›æ¯”ä¾‹è¶‹åŠ¿
        axes[0, 1].plot(epochs, watermark_ratios, 'r-o', linewidth=2, markersize=4)
        axes[0, 1].axhline(y=0.3, color='orange', linestyle='--', alpha=0.7, label='è­¦å‘Šé˜ˆå€¼')
        axes[0, 1].set_title('æ°´å°/ä¸­å¿ƒæ³¨æ„åŠ›æ¯”ä¾‹' if self.font_available else 'Watermark/Center Ratio')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('æ¯”ä¾‹' if self.font_available else 'Ratio')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # è­¦å‘Šç»Ÿè®¡
        warning_epochs = [epoch for epoch, warning in zip(epochs, warnings) if warning]
        axes[1, 0].bar(['æ­£å¸¸', 'è­¦å‘Š'] if self.font_available else ['Normal', 'Warning'], 
                      [len(epochs) - len(warning_epochs), len(warning_epochs)],
                      color=['green', 'red'], alpha=0.7)
        axes[1, 0].set_title('è­¦å‘Šç»Ÿè®¡' if self.font_available else 'Warning Statistics')
        axes[1, 0].set_ylabel('Epochæ•°é‡' if self.font_available else 'Number of Epochs')
        
        # ç›‘æ§æ€»ç»“
        summary_text = self._generate_summary_text(epochs, center_attentions, watermark_ratios, warnings)
        axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes,
                        fontsize=10, verticalalignment='top', fontfamily='monospace')
        axes[1, 1].axis('off')
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.save_dir, f'{save_name}.png')
        plt.tight_layout()
        plt.savefig(report_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ è®­ç»ƒç›‘æ§æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path
    
    def _generate_summary_text(self, epochs, center_attentions, watermark_ratios, warnings):
        """ç”Ÿæˆç›‘æ§æ€»ç»“æ–‡æœ¬"""
        total_epochs = len(epochs)
        warning_count = sum(warnings)
        avg_center_attention = np.mean(center_attentions)
        avg_watermark_ratio = np.mean(watermark_ratios)
        max_watermark_ratio = np.max(watermark_ratios)
        
        text = "ç›‘æ§æ€»ç»“:\n\n" if self.font_available else "Monitoring Summary:\n\n"
        text += f"æ€»ç›‘æ§è½®æ•°: {total_epochs}\n"
        text += f"è­¦å‘Šæ¬¡æ•°: {warning_count}\n"
        text += f"è­¦å‘Šç‡: {warning_count/total_epochs*100:.1f}%\n\n"
        
        text += f"å¹³å‡ä¸­å¿ƒæ³¨æ„åŠ›: {avg_center_attention:.3f}\n"
        text += f"å¹³å‡æ°´å°æ¯”ä¾‹: {avg_watermark_ratio:.3f}\n"
        text += f"æœ€å¤§æ°´å°æ¯”ä¾‹: {max_watermark_ratio:.3f}\n\n"
        
        # è¯„ä¼°ç»“æœ
        if warning_count == 0:
            text += "âœ“ è®­ç»ƒè¿‡ç¨‹æ­£å¸¸\n"
            text += "æ¨¡å‹æ­£ç¡®å…³æ³¨ä¸­å¿ƒåŒºåŸŸ"
        elif warning_count / total_epochs < 0.3:
            text += "âš ï¸ å¶æœ‰å¼‚å¸¸\n"
            text += "å»ºè®®ç»§ç»­ç›‘æ§"
        else:
            text += "âŒ é¢‘ç¹è­¦å‘Š\n"
            text += "å»ºè®®è°ƒæ•´è®­ç»ƒç­–ç•¥"
        
        return text


class GradCAM:
    """Grad-CAMå®ç°"""
    
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        self.hooks = []
        self._register_hooks()
    
    def _register_hooks(self):
        def forward_hook(module, input, output):
            self.activations = output.detach()
        
        def backward_hook(module, grad_in, grad_out):
            self.gradients = grad_out[0].detach()
        
        # ä½¿ç”¨æ–°çš„hookæ–¹å¼é¿å…è­¦å‘Š
        self.hooks.append(self.target_layer.register_forward_hook(forward_hook))
        self.hooks.append(self.target_layer.register_full_backward_hook(backward_hook))
    
    def remove_hooks(self):
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
    
    def __call__(self, input_tensor, target_index=None):
        # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼ä»¥è®¡ç®—æ¢¯åº¦
        self.model.train()
        
        # ç¡®ä¿è¾“å…¥tensoréœ€è¦æ¢¯åº¦
        if not input_tensor.requires_grad:
            input_tensor = input_tensor.requires_grad_(True)
        
        # å‰å‘ä¼ æ’­
        output = self.model(input_tensor)
        
        # å¤„ç†ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
        if isinstance(output, tuple):
            # CNNæ¨¡å‹è¿”å›(concentration, features)å…ƒç»„
            target_output = output[0]
        else:
            # VGGæ¨¡å‹ç›´æ¥è¿”å›concentration
            target_output = output
        
        # å¦‚æœæ˜¯å›å½’ä»»åŠ¡ï¼Œä½¿ç”¨è¾“å‡ºå€¼ä½œä¸ºç›®æ ‡
        if target_index is None:
            target = target_output.mean()
        else:
            target = target_output[0, target_index]
        
        # ç¡®ä¿targetéœ€è¦æ¢¯åº¦
        if not target.requires_grad:
            print("âš ï¸ è­¦å‘Šï¼štargetä¸éœ€è¦æ¢¯åº¦ï¼ŒGradCAMå¯èƒ½å¤±è´¥")
            return np.zeros((224, 224))
        
        # åå‘ä¼ æ’­
        self.model.zero_grad()
        target.backward(retain_graph=True)
        
        # æ£€æŸ¥æ¢¯åº¦å’Œæ¿€æ´»æ˜¯å¦å­˜åœ¨
        if self.gradients is None or self.activations is None:
            print("âš ï¸ è­¦å‘Šï¼šæ¢¯åº¦æˆ–æ¿€æ´»ä¸ºç©ºï¼Œè¿”å›é›¶çŸ©é˜µ")
            return np.zeros((224, 224))
        
        # è®¡ç®—æƒé‡
        weights = torch.mean(self.gradients, dim=[2, 3], keepdim=True)
        
        # ç”ŸæˆCAM
        cam = torch.sum(weights * self.activations, dim=1).squeeze()
        
        # åº”ç”¨ReLUå¹¶å½’ä¸€åŒ–
        cam = torch.relu(cam)
        cam = cam - cam.min()
        if cam.max() > 0:
            cam = cam / cam.max()
        
        # ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸º224x224
        cam_np = cam.detach().cpu().numpy()
        if cam_np.shape != (224, 224):
            import cv2
            cam_np = cv2.resize(cam_np, (224, 224))
        
        return cam_np 