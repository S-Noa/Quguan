#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºGrad-CAMæµ“åº¦å¯è§†åŒ–å·¥å…·
æ”¯æŒæŒ‰æ‚¬æµ®ç‰©æµ“åº¦ä»æ•°æ®é›†éšæœºé€‰å–å¯¹åº”æµ“åº¦å›¾åƒç”ŸæˆGrad-CAMå¯è§†åŒ–å›¾åƒ
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import cv2
from PIL import Image
import json
import random
from datetime import datetime
from pathlib import Path
from collections import defaultdict

# æ·»åŠ è·¯å¾„
sys.path.append('src')
sys.path.append('cloud_vgg_training_package')

# è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class GradCAMConcentrationVisualizer:
    """æŒ‰æµ“åº¦çš„Grad-CAMå¯è§†åŒ–å™¨"""
    
    def __init__(self, output_dir=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"gradcam_concentration_analysis_{timestamp}"
        
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸ” Grad-CAMæµ“åº¦å¯è§†åŒ–å·¥å…·åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # Grad-CAMç›¸å…³
        self.gradients = None
        self.activations = None
    
    def register_hooks(self, model, target_layer_name):
        """æ³¨å†Œé’©å­å‡½æ•°"""
        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0]
        
        def forward_hook(module, input, output):
            self.activations = output
        
        # æ‰¾åˆ°ç›®æ ‡å±‚
        target_layer = None
        for name, module in model.named_modules():
            if target_layer_name in name:
                target_layer = module
                break
        
        if target_layer is None:
            raise ValueError(f"æœªæ‰¾åˆ°ç›®æ ‡å±‚: {target_layer_name}")
        
        # æ³¨å†Œé’©å­
        target_layer.register_forward_hook(forward_hook)
        target_layer.register_backward_hook(backward_hook)
        
        print(f"   å·²æ³¨å†Œé’©å­åˆ°å±‚: {target_layer_name}")
        return target_layer
    
    def generate_gradcam(self, model, image, target_layer_name, class_idx=None):
        """ç”ŸæˆGrad-CAMçƒ­åŠ›å›¾"""
        # æ³¨å†Œé’©å­
        self.register_hooks(model, target_layer_name)
        
        # å‰å‘ä¼ æ’­
        model.eval()
        image_tensor = image.unsqueeze(0).to(self.device)
        image_tensor.requires_grad_(True)
        
        output = model(image_tensor)
        
        # å¤„ç†å¯èƒ½çš„tupleè¾“å‡º
        if isinstance(output, tuple):
            output = output[0]
        
        # å¦‚æœæ˜¯å›å½’ä»»åŠ¡ï¼Œç›´æ¥å¯¹è¾“å‡ºæ±‚æ¢¯åº¦
        if class_idx is None:
            score = output[0]  # å›å½’è¾“å‡ºçš„ç¬¬ä¸€ä¸ªå€¼
        else:
            score = output[0][class_idx]
        
        # åå‘ä¼ æ’­
        model.zero_grad()
        score.backward(retain_graph=True)
        
        # è·å–æ¢¯åº¦å’Œæ¿€æ´»
        gradients = self.gradients[0].cpu().data.numpy()
        activations = self.activations[0].cpu().data.numpy()
        
        # è®¡ç®—æƒé‡
        weights = np.mean(gradients, axis=(1, 2))
        
        # ç”ŸæˆGrad-CAM
        cam = np.zeros(activations.shape[1:], dtype=np.float32)
        for i, w in enumerate(weights):
            cam += w * activations[i, :, :]
        
        # ReLUæ¿€æ´»
        cam = np.maximum(cam, 0)
        
        # å½’ä¸€åŒ–
        if cam.max() > 0:
            cam = cam / cam.max()
        
        return cam
    
    def apply_colormap_on_image(self, org_im, activation, colormap_name='jet'):
        """å°†çƒ­åŠ›å›¾åº”ç”¨åˆ°åŸå§‹å›¾åƒä¸Š"""
        # å°†æ¿€æ´»å›¾è°ƒæ•´åˆ°åŸå§‹å›¾åƒå¤§å°
        heatmap = cv2.resize(activation, (org_im.shape[1], org_im.shape[0]))
        heatmap = np.uint8(255 * heatmap)
        
        # åº”ç”¨é¢œè‰²æ˜ å°„
        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        
        # å åŠ åˆ°åŸå§‹å›¾åƒ
        superimposed_img = heatmap * 0.4 + org_im * 0.6
        superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)
        
        return superimposed_img, heatmap
    
    def load_dataset_by_concentration(self, dataset_path, bg_mode='all'):
        """æŒ‰æµ“åº¦åŠ è½½æ•°æ®é›†"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {dataset_path}")
        
        # åŠ è½½æ•°æ®é›†
        from feature_dataset_loader import create_feature_dataloader
        
        bg_filter, power_filter = self._parse_bg_mode(bg_mode)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¸æ‰“ä¹±é¡ºåºï¼‰
        dataloader, dataset = create_feature_dataloader(
            feature_dataset_path=dataset_path,
            batch_size=1,
            shuffle=False,
            bg_type=bg_filter,
            power_filter=power_filter
        )
        
        # æŒ‰æµ“åº¦åˆ†ç»„
        concentration_groups = defaultdict(list)
        
        print(f"   æ­£åœ¨æŒ‰æµ“åº¦åˆ†ç»„... (å…±{len(dataset)}ä¸ªæ ·æœ¬)")
        
        for idx, (image, concentration, metadata) in enumerate(dataset):
            # å°†æµ“åº¦å››èˆäº”å…¥åˆ°æœ€è¿‘çš„æ•´æ•°
            conc_rounded = round(float(concentration))
            
            # è·å–åŸå§‹å›¾åƒè·¯å¾„
            original_image_path = dataset.image_files[idx] if hasattr(dataset, 'image_files') else None
            
            concentration_groups[conc_rounded].append({
                'index': idx,
                'image': image,
                'concentration': float(concentration),
                'metadata': metadata,
                'original_image_path': original_image_path  # æ·»åŠ åŸå§‹å›¾åƒè·¯å¾„
            })
            
            if (idx + 1) % 1000 == 0:
                print(f"   è¿›åº¦: {idx + 1}/{len(dataset)}")
        
        print(f"   å®Œæˆåˆ†ç»„ï¼Œå…±{len(concentration_groups)}ä¸ªæµ“åº¦çº§åˆ«")
        
        # ç»Ÿè®¡æ¯ä¸ªæµ“åº¦çš„æ ·æœ¬æ•°
        conc_stats = {}
        for conc, samples in concentration_groups.items():
            conc_stats[conc] = len(samples)
        
        # æ˜¾ç¤ºæµ“åº¦åˆ†å¸ƒ
        sorted_concs = sorted(conc_stats.keys())
        print(f"   æµ“åº¦èŒƒå›´: {min(sorted_concs)} - {max(sorted_concs)} mg/L")
        print(f"   æµ“åº¦çº§åˆ«æ•°: {len(sorted_concs)}")
        
        return concentration_groups, conc_stats
    
    def load_original_image(self, image_path):
        """åŠ è½½åŸå§‹å›¾åƒï¼ˆæœªç»é¢„å¤„ç†ï¼‰"""
        try:
            from PIL import Image
            original_image = Image.open(image_path).convert('RGB')
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            original_np = np.array(original_image)
            return original_np
        except Exception as e:
            print(f"   è­¦å‘Š: æ— æ³•åŠ è½½åŸå§‹å›¾åƒ {image_path}: {e}")
            return None
    
    def select_samples_by_concentration(self, concentration_groups, 
                                      min_conc, max_conc, interval, 
                                      samples_per_conc=10):
        """æŒ‰æµ“åº¦åŒºé—´é€‰æ‹©æ ·æœ¬"""
        print(f"\nğŸ¯ æŒ‰æµ“åº¦é€‰æ‹©æ ·æœ¬:")
        print(f"   æµ“åº¦èŒƒå›´: {min_conc} - {max_conc} mg/L")
        print(f"   é—´éš”: {interval} mg/L")
        print(f"   æ¯ä¸ªæµ“åº¦é€‰æ‹©: {samples_per_conc} ä¸ªæ ·æœ¬")
        
        selected_samples = []
        target_concentrations = list(range(min_conc, max_conc + 1, interval))
        
        for target_conc in target_concentrations:
            # æ‰¾åˆ°æœ€æ¥è¿‘çš„æµ“åº¦çº§åˆ«
            available_concs = list(concentration_groups.keys())
            closest_conc = min(available_concs, 
                             key=lambda x: abs(x - target_conc))
            
            # æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
            if abs(closest_conc - target_conc) <= interval / 2:
                samples = concentration_groups[closest_conc]
                
                # éšæœºé€‰æ‹©æ ·æœ¬
                num_to_select = min(samples_per_conc, len(samples))
                selected = random.sample(samples, num_to_select)
                
                for sample in selected:
                    sample['target_concentration'] = target_conc
                    sample['actual_concentration'] = closest_conc
                
                selected_samples.extend(selected)
                
                print(f"   {target_conc} mg/L: é€‰æ‹©äº†{num_to_select}ä¸ªæ ·æœ¬ (å®é™…æµ“åº¦:{closest_conc} mg/L)")
            else:
                print(f"   {target_conc} mg/L: æœªæ‰¾åˆ°åˆé€‚çš„æ ·æœ¬ (æœ€è¿‘:{closest_conc} mg/L)")
        
        print(f"\n   æ€»å…±é€‰æ‹©äº†{len(selected_samples)}ä¸ªæ ·æœ¬")
        return selected_samples
    
    def visualize_concentration_samples(self, model, model_name, target_layer_name,
                                      selected_samples, save_individual=True):
        """å¯è§†åŒ–é€‰å®šçš„æµ“åº¦æ ·æœ¬"""
        print(f"\nğŸ¨ ç”ŸæˆGrad-CAMå¯è§†åŒ–...")
        
        # æŒ‰ç›®æ ‡æµ“åº¦åˆ†ç»„
        conc_groups = defaultdict(list)
        for sample in selected_samples:
            conc_groups[sample['target_concentration']].append(sample)
        
        all_visualizations = []
        
        for target_conc in sorted(conc_groups.keys()):
            samples = conc_groups[target_conc]
            print(f"   å¤„ç†æµ“åº¦ {target_conc} mg/L: {len(samples)} ä¸ªæ ·æœ¬")
            
            conc_visualizations = []
            
            for i, sample in enumerate(samples):
                try:
                    # ç”ŸæˆGrad-CAM
                    cam = self.generate_gradcam(
                        model, sample['image'], target_layer_name
                    )
                    
                    # è½¬æ¢é¢„å¤„ç†åçš„å›¾åƒæ ¼å¼ï¼ˆç”¨äºç”Ÿæˆçƒ­åŠ›å›¾ï¼‰
                    processed_image_np = sample['image'].permute(1, 2, 0).numpy()
                    processed_image_np = (processed_image_np * 255).astype(np.uint8)
                    
                    # åŠ è½½åŸå§‹å›¾åƒ
                    original_image_np = None
                    if sample.get('original_image_path'):
                        original_image_np = self.load_original_image(sample['original_image_path'])
                    
                    # å¦‚æœæ— æ³•åŠ è½½åŸå§‹å›¾åƒï¼Œä½¿ç”¨é¢„å¤„ç†åçš„å›¾åƒä½œä¸ºæ›¿ä»£
                    if original_image_np is None:
                        original_image_np = processed_image_np
                        print(f"     æ ·æœ¬ {i}: ä½¿ç”¨é¢„å¤„ç†å›¾åƒä½œä¸ºåŸå§‹å›¾åƒ")
                    
                    # åº”ç”¨çƒ­åŠ›å›¾åˆ°é¢„å¤„ç†å›¾åƒï¼ˆç”¨äºGrad-CAMç”Ÿæˆï¼‰
                    superimposed_processed, heatmap = self.apply_colormap_on_image(
                        processed_image_np, cam
                    )
                    
                    # åº”ç”¨çƒ­åŠ›å›¾åˆ°åŸå§‹å›¾åƒï¼ˆç”¨äºå±•ç¤ºï¼‰
                    superimposed_original, _ = self.apply_colormap_on_image(
                        original_image_np, cam
                    )
                    
                    # é¢„æµ‹æµ“åº¦
                    with torch.no_grad():
                        model.eval()
                        image_tensor = sample['image'].unsqueeze(0).to(self.device)
                        output = model(image_tensor)
                        if isinstance(output, tuple):
                            output = output[0]
                        predicted_conc = output.item()
                    
                    visualization = {
                        'target_concentration': target_conc,
                        'actual_concentration': sample['actual_concentration'],
                        'predicted_concentration': predicted_conc,
                        'original_image': original_image_np,  # çœŸæ­£çš„åŸå§‹å›¾åƒ
                        'processed_image': processed_image_np,  # é¢„å¤„ç†åçš„å›¾åƒ
                        'heatmap': heatmap,
                        'superimposed_processed': superimposed_processed,  # çƒ­åŠ›å›¾+é¢„å¤„ç†å›¾åƒ
                        'superimposed_original': superimposed_original,    # çƒ­åŠ›å›¾+åŸå§‹å›¾åƒ
                        'sample_index': sample['index'],
                        'image_path': sample.get('original_image_path', 'Unknown')
                    }
                    
                    conc_visualizations.append(visualization)
                    
                    # ä¿å­˜å•ä¸ªå¯è§†åŒ–
                    if save_individual:
                        self._save_individual_visualization(
                            visualization, model_name, target_conc, i
                        )
                    
                except Exception as e:
                    print(f"     æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
                    continue
            
            if conc_visualizations:
                # åˆ›å»ºæµ“åº¦ç½‘æ ¼
                grid_path = self._create_concentration_grid(
                    conc_visualizations, model_name, target_conc
                )
                
                all_visualizations.extend(conc_visualizations)
        
        # åˆ›å»ºæ€»è§ˆå¯è§†åŒ–
        if all_visualizations:
            overview_path = self._create_overview_visualization(
                all_visualizations, model_name
            )
            print(f"   âœ… æ€»è§ˆå¯è§†åŒ–å·²ä¿å­˜: {overview_path}")
        
        return all_visualizations
    
    def _save_individual_visualization(self, viz, model_name, target_conc, sample_idx):
        """ä¿å­˜å•ä¸ªå¯è§†åŒ–å›¾åƒ"""
        # åˆ›å»ºå­ç›®å½•
        conc_dir = os.path.join(self.output_dir, f"concentration_{target_conc}mg_L")
        os.makedirs(conc_dir, exist_ok=True)
        
        # åˆ›å»ºç»„åˆå›¾ï¼ˆ2è¡Œ2åˆ—ï¼‰
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # åŸå§‹æ•°æ®å›¾åƒ
        axes[0, 0].imshow(viz['original_image'])
        axes[0, 0].set_title('åŸå§‹æ•°æ®å›¾åƒ', fontsize=12)
        axes[0, 0].axis('off')
        
        # é¢„å¤„ç†åå›¾åƒ
        axes[0, 1].imshow(viz['processed_image'])
        axes[0, 1].set_title('é¢„å¤„ç†åå›¾åƒ', fontsize=12)
        axes[0, 1].axis('off')
        
        # Grad-CAMçƒ­åŠ›å›¾
        axes[1, 0].imshow(viz['heatmap'])
        axes[1, 0].set_title('Grad-CAMçƒ­åŠ›å›¾', fontsize=12)
        axes[1, 0].axis('off')
        
        # åŸå§‹å›¾åƒ+çƒ­åŠ›å›¾å åŠ 
        axes[1, 1].imshow(viz['superimposed_original'])
        axes[1, 1].set_title('åŸå§‹å›¾åƒ+çƒ­åŠ›å›¾', fontsize=12)
        axes[1, 1].axis('off')
        
        # æ·»åŠ è¯¦ç»†ä¿¡æ¯
        info_text = (f"ç›®æ ‡æµ“åº¦: {viz['target_concentration']} mg/L\n"
                    f"å®é™…æµ“åº¦: {viz['actual_concentration']:.1f} mg/L\n"
                    f"é¢„æµ‹æµ“åº¦: {viz['predicted_concentration']:.1f} mg/L\n"
                    f"å›¾åƒè·¯å¾„: {os.path.basename(viz['image_path'])}")
        
        fig.suptitle(f"{model_name} - æ ·æœ¬ {sample_idx + 1}\n{info_text}", 
                    fontsize=14, y=0.95)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.85)  # ä¸ºæ ‡é¢˜ç•™å‡ºç©ºé—´
        
        # ä¿å­˜
        filename = f"sample_{sample_idx + 1:02d}_{target_conc}mg_L_detailed.png"
        save_path = os.path.join(conc_dir, filename)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # åŒæ—¶ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆåªæœ‰åŸå§‹å›¾åƒã€çƒ­åŠ›å›¾ã€å åŠ å›¾åƒï¼‰
        fig_simple, axes_simple = plt.subplots(1, 3, figsize=(15, 5))
        
        # åŸå§‹æ•°æ®å›¾åƒ
        axes_simple[0].imshow(viz['original_image'])
        axes_simple[0].set_title('åŸå§‹æ•°æ®å›¾åƒ')
        axes_simple[0].axis('off')
        
        # çƒ­åŠ›å›¾
        axes_simple[1].imshow(viz['heatmap'])
        axes_simple[1].set_title('Grad-CAMçƒ­åŠ›å›¾')
        axes_simple[1].axis('off')
        
        # å åŠ å›¾åƒ
        axes_simple[2].imshow(viz['superimposed_original'])
        axes_simple[2].set_title('åŸå§‹å›¾åƒ+çƒ­åŠ›å›¾')
        axes_simple[2].axis('off')
        
        # ç®€åŒ–ä¿¡æ¯
        info_simple = (f"ç›®æ ‡: {viz['target_concentration']} mg/L | "
                      f"å®é™…: {viz['actual_concentration']:.1f} mg/L | "
                      f"é¢„æµ‹: {viz['predicted_concentration']:.1f} mg/L")
        
        fig_simple.suptitle(f"{model_name} - æ ·æœ¬ {sample_idx + 1}\n{info_simple}", 
                           fontsize=12)
        
        plt.tight_layout()
        
        # ä¿å­˜ç®€åŒ–ç‰ˆæœ¬
        filename_simple = f"sample_{sample_idx + 1:02d}_{target_conc}mg_L_simple.png"
        save_path_simple = os.path.join(conc_dir, filename_simple)
        plt.savefig(save_path_simple, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_concentration_grid(self, visualizations, model_name, target_conc):
        """åˆ›å»ºå•ä¸ªæµ“åº¦çš„ç½‘æ ¼å›¾"""
        if not visualizations:
            return None # Changed to return None
        
        num_samples = len(visualizations)
        cols = min(5, num_samples)
        rows = (num_samples + cols - 1) // cols
        
        fig, axes = plt.subplots(rows * 3, cols, figsize=(cols * 3, rows * 8))
        
        if rows == 1 and cols == 1:
            axes = axes.reshape(3, 1)
        elif rows == 1:
            axes = axes.reshape(3, cols)
        elif cols == 1:
            axes = axes.reshape(rows * 3, 1)
        
        for i, viz in enumerate(visualizations):
            col = i % cols
            row_base = (i // cols) * 3
            
            # åŸå§‹å›¾åƒ
            if rows == 1:
                axes[0, col].imshow(viz['original_image'])
                axes[0, col].set_title(f'åŸå§‹ #{i+1}')
                axes[0, col].axis('off')
                
                # çƒ­åŠ›å›¾
                axes[1, col].imshow(viz['heatmap'])
                axes[1, col].set_title(f'çƒ­åŠ›å›¾ #{i+1}')
                axes[1, col].axis('off')
                
                # å åŠ å›¾åƒ
                axes[2, col].imshow(viz['superimposed_original']) # Changed to superimposed_original
                axes[2, col].set_title(f'å åŠ  #{i+1}\né¢„æµ‹:{viz["predicted_concentration"]:.1f}')
                axes[2, col].axis('off')
            else:
                axes[row_base, col].imshow(viz['original_image'])
                axes[row_base, col].set_title(f'åŸå§‹ #{i+1}')
                axes[row_base, col].axis('off')
                
                # çƒ­åŠ›å›¾
                axes[row_base + 1, col].imshow(viz['heatmap'])
                axes[row_base + 1, col].set_title(f'çƒ­åŠ›å›¾ #{i+1}')
                axes[row_base + 1, col].axis('off')
                
                # å åŠ å›¾åƒ
                axes[row_base + 2, col].imshow(viz['superimposed_original']) # Changed to superimposed_original
                axes[row_base + 2, col].set_title(f'å åŠ  #{i+1}\né¢„æµ‹:{viz["predicted_concentration"]:.1f}')
                axes[row_base + 2, col].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        total_subplots = rows * 3 * cols
        used_subplots = len(visualizations) * 3
        for i in range(used_subplots, total_subplots):
            row = i // cols
            col = i % cols
            if rows == 1:
                axes[row, col].axis('off')
            else:
                axes[row, col].axis('off')
        
        plt.suptitle(f"{model_name} - {target_conc} mg/L æµ“åº¦æ ·æœ¬", fontsize=16)
        plt.tight_layout()
        
        # ä¿å­˜
        filename = f"concentration_{target_conc}mg_L_grid.png"
        save_path = os.path.join(self.output_dir, filename)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"     ç½‘æ ¼å›¾å·²ä¿å­˜: {filename}")
        return save_path # Added return statement
    
    def _create_overview_visualization(self, all_visualizations, model_name):
        """åˆ›å»ºæ€»è§ˆå¯è§†åŒ–"""
        if not all_visualizations:
            return None # Changed to return None
        
        # æŒ‰æµ“åº¦åˆ†ç»„
        conc_groups = defaultdict(list)
        for viz in all_visualizations:
            conc_groups[viz['target_concentration']].append(viz)
        
        # æ¯ä¸ªæµ“åº¦é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§æ ·æœ¬
        representative_samples = []
        for target_conc in sorted(conc_groups.keys()):
            samples = conc_groups[target_conc]
            # é€‰æ‹©é¢„æµ‹æœ€å‡†ç¡®çš„æ ·æœ¬
            best_sample = min(samples, 
                            key=lambda x: abs(x['predicted_concentration'] - x['actual_concentration']))
            representative_samples.append(best_sample)
        
        # åˆ›å»ºæ€»è§ˆå›¾
        num_concs = len(representative_samples)
        cols = min(6, num_concs)
        rows = (num_concs + cols - 1) // cols
        
        fig, axes = plt.subplots(rows * 2, cols, figsize=(cols * 3, rows * 6))
        
        if rows == 1 and cols == 1:
            axes = axes.reshape(2, 1)
        elif rows == 1:
            axes = axes.reshape(2, cols)
        elif cols == 1:
            axes = axes.reshape(rows * 2, 1)
        
        for i, viz in enumerate(representative_samples):
            col = i % cols
            row_base = (i // cols) * 2
            
            # åŸå§‹å›¾åƒ
            if rows == 1:
                axes[0, col].imshow(viz['original_image'])
                axes[0, col].set_title(f'{viz["target_concentration"]} mg/L\nåŸå§‹å›¾åƒ')
                axes[0, col].axis('off')
                
                # å åŠ å›¾åƒ
                axes[1, col].imshow(viz['superimposed_original']) # Changed to superimposed_original
                axes[1, col].set_title(f'Grad-CAM\né¢„æµ‹:{viz["predicted_concentration"]:.1f}')
                axes[1, col].axis('off')
            else:
                axes[row_base, col].imshow(viz['original_image'])
                axes[row_base, col].set_title(f'{viz["target_concentration"]} mg/L\nåŸå§‹å›¾åƒ')
                axes[row_base, col].axis('off')
                
                # å åŠ å›¾åƒ
                axes[row_base + 1, col].imshow(viz['superimposed_original']) # Changed to superimposed_original
                axes[row_base + 1, col].set_title(f'Grad-CAM\né¢„æµ‹:{viz["predicted_concentration"]:.1f}')
                axes[row_base + 1, col].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        total_subplots = rows * 2 * cols
        used_subplots = len(representative_samples) * 2
        for i in range(used_subplots, total_subplots):
            row = i // cols
            col = i % cols
            if rows == 1:
                axes[row, col].axis('off')
            else:
                axes[row, col].axis('off')
        
        plt.suptitle(f"{model_name} - ä¸åŒæµ“åº¦çš„Grad-CAMå¯è§†åŒ–æ€»è§ˆ", fontsize=16)
        plt.tight_layout()
        
        # ä¿å­˜
        overview_path = os.path.join(self.output_dir, "concentration_overview.png")
        plt.savefig(overview_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   æ€»è§ˆå›¾å·²ä¿å­˜: concentration_overview.png")
        return overview_path # Added return statement
    
    def _parse_bg_mode(self, bg_mode):
        """è§£æèƒŒæ™¯æ¨¡å¼"""
        if '_' in bg_mode:
            parts = bg_mode.split('_')
            bg_filter = parts[0]
            power_filter = parts[1]
        else:
            bg_filter = bg_mode if bg_mode != 'all' else None
            power_filter = None
        
        return bg_filter, power_filter
    
    def _load_checkpoint_safely(self, model_path):
        """å®‰å…¨åŠ è½½checkpoint"""
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨weights_only=Trueï¼ˆå®‰å…¨æ¨¡å¼ï¼‰
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)
        except Exception as e:
            # å¦‚æœå®‰å…¨æ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨weights_only=Falseï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        return checkpoint
    
    def load_model(self, model_type, model_path):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ”§ åŠ è½½{model_type}æ¨¡å‹: {model_path}")
        
        checkpoint_to_use = None  # ç”¨äºå­˜å‚¨é¢„åŠ è½½çš„checkpoint
        
        if model_type == 'baseline_cnn':
            from cnn_model import CNNFeatureExtractor
            model = CNNFeatureExtractor().to(self.device)
            target_layer = 'last_conv'  # åŸºçº¿CNNæ¨¡å‹çš„æœ€åä¸€ä¸ªå·ç§¯å±‚
        elif model_type == 'enhanced_cnn':
            from enhanced_laser_spot_cnn import create_enhanced_laser_spot_model
            model = create_enhanced_laser_spot_model(backbone='resnet18').to(self.device)
            target_layer = 'backbone.layer4'
        elif model_type in ['cloud_resnet50', 'adaptive_resnet50']:
            # é¦–å…ˆå°è¯•åŠ è½½æƒé‡æ¥æ£€æµ‹æ¨¡å‹ç±»å‹
            try:
                # å…ˆåŠ è½½checkpointæ£€æŸ¥æƒé‡é”®
                checkpoint = self._load_checkpoint_safely(model_path)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ³¨æ„åŠ›æ¨¡å—æƒé‡
                has_attention = any('attention_modules' in key for key in checkpoint.get('model_state_dict', checkpoint).keys())
                
                if has_attention or model_type == 'adaptive_resnet50':
                    # ä½¿ç”¨è‡ªé€‚åº”ResNet50
                    try:
                        from compatible_adaptive_resnet50 import CompatibleAdaptiveResNet50
                        model = CompatibleAdaptiveResNet50().to(self.device)
                        target_layer = 'backbone.layer4'
                        print(f"   ä½¿ç”¨è‡ªé€‚åº”ResNet50æ¨¡å‹")
                    except ImportError:
                        print(f"   è­¦å‘Š: æ— æ³•å¯¼å…¥CompatibleAdaptiveResNet50ï¼Œå›é€€åˆ°æ ‡å‡†ResNet50")
                        # å›é€€åˆ°æ ‡å‡†ResNet50
                        import torchvision.models as models
                        model = models.resnet50(pretrained=False)
                        model.fc = torch.nn.Linear(model.fc.in_features, 1)
                        model = model.to(self.device)
                        target_layer = 'layer4'
                else:
                    # ä½¿ç”¨æ ‡å‡†ResNet50
                    import torchvision.models as models
                    model = models.resnet50(pretrained=False)
                    model.fc = torch.nn.Linear(model.fc.in_features, 1)
                    model = model.to(self.device)
                    target_layer = 'layer4'
                    print(f"   ä½¿ç”¨æ ‡å‡†ResNet50æ¨¡å‹")
                
                # ç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„checkpoint
                checkpoint_to_use = checkpoint
                
            except Exception as e:
                print(f"   æ¨¡å‹æ£€æµ‹å¤±è´¥: {e}")
                # é»˜è®¤ä½¿ç”¨æ ‡å‡†ResNet50
                import torchvision.models as models
                model = models.resnet50(pretrained=False)
                model.fc = torch.nn.Linear(model.fc.in_features, 1)
                model = model.to(self.device)
                target_layer = 'layer4'
                checkpoint_to_use = None  # ç¨åé‡æ–°åŠ è½½
 
        elif model_type == 'cloud_vgg':
            from vgg_regression import VGGRegressionCBAM
            model = VGGRegressionCBAM().to(self.device)
            target_layer = 'features'
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        # åŠ è½½æƒé‡
        if checkpoint_to_use is None:
            # å¦‚æœæ²¡æœ‰é¢„åŠ è½½checkpointï¼Œç°åœ¨åŠ è½½
            checkpoint_to_use = self._load_checkpoint_safely(model_path)
        
        try:
            if isinstance(checkpoint_to_use, dict) and 'model_state_dict' in checkpoint_to_use:
                model.load_state_dict(checkpoint_to_use['model_state_dict'], strict=False)
            else:
                model.load_state_dict(checkpoint_to_use, strict=False)
            print(f"   æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨strict=Falseæ¨¡å¼ï¼‰")
        except Exception as e:
            print(f"   æƒé‡åŠ è½½å¤±è´¥: {e}")
            raise
        
        model.eval()
        
        print(f"   æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç›®æ ‡å±‚: {target_layer}")
        return model, target_layer

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºGrad-CAMæµ“åº¦å¯è§†åŒ–å·¥å…·')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--model_type', type=str, required=True,
                       choices=['baseline_cnn', 'enhanced_cnn', 'cloud_resnet50', 'cloud_vgg', 'adaptive_resnet50'],
                       help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--model_path', type=str, required=True,
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset_path', type=str, default=None,
                       help='ç‰¹å¾æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•')
    
    # æµ“åº¦é€‰æ‹©å‚æ•°
    parser.add_argument('--min_concentration', type=int, default=0,
                       help='æœ€å°æµ“åº¦ (mg/L)')
    parser.add_argument('--max_concentration', type=int, default=1000,
                       help='æœ€å¤§æµ“åº¦ (mg/L)')
    parser.add_argument('--concentration_interval', type=int, default=100,
                       help='æµ“åº¦é—´éš” (mg/L)')
    parser.add_argument('--samples_per_concentration', type=int, default=10,
                       help='æ¯ä¸ªæµ“åº¦é€‰æ‹©çš„æ ·æœ¬æ•°')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--bg_mode', type=str, default='all',
                       help='èƒŒæ™¯æ¨¡å¼ (bg0, bg1, all, bg0_20mw, etc.)')
    parser.add_argument('--target_layer', type=str, default=None,
                       help='ç›®æ ‡å±‚åç§° (è‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--save_individual', action='store_true',
                       help='ä¿å­˜å•ä¸ªæ ·æœ¬çš„å¯è§†åŒ–')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    print("ğŸ” å¢å¼ºGrad-CAMæµ“åº¦å¯è§†åŒ–å·¥å…·")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = GradCAMConcentrationVisualizer(args.output_dir)
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†
    if args.dataset_path is None:
        sys.path.append('src')
        from feature_dataset_loader import detect_feature_datasets
        datasets = detect_feature_datasets()
        if datasets:
            args.dataset_path = datasets[-1]['path']
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æ•°æ®é›†: {args.dataset_path}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°ç‰¹å¾æ•°æ®é›†ï¼Œè¯·ä½¿ç”¨--dataset_pathæŒ‡å®š")
    
    # åŠ è½½æ¨¡å‹
    model, target_layer = visualizer.load_model(args.model_type, args.model_path)
    
    if args.target_layer:
        target_layer = args.target_layer
    
    # æŒ‰æµ“åº¦åŠ è½½æ•°æ®é›†
    concentration_groups, conc_stats = visualizer.load_dataset_by_concentration(
        args.dataset_path, args.bg_mode
    )
    
    # é€‰æ‹©æ ·æœ¬
    selected_samples = visualizer.select_samples_by_concentration(
        concentration_groups,
        args.min_concentration,
        args.max_concentration, 
        args.concentration_interval,
        args.samples_per_concentration
    )
    
    if not selected_samples:
        print("âŒ æœªé€‰æ‹©åˆ°ä»»ä½•æ ·æœ¬")
        return
    
    # ç”Ÿæˆå¯è§†åŒ–
    model_name = f"{args.model_type}_{Path(args.model_path).stem}"
    visualizations = visualizer.visualize_concentration_samples(
        model, model_name, target_layer, selected_samples, args.save_individual
    )
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'model_type': args.model_type,
        'model_path': args.model_path,
        'dataset_path': args.dataset_path,
        'concentration_range': f"{args.min_concentration}-{args.max_concentration}",
        'interval': args.concentration_interval,
        'samples_per_concentration': args.samples_per_concentration,
        'total_selected_samples': len(selected_samples),
        'total_visualizations': len(visualizations),
        'concentration_stats': conc_stats,
        'bg_mode': args.bg_mode
    }
    
    stats_path = os.path.join(visualizer.output_dir, "visualization_stats.json")
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ‰ Grad-CAMæµ“åº¦å¯è§†åŒ–å®Œæˆ!")
    print(f"   ç”Ÿæˆäº†{len(visualizations)}ä¸ªå¯è§†åŒ–")
    print(f"   ç»“æœä¿å­˜åœ¨: {visualizer.output_dir}")
    print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats_path}")

if __name__ == "__main__":
    main() 