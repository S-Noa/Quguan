#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…· - ç»Ÿè®¡å¤šä¸ªæœ€ä½³æ¨¡å‹çš„è®­ç»ƒæŸå¤±ä¸éªŒè¯æŸå¤±
æ”¯æŒäº‘ç«¯å’Œæœ¬åœ°ç¯å¢ƒï¼Œå¢å¼ºäº†training.logè§£æå’Œæ£€æŸ¥ç‚¹æŸå¤±æå–åŠŸèƒ½
"""

import os
import json
import pickle
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import argparse
from pathlib import Path
import glob
import torch
import warnings
warnings.filterwarnings("ignore")

class EnhancedBatchLossAnalyzer:
    def __init__(self, base_dir=None):
        self.base_dir = base_dir or os.getcwd()
        self.models_data = []
        self.setup_chinese_font()
    
    def setup_chinese_font(self):
        """è®¾ç½®ä¸­æ–‡å­—ä½“"""
        try:
            import matplotlib.font_manager as fm
            chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun']
            available_fonts = [f.name for f in fm.fontManager.ttflist]
            
            for font in chinese_fonts:
                if font in available_fonts:
                    plt.rcParams['font.sans-serif'] = [font]
                    plt.rcParams['axes.unicode_minus'] = False
                    print(f"âœ“ ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
                    return
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
        except Exception as e:
            print(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
    
    def detect_model_directories(self):
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç›®å½•"""
        print("ğŸ” æ£€æµ‹æ¨¡å‹ç›®å½•...")
        
        # å¸¸è§çš„æ¨¡å‹ç›®å½•æ¨¡å¼
        patterns = [
            "*_results_*",
            "feature_training_*_results_*",
            "*_training_results_*",
            "model_*",
            "checkpoint_*"
        ]
        
        model_dirs = []
        for pattern in patterns:
            dirs = glob.glob(os.path.join(self.base_dir, pattern))
            model_dirs.extend([d for d in dirs if os.path.isdir(d)])
        
        # å»é‡å¹¶æ’åº
        model_dirs = sorted(list(set(model_dirs)))
        
        print(f"ğŸ“ å‘ç° {len(model_dirs)} ä¸ªæ¨¡å‹ç›®å½•:")
        for i, dir_path in enumerate(model_dirs, 1):
            dir_name = os.path.basename(dir_path)
            print(f"   {i:2d}. {dir_name}")
        
        return model_dirs
    
    def load_loss_data(self, model_dir):
        """ä»æ¨¡å‹ç›®å½•åŠ è½½æŸå¤±æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        model_name = os.path.basename(model_dir)
        print(f"\nğŸ“Š åˆ†ææ¨¡å‹: {model_name}")
        
        loss_data = {
            'name': model_name,
            'path': model_dir,
            'train_losses': [],
            'val_losses': [],
            'epochs': [],
            'best_epoch': None,
            'best_val_loss': None,
            'final_train_loss': None,
            'final_val_loss': None,
            'source_file': None
        }
        
        # ä¼˜å…ˆçº§é¡ºåºçš„æŸå¤±æ•°æ®æ–‡ä»¶
        loss_files = [
            'training_history.json',  # æœ€é«˜ä¼˜å…ˆçº§
            'training_log.json',
            'loss_history.json',
            'training_results.json',
            'losses.json',
            'log.json',
            'results.json',
            'history.json',
            'training_log.pkl',
            'loss_history.pkl',
            'training_history.pkl'
        ]
        
        # é¦–å…ˆå°è¯•JSONå’ŒPKLæ–‡ä»¶
        for loss_file in loss_files:
            file_path = os.path.join(model_dir, loss_file)
            if os.path.exists(file_path):
                try:
                    data = self._load_file(file_path)
                    if self._extract_losses(data, loss_data):
                        loss_data['source_file'] = loss_file
                        print(f"   âœ“ ä» {loss_file} åŠ è½½æŸå¤±æ•°æ®")
                        return self._finalize_loss_data(loss_data)
                except Exception as e:
                    print(f"   âš ï¸ åŠ è½½ {loss_file} å¤±è´¥: {e}")
                    continue
        
        # å°è¯•è§£ætraining.logä½œä¸ºçº¯æ–‡æœ¬
        training_log_path = os.path.join(model_dir, 'training.log')
        if os.path.exists(training_log_path):
            try:
                print(f"   ğŸ“ å°è¯•è§£ætraining.logä½œä¸ºçº¯æ–‡æœ¬...")
                with open(training_log_path, 'r', encoding='utf-8', errors='ignore') as f:
                    log_content = f.read()
                
                if self._extract_losses_from_log(log_content, loss_data):
                    loss_data['source_file'] = 'training.log (parsed)'
                    print(f"   âœ“ ä» training.log è§£ææŸå¤±æ•°æ®")
                    return self._finalize_loss_data(loss_data)
            except Exception as e:
                print(f"   âš ï¸ è§£æ training.log å¤±è´¥: {e}")
        
        # æœ€åå°è¯•ä»checkpointåŠ è½½
        print(f"   ğŸ“ å°è¯•ä»æ¨¡å‹æ–‡ä»¶åŠ è½½...")
        checkpoint_files = ['best_model.pth', 'checkpoint_latest.pth', 'checkpoint.pth', 'model.pth']
        for checkpoint_file in checkpoint_files:
            file_path = os.path.join(model_dir, checkpoint_file)
            if os.path.exists(file_path):
                try:
                    # é¦–å…ˆå°è¯•å®‰å…¨åŠ è½½
                    try:
                        checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
                    except Exception:
                        print(f"   ğŸ“ å®‰å…¨åŠ è½½å¤±è´¥ï¼Œå°è¯•å®Œæ•´åŠ è½½ {checkpoint_file}...")
                        checkpoint = torch.load(file_path, map_location='cpu', weights_only=False)
                    
                    if self._extract_losses_from_checkpoint(checkpoint, loss_data):
                        loss_data['source_file'] = checkpoint_file
                        print(f"   âœ“ ä» {checkpoint_file} åŠ è½½æŸå¤±æ•°æ®")
                        return self._finalize_loss_data(loss_data)
                except Exception as e:
                    print(f"   âš ï¸ åŠ è½½ {checkpoint_file} å¤±è´¥: {e}")
                    continue
        
        print(f"   âŒ æœªæ‰¾åˆ°æŸå¤±æ•°æ®")
        return None
    
    def _load_file(self, file_path):
        """åŠ è½½æ–‡ä»¶ï¼ˆJSONæˆ–PKLï¼‰"""
        if file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        elif file_path.endswith('.pkl'):
            with open(file_path, 'rb') as f:
                return pickle.load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
    
    def _extract_losses(self, data, loss_data):
        """ä»æ•°æ®ä¸­æå–æŸå¤±å€¼"""
        if isinstance(data, dict):
            # æ ‡å‡†æ ¼å¼: {'train_losses': [...], 'val_losses': [...]}
            if 'train_losses' in data and 'val_losses' in data:
                loss_data['train_losses'] = data['train_losses']
                loss_data['val_losses'] = data['val_losses']
                if 'epochs' in data:
                    loss_data['epochs'] = data['epochs']
                else:
                    loss_data['epochs'] = list(range(1, len(data['train_losses']) + 1))
                
                # æå–å…¶ä»–ä¿¡æ¯
                if 'best_epoch' in data:
                    loss_data['best_epoch'] = data['best_epoch']
                if 'best_val_loss' in data:
                    loss_data['best_val_loss'] = data['best_val_loss']
                
                return True
        
        return False
    
    def _extract_losses_from_checkpoint(self, checkpoint, loss_data):
        """ä»checkpointä¸­æå–æŸå¤±æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print(f"   ğŸ” æ£€æŸ¥checkpointå†…å®¹ï¼Œå¯ç”¨é”®: {list(checkpoint.keys())}")
        
        # å°è¯•å®Œæ•´çš„è®­ç»ƒå†å²
        history_keys = [
            ('train_losses', 'val_losses'),
            ('train_loss_history', 'val_loss_history'),
            ('training_losses', 'validation_losses'),
            ('loss_history', 'val_loss_history')
        ]
        
        for train_key, val_key in history_keys:
            if train_key in checkpoint and val_key in checkpoint:
                train_losses = checkpoint[train_key]
                val_losses = checkpoint[val_key]
                
                if isinstance(train_losses, (list, tuple)) and isinstance(val_losses, (list, tuple)):
                    if len(train_losses) > 0 and len(val_losses) > 0:
                        loss_data['train_losses'] = list(train_losses)
                        loss_data['val_losses'] = list(val_losses)
                        loss_data['epochs'] = list(range(1, len(train_losses) + 1))
                        print(f"   âœ“ æ‰¾åˆ°è®­ç»ƒå†å²: {train_key}, {val_key}")
                        return True
        
        # å°è¯•ä»åµŒå¥—çš„historyå­—å…¸ä¸­æå–
        if 'history' in checkpoint and isinstance(checkpoint['history'], dict):
            history = checkpoint['history']
            for train_key in ['loss', 'train_loss', 'training_loss', 'train_losses']:
                for val_key in ['val_loss', 'validation_loss', 'val_losses', 'valid_loss']:
                    if train_key in history and val_key in history:
                        train_losses = history[train_key]
                        val_losses = history[val_key]
                        
                        if isinstance(train_losses, (list, tuple)) and isinstance(val_losses, (list, tuple)):
                            if len(train_losses) > 0 and len(val_losses) > 0:
                                loss_data['train_losses'] = list(train_losses)
                                loss_data['val_losses'] = list(val_losses)
                                loss_data['epochs'] = list(range(1, len(train_losses) + 1))
                                print(f"   âœ“ ä»historyä¸­æ‰¾åˆ°æŸå¤±: {train_key}, {val_key}")
                                return True
        
        # ã€æ–°å¢ã€‘å°è¯•ä»å•ç‹¬çš„éªŒè¯æŸå¤±ä¸­æå–ä¿¡æ¯
        val_loss_keys = ['best_val_loss', 'val_loss', 'validation_loss', 'best_validation_loss']
        epoch_keys = ['best_epoch', 'epoch', 'best_epoch_num']
        
        best_val_loss = None
        best_epoch = None
        
        for val_key in val_loss_keys:
            if val_key in checkpoint:
                best_val_loss = float(checkpoint[val_key])
                break
        
        for epoch_key in epoch_keys:
            if epoch_key in checkpoint:
                best_epoch = int(checkpoint[epoch_key])
                break
        
        if best_val_loss is not None:
            loss_data['best_val_loss'] = best_val_loss
            if best_epoch is not None:
                loss_data['best_epoch'] = best_epoch
                print(f"   âœ“ æ‰¾åˆ°æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f} (ç¬¬{best_epoch}è½®)")
            else:
                print(f"   âœ“ æ‰¾åˆ°éªŒè¯æŸå¤±: {best_val_loss:.4f}")
            return True
        
        print(f"   âš ï¸ æœªåœ¨checkpointä¸­æ‰¾åˆ°æŸå¤±æ•°æ®")
        return False
    
    def _extract_losses_from_log(self, log_content, loss_data):
        """ä»training.logæ–‡ä»¶ä¸­æå–æŸå¤±æ•°æ®ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        print(f"   ğŸ” å°è¯•ä»training.logè§£ææŸå¤±æ•°æ®...")
        
        train_losses = []
        val_losses = []
        epochs = []
        
        lines = log_content.split('\n')
        current_epoch = None
        epoch_train_loss = None
        epoch_val_loss = None
        
        # åŸºçº¿CNNæ ¼å¼ï¼šè½®æ¬¡ X æ€»ç»“
        epoch_summary_pattern = r'è½®æ¬¡\s+(\d+)\s+æ€»ç»“:'
        train_loss_pattern = r'è®­ç»ƒæŸå¤±:\s*([\d.]+)'
        val_loss_pattern = r'éªŒè¯æŸå¤±:\s*([\d.]+)'
        
        # VGGæ ¼å¼ï¼šEpoch X éªŒè¯æŸå¤±
        vgg_epoch_pattern = r'Epoch\s+(\d+)\s+éªŒè¯æŸå¤±:\s*([\d.]+)'
        
        # å¢å¼ºCNNæ ¼å¼
        enhanced_epoch_pattern = r'--- è½®æ¬¡\s+(\d+)/\d+ ---'
        enhanced_summary_pattern = r'ç¬¬(\d+)è½®æ€»ç»“:'
        
        for line in lines:
            line = line.strip()
            
            # åŸºçº¿CNNæ ¼å¼è§£æ
            epoch_match = re.search(epoch_summary_pattern, line)
            if epoch_match:
                current_epoch = int(epoch_match.group(1))
                continue
            
            if current_epoch is not None:
                train_loss_match = re.search(train_loss_pattern, line)
                if train_loss_match:
                    epoch_train_loss = float(train_loss_match.group(1))
                
                val_loss_match = re.search(val_loss_pattern, line)
                if val_loss_match:
                    epoch_val_loss = float(val_loss_match.group(1))
                
                # å¦‚æœæ‰¾åˆ°äº†è¯¥è½®æ¬¡çš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±ï¼Œè®°å½•å®ƒä»¬
                if epoch_train_loss is not None and epoch_val_loss is not None:
                    epochs.append(current_epoch)
                    train_losses.append(epoch_train_loss)
                    val_losses.append(epoch_val_loss)
                    
                    # é‡ç½®ä¸ºä¸‹ä¸€è½®æ¬¡
                    current_epoch = None
                    epoch_train_loss = None
                    epoch_val_loss = None
            
            # VGGæ ¼å¼è§£æ
            vgg_match = re.search(vgg_epoch_pattern, line)
            if vgg_match:
                epoch = int(vgg_match.group(1))
                val_loss = float(vgg_match.group(2))
                
                epochs.append(epoch)
                val_losses.append(val_loss)
                # VGGæ—¥å¿—é€šå¸¸ä¸è®°å½•è®­ç»ƒæŸå¤±ï¼Œä½¿ç”¨Noneå ä½
                train_losses.append(None)
        
        # å¦‚æœæˆåŠŸè§£æåˆ°æ•°æ®
        if epochs and val_losses:
            loss_data['epochs'] = epochs
            loss_data['val_losses'] = val_losses
            
            # è¿‡æ»¤æ‰Noneå€¼çš„è®­ç»ƒæŸå¤±
            valid_train_losses = [loss for loss in train_losses if loss is not None]
            if valid_train_losses and len(valid_train_losses) == len(val_losses):
                loss_data['train_losses'] = valid_train_losses
                print(f"   âœ“ ä»training.logè§£æåˆ°å®Œæ•´æ•°æ®: {len(epochs)}è½®ï¼Œè®­ç»ƒ+éªŒè¯æŸå¤±")
            else:
                print(f"   âœ“ ä»training.logè§£æåˆ°éªŒè¯æŸå¤±: {len(epochs)}è½®")
            
            return True
        
        print(f"   âŒ æ— æ³•ä»training.logè§£ææŸå¤±æ•°æ®")
        return False
    
    def _finalize_loss_data(self, loss_data):
        """å®ŒæˆæŸå¤±æ•°æ®çš„å¤„ç†"""
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if loss_data['train_losses'] and loss_data['val_losses']:
            # æ‰¾åˆ°æœ€ä½³éªŒè¯æŸå¤±
            val_losses = loss_data['val_losses']
            best_idx = np.argmin(val_losses)
            
            if loss_data['best_epoch'] is None:
                loss_data['best_epoch'] = loss_data['epochs'][best_idx] if loss_data['epochs'] else best_idx + 1
            if loss_data['best_val_loss'] is None:
                loss_data['best_val_loss'] = val_losses[best_idx]
            
            # æœ€ç»ˆæŸå¤±
            if loss_data['final_train_loss'] is None and loss_data['train_losses']:
                loss_data['final_train_loss'] = loss_data['train_losses'][-1]
            if loss_data['final_val_loss'] is None:
                loss_data['final_val_loss'] = loss_data['val_losses'][-1]
            
            print(f"   ğŸ“ˆ è®­ç»ƒè½®æ•°: {len(loss_data['val_losses'])}")
            print(f"   ğŸ¯ æœ€ä½³éªŒè¯æŸå¤±: {loss_data['best_val_loss']:.4f} (ç¬¬{loss_data['best_epoch']}è½®)")
            if loss_data['final_train_loss'] is not None:
                print(f"   ğŸ“Š æœ€ç»ˆè®­ç»ƒæŸå¤±: {loss_data['final_train_loss']:.4f}")
            print(f"   ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {loss_data['final_val_loss']:.4f}")
        elif loss_data['best_val_loss'] is not None:
            print(f"   ğŸ“Š éªŒè¯æŸå¤±: {loss_data['best_val_loss']:.4f}")
            if loss_data['best_epoch'] is not None:
                print(f"   ğŸ“Š æœ€ä½³è½®æ¬¡: {loss_data['best_epoch']}")
        
        return loss_data
    
    def analyze_models(self, model_dirs=None):
        """åˆ†æå¤šä¸ªæ¨¡å‹çš„æŸå¤±"""
        if model_dirs is None:
            model_dirs = self.detect_model_directories()
        
        if not model_dirs:
            print("âŒ æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•")
            return []
        
        print(f"\nğŸ” å¼€å§‹åˆ†æ {len(model_dirs)} ä¸ªæ¨¡å‹...")
        
        for model_dir in model_dirs:
            loss_data = self.load_loss_data(model_dir)
            if loss_data:
                self.models_data.append(loss_data)
        
        print(f"\nâœ… æˆåŠŸåˆ†æ {len(self.models_data)} ä¸ªæ¨¡å‹")
        return self.models_data
    
    def create_summary_table(self, output_dir="loss_analysis"):
        """åˆ›å»ºæ±‡æ€»è¡¨æ ¼"""
        if not self.models_data:
            print("âŒ æ²¡æœ‰æ¨¡å‹æ•°æ®å¯ç”Ÿæˆè¡¨æ ¼")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        
        # å‡†å¤‡æ•°æ®
        table_data = []
        for model in self.models_data:
            row = {
                'æ¨¡å‹åç§°': model['name'],
                'æ•°æ®æº': model['source_file'],
                'æœ€ä½³éªŒè¯æŸå¤±': model['best_val_loss'] if model['best_val_loss'] is not None else 'N/A',
                'æœ€ä½³è½®æ¬¡': model['best_epoch'] if model['best_epoch'] is not None else 'N/A',
                'æœ€ç»ˆéªŒè¯æŸå¤±': model['final_val_loss'] if model['final_val_loss'] is not None else 'N/A',
                'è®­ç»ƒè½®æ•°': len(model['val_losses']) if model['val_losses'] else 'N/A'
            }
            table_data.append(row)
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(table_data)
        
        # æŒ‰æœ€ä½³éªŒè¯æŸå¤±æ’åº
        def sort_key(x):
            if x == 'N/A':
                return float('inf')
            return float(x)
        
        df_sorted = df.copy()
        df_sorted['sort_key'] = df_sorted['æœ€ä½³éªŒè¯æŸå¤±'].apply(sort_key)
        df_sorted = df_sorted.sort_values('sort_key').drop('sort_key', axis=1)
        
        # ä¿å­˜CSV
        csv_path = os.path.join(output_dir, 'model_summary.csv')
        df_sorted.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜HTMLè¡¨æ ¼
        html_path = os.path.join(output_dir, 'model_summary.html')
        df_sorted.to_html(html_path, index=False, escape=False, 
                         table_id='model_summary', classes='table table-striped')
        
        print(f"âœ… æ±‡æ€»è¡¨æ ¼å·²ä¿å­˜:")
        print(f"   CSV: {csv_path}")
        print(f"   HTML: {html_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆæ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…·')
    parser.add_argument('--base_dir', type=str, default=None, help='åŸºç¡€ç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='enhanced_loss_analysis', help='è¾“å‡ºç›®å½•')
    args = parser.parse_args()
    
    print("ğŸš€ å¢å¼ºç‰ˆæ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…·")
    print("=" * 60)
    print("æ–°å¢åŠŸèƒ½:")
    print("- æ”¯æŒä»training.logè§£æVGGç­‰è®­ç»ƒæ—¥å¿—")
    print("- ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æå–å•ç‹¬çš„éªŒè¯æŸå¤±ä¿¡æ¯")
    print("- ä¼˜åŒ–çš„æ¨¡å‹ç›®å½•æ£€æµ‹å’Œå»é‡")
    print("- å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œå…¼å®¹æ€§")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = EnhancedBatchLossAnalyzer(args.base_dir)
    
    # åˆ†ææ¨¡å‹
    models_data = analyzer.analyze_models()
    
    if models_data:
        print(f"\nğŸ“ˆ ç”Ÿæˆåˆ†æç»“æœ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
        analyzer.create_summary_table(args.output_dir)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")
        print(f"ğŸ“Š æˆåŠŸåˆ†æ {len(models_data)} ä¸ªæ¨¡å‹")
        
        # æ˜¾ç¤ºç»“æœç»Ÿè®¡
        models_with_full_history = sum(1 for m in models_data if m['train_losses'] and m['val_losses'])
        models_with_val_only = sum(1 for m in models_data if m['best_val_loss'] is not None and not m['train_losses'])
        
        print(f"   - å®Œæ•´è®­ç»ƒå†å²: {models_with_full_history} ä¸ª")
        print(f"   - ä»…éªŒè¯æŸå¤±: {models_with_val_only} ä¸ª")
    else:
        print(f"âŒ æœªæ‰¾åˆ°å¯åˆ†æçš„æ¨¡å‹æ•°æ®")

if __name__ == "__main__":
    main() 
"""
å¢å¼ºç‰ˆæ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…· - ç»Ÿè®¡å¤šä¸ªæœ€ä½³æ¨¡å‹çš„è®­ç»ƒæŸå¤±ä¸éªŒè¯æŸå¤±
æ”¯æŒäº‘ç«¯å’Œæœ¬åœ°ç¯å¢ƒï¼Œå¢å¼ºäº†training.logè§£æå’Œæ£€æŸ¥ç‚¹æŸå¤±æå–åŠŸèƒ½
"""

import os
import json
import pickle
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import argparse
from pathlib import Path
import glob
import torch
import warnings
warnings.filterwarnings("ignore")

class EnhancedBatchLossAnalyzer:
    def __init__(self, base_dir=None):
        self.base_dir = base_dir or os.getcwd()
        self.models_data = []
        self.setup_chinese_font()
    
    def setup_chinese_font(self):
        """è®¾ç½®ä¸­æ–‡å­—ä½“"""
        try:
            import matplotlib.font_manager as fm
            chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun']
            available_fonts = [f.name for f in fm.fontManager.ttflist]
            
            for font in chinese_fonts:
                if font in available_fonts:
                    plt.rcParams['font.sans-serif'] = [font]
                    plt.rcParams['axes.unicode_minus'] = False
                    print(f"âœ“ ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
                    return
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
        except Exception as e:
            print(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
    
    def detect_model_directories(self):
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç›®å½•"""
        print("ğŸ” æ£€æµ‹æ¨¡å‹ç›®å½•...")
        
        # å¸¸è§çš„æ¨¡å‹ç›®å½•æ¨¡å¼
        patterns = [
            "*_results_*",
            "feature_training_*_results_*",
            "*_training_results_*",
            "model_*",
            "checkpoint_*"
        ]
        
        model_dirs = []
        for pattern in patterns:
            dirs = glob.glob(os.path.join(self.base_dir, pattern))
            model_dirs.extend([d for d in dirs if os.path.isdir(d)])
        
        # å»é‡å¹¶æ’åº
        model_dirs = sorted(list(set(model_dirs)))
        
        print(f"ğŸ“ å‘ç° {len(model_dirs)} ä¸ªæ¨¡å‹ç›®å½•:")
        for i, dir_path in enumerate(model_dirs, 1):
            dir_name = os.path.basename(dir_path)
            print(f"   {i:2d}. {dir_name}")
        
        return model_dirs
    
    def load_loss_data(self, model_dir):
        """ä»æ¨¡å‹ç›®å½•åŠ è½½æŸå¤±æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        model_name = os.path.basename(model_dir)
        print(f"\nğŸ“Š åˆ†ææ¨¡å‹: {model_name}")
        
        loss_data = {
            'name': model_name,
            'path': model_dir,
            'train_losses': [],
            'val_losses': [],
            'epochs': [],
            'best_epoch': None,
            'best_val_loss': None,
            'final_train_loss': None,
            'final_val_loss': None,
            'source_file': None
        }
        
        # ä¼˜å…ˆçº§é¡ºåºçš„æŸå¤±æ•°æ®æ–‡ä»¶
        loss_files = [
            'training_history.json',  # æœ€é«˜ä¼˜å…ˆçº§
            'training_log.json',
            'loss_history.json',
            'training_results.json',
            'losses.json',
            'log.json',
            'results.json',
            'history.json',
            'training_log.pkl',
            'loss_history.pkl',
            'training_history.pkl'
        ]
        
        # é¦–å…ˆå°è¯•JSONå’ŒPKLæ–‡ä»¶
        for loss_file in loss_files:
            file_path = os.path.join(model_dir, loss_file)
            if os.path.exists(file_path):
                try:
                    data = self._load_file(file_path)
                    if self._extract_losses(data, loss_data):
                        loss_data['source_file'] = loss_file
                        print(f"   âœ“ ä» {loss_file} åŠ è½½æŸå¤±æ•°æ®")
                        return self._finalize_loss_data(loss_data)
                except Exception as e:
                    print(f"   âš ï¸ åŠ è½½ {loss_file} å¤±è´¥: {e}")
                    continue
        
        # å°è¯•è§£ætraining.logä½œä¸ºçº¯æ–‡æœ¬
        training_log_path = os.path.join(model_dir, 'training.log')
        if os.path.exists(training_log_path):
            try:
                print(f"   ğŸ“ å°è¯•è§£ætraining.logä½œä¸ºçº¯æ–‡æœ¬...")
                with open(training_log_path, 'r', encoding='utf-8', errors='ignore') as f:
                    log_content = f.read()
                
                if self._extract_losses_from_log(log_content, loss_data):
                    loss_data['source_file'] = 'training.log (parsed)'
                    print(f"   âœ“ ä» training.log è§£ææŸå¤±æ•°æ®")
                    return self._finalize_loss_data(loss_data)
            except Exception as e:
                print(f"   âš ï¸ è§£æ training.log å¤±è´¥: {e}")
        
        # æœ€åå°è¯•ä»checkpointåŠ è½½
        print(f"   ğŸ“ å°è¯•ä»æ¨¡å‹æ–‡ä»¶åŠ è½½...")
        checkpoint_files = ['best_model.pth', 'checkpoint_latest.pth', 'checkpoint.pth', 'model.pth']
        for checkpoint_file in checkpoint_files:
            file_path = os.path.join(model_dir, checkpoint_file)
            if os.path.exists(file_path):
                try:
                    # é¦–å…ˆå°è¯•å®‰å…¨åŠ è½½
                    try:
                        checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
                    except Exception:
                        print(f"   ğŸ“ å®‰å…¨åŠ è½½å¤±è´¥ï¼Œå°è¯•å®Œæ•´åŠ è½½ {checkpoint_file}...")
                        checkpoint = torch.load(file_path, map_location='cpu', weights_only=False)
                    
                    if self._extract_losses_from_checkpoint(checkpoint, loss_data):
                        loss_data['source_file'] = checkpoint_file
                        print(f"   âœ“ ä» {checkpoint_file} åŠ è½½æŸå¤±æ•°æ®")
                        return self._finalize_loss_data(loss_data)
                except Exception as e:
                    print(f"   âš ï¸ åŠ è½½ {checkpoint_file} å¤±è´¥: {e}")
                    continue
        
        print(f"   âŒ æœªæ‰¾åˆ°æŸå¤±æ•°æ®")
        return None
    
    def _load_file(self, file_path):
        """åŠ è½½æ–‡ä»¶ï¼ˆJSONæˆ–PKLï¼‰"""
        if file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        elif file_path.endswith('.pkl'):
            with open(file_path, 'rb') as f:
                return pickle.load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
    
    def _extract_losses(self, data, loss_data):
        """ä»æ•°æ®ä¸­æå–æŸå¤±å€¼"""
        if isinstance(data, dict):
            # æ ‡å‡†æ ¼å¼: {'train_losses': [...], 'val_losses': [...]}
            if 'train_losses' in data and 'val_losses' in data:
                loss_data['train_losses'] = data['train_losses']
                loss_data['val_losses'] = data['val_losses']
                if 'epochs' in data:
                    loss_data['epochs'] = data['epochs']
                else:
                    loss_data['epochs'] = list(range(1, len(data['train_losses']) + 1))
                
                # æå–å…¶ä»–ä¿¡æ¯
                if 'best_epoch' in data:
                    loss_data['best_epoch'] = data['best_epoch']
                if 'best_val_loss' in data:
                    loss_data['best_val_loss'] = data['best_val_loss']
                
                return True
        
        return False
    
    def _extract_losses_from_checkpoint(self, checkpoint, loss_data):
        """ä»checkpointä¸­æå–æŸå¤±æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print(f"   ğŸ” æ£€æŸ¥checkpointå†…å®¹ï¼Œå¯ç”¨é”®: {list(checkpoint.keys())}")
        
        # å°è¯•å®Œæ•´çš„è®­ç»ƒå†å²
        history_keys = [
            ('train_losses', 'val_losses'),
            ('train_loss_history', 'val_loss_history'),
            ('training_losses', 'validation_losses'),
            ('loss_history', 'val_loss_history')
        ]
        
        for train_key, val_key in history_keys:
            if train_key in checkpoint and val_key in checkpoint:
                train_losses = checkpoint[train_key]
                val_losses = checkpoint[val_key]
                
                if isinstance(train_losses, (list, tuple)) and isinstance(val_losses, (list, tuple)):
                    if len(train_losses) > 0 and len(val_losses) > 0:
                        loss_data['train_losses'] = list(train_losses)
                        loss_data['val_losses'] = list(val_losses)
                        loss_data['epochs'] = list(range(1, len(train_losses) + 1))
                        print(f"   âœ“ æ‰¾åˆ°è®­ç»ƒå†å²: {train_key}, {val_key}")
                        return True
        
        # å°è¯•ä»åµŒå¥—çš„historyå­—å…¸ä¸­æå–
        if 'history' in checkpoint and isinstance(checkpoint['history'], dict):
            history = checkpoint['history']
            for train_key in ['loss', 'train_loss', 'training_loss', 'train_losses']:
                for val_key in ['val_loss', 'validation_loss', 'val_losses', 'valid_loss']:
                    if train_key in history and val_key in history:
                        train_losses = history[train_key]
                        val_losses = history[val_key]
                        
                        if isinstance(train_losses, (list, tuple)) and isinstance(val_losses, (list, tuple)):
                            if len(train_losses) > 0 and len(val_losses) > 0:
                                loss_data['train_losses'] = list(train_losses)
                                loss_data['val_losses'] = list(val_losses)
                                loss_data['epochs'] = list(range(1, len(train_losses) + 1))
                                print(f"   âœ“ ä»historyä¸­æ‰¾åˆ°æŸå¤±: {train_key}, {val_key}")
                                return True
        
        # ã€æ–°å¢ã€‘å°è¯•ä»å•ç‹¬çš„éªŒè¯æŸå¤±ä¸­æå–ä¿¡æ¯
        val_loss_keys = ['best_val_loss', 'val_loss', 'validation_loss', 'best_validation_loss']
        epoch_keys = ['best_epoch', 'epoch', 'best_epoch_num']
        
        best_val_loss = None
        best_epoch = None
        
        for val_key in val_loss_keys:
            if val_key in checkpoint:
                best_val_loss = float(checkpoint[val_key])
                break
        
        for epoch_key in epoch_keys:
            if epoch_key in checkpoint:
                best_epoch = int(checkpoint[epoch_key])
                break
        
        if best_val_loss is not None:
            loss_data['best_val_loss'] = best_val_loss
            if best_epoch is not None:
                loss_data['best_epoch'] = best_epoch
                print(f"   âœ“ æ‰¾åˆ°æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f} (ç¬¬{best_epoch}è½®)")
            else:
                print(f"   âœ“ æ‰¾åˆ°éªŒè¯æŸå¤±: {best_val_loss:.4f}")
            return True
        
        print(f"   âš ï¸ æœªåœ¨checkpointä¸­æ‰¾åˆ°æŸå¤±æ•°æ®")
        return False
    
    def _extract_losses_from_log(self, log_content, loss_data):
        """ä»training.logæ–‡ä»¶ä¸­æå–æŸå¤±æ•°æ®ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        print(f"   ğŸ” å°è¯•ä»training.logè§£ææŸå¤±æ•°æ®...")
        
        train_losses = []
        val_losses = []
        epochs = []
        
        lines = log_content.split('\n')
        current_epoch = None
        epoch_train_loss = None
        epoch_val_loss = None
        
        # åŸºçº¿CNNæ ¼å¼ï¼šè½®æ¬¡ X æ€»ç»“
        epoch_summary_pattern = r'è½®æ¬¡\s+(\d+)\s+æ€»ç»“:'
        train_loss_pattern = r'è®­ç»ƒæŸå¤±:\s*([\d.]+)'
        val_loss_pattern = r'éªŒè¯æŸå¤±:\s*([\d.]+)'
        
        # VGGæ ¼å¼ï¼šEpoch X éªŒè¯æŸå¤±
        vgg_epoch_pattern = r'Epoch\s+(\d+)\s+éªŒè¯æŸå¤±:\s*([\d.]+)'
        
        # å¢å¼ºCNNæ ¼å¼
        enhanced_epoch_pattern = r'--- è½®æ¬¡\s+(\d+)/\d+ ---'
        enhanced_summary_pattern = r'ç¬¬(\d+)è½®æ€»ç»“:'
        
        for line in lines:
            line = line.strip()
            
            # åŸºçº¿CNNæ ¼å¼è§£æ
            epoch_match = re.search(epoch_summary_pattern, line)
            if epoch_match:
                current_epoch = int(epoch_match.group(1))
                continue
            
            if current_epoch is not None:
                train_loss_match = re.search(train_loss_pattern, line)
                if train_loss_match:
                    epoch_train_loss = float(train_loss_match.group(1))
                
                val_loss_match = re.search(val_loss_pattern, line)
                if val_loss_match:
                    epoch_val_loss = float(val_loss_match.group(1))
                
                # å¦‚æœæ‰¾åˆ°äº†è¯¥è½®æ¬¡çš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±ï¼Œè®°å½•å®ƒä»¬
                if epoch_train_loss is not None and epoch_val_loss is not None:
                    epochs.append(current_epoch)
                    train_losses.append(epoch_train_loss)
                    val_losses.append(epoch_val_loss)
                    
                    # é‡ç½®ä¸ºä¸‹ä¸€è½®æ¬¡
                    current_epoch = None
                    epoch_train_loss = None
                    epoch_val_loss = None
            
            # VGGæ ¼å¼è§£æ
            vgg_match = re.search(vgg_epoch_pattern, line)
            if vgg_match:
                epoch = int(vgg_match.group(1))
                val_loss = float(vgg_match.group(2))
                
                epochs.append(epoch)
                val_losses.append(val_loss)
                # VGGæ—¥å¿—é€šå¸¸ä¸è®°å½•è®­ç»ƒæŸå¤±ï¼Œä½¿ç”¨Noneå ä½
                train_losses.append(None)
        
        # å¦‚æœæˆåŠŸè§£æåˆ°æ•°æ®
        if epochs and val_losses:
            loss_data['epochs'] = epochs
            loss_data['val_losses'] = val_losses
            
            # è¿‡æ»¤æ‰Noneå€¼çš„è®­ç»ƒæŸå¤±
            valid_train_losses = [loss for loss in train_losses if loss is not None]
            if valid_train_losses and len(valid_train_losses) == len(val_losses):
                loss_data['train_losses'] = valid_train_losses
                print(f"   âœ“ ä»training.logè§£æåˆ°å®Œæ•´æ•°æ®: {len(epochs)}è½®ï¼Œè®­ç»ƒ+éªŒè¯æŸå¤±")
            else:
                print(f"   âœ“ ä»training.logè§£æåˆ°éªŒè¯æŸå¤±: {len(epochs)}è½®")
            
            return True
        
        print(f"   âŒ æ— æ³•ä»training.logè§£ææŸå¤±æ•°æ®")
        return False
    
    def _finalize_loss_data(self, loss_data):
        """å®ŒæˆæŸå¤±æ•°æ®çš„å¤„ç†"""
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if loss_data['train_losses'] and loss_data['val_losses']:
            # æ‰¾åˆ°æœ€ä½³éªŒè¯æŸå¤±
            val_losses = loss_data['val_losses']
            best_idx = np.argmin(val_losses)
            
            if loss_data['best_epoch'] is None:
                loss_data['best_epoch'] = loss_data['epochs'][best_idx] if loss_data['epochs'] else best_idx + 1
            if loss_data['best_val_loss'] is None:
                loss_data['best_val_loss'] = val_losses[best_idx]
            
            # æœ€ç»ˆæŸå¤±
            if loss_data['final_train_loss'] is None and loss_data['train_losses']:
                loss_data['final_train_loss'] = loss_data['train_losses'][-1]
            if loss_data['final_val_loss'] is None:
                loss_data['final_val_loss'] = loss_data['val_losses'][-1]
            
            print(f"   ğŸ“ˆ è®­ç»ƒè½®æ•°: {len(loss_data['val_losses'])}")
            print(f"   ğŸ¯ æœ€ä½³éªŒè¯æŸå¤±: {loss_data['best_val_loss']:.4f} (ç¬¬{loss_data['best_epoch']}è½®)")
            if loss_data['final_train_loss'] is not None:
                print(f"   ğŸ“Š æœ€ç»ˆè®­ç»ƒæŸå¤±: {loss_data['final_train_loss']:.4f}")
            print(f"   ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {loss_data['final_val_loss']:.4f}")
        elif loss_data['best_val_loss'] is not None:
            print(f"   ğŸ“Š éªŒè¯æŸå¤±: {loss_data['best_val_loss']:.4f}")
            if loss_data['best_epoch'] is not None:
                print(f"   ğŸ“Š æœ€ä½³è½®æ¬¡: {loss_data['best_epoch']}")
        
        return loss_data
    
    def analyze_models(self, model_dirs=None):
        """åˆ†æå¤šä¸ªæ¨¡å‹çš„æŸå¤±"""
        if model_dirs is None:
            model_dirs = self.detect_model_directories()
        
        if not model_dirs:
            print("âŒ æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•")
            return []
        
        print(f"\nğŸ” å¼€å§‹åˆ†æ {len(model_dirs)} ä¸ªæ¨¡å‹...")
        
        for model_dir in model_dirs:
            loss_data = self.load_loss_data(model_dir)
            if loss_data:
                self.models_data.append(loss_data)
        
        print(f"\nâœ… æˆåŠŸåˆ†æ {len(self.models_data)} ä¸ªæ¨¡å‹")
        return self.models_data
    
    def create_summary_table(self, output_dir="loss_analysis"):
        """åˆ›å»ºæ±‡æ€»è¡¨æ ¼"""
        if not self.models_data:
            print("âŒ æ²¡æœ‰æ¨¡å‹æ•°æ®å¯ç”Ÿæˆè¡¨æ ¼")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        
        # å‡†å¤‡æ•°æ®
        table_data = []
        for model in self.models_data:
            row = {
                'æ¨¡å‹åç§°': model['name'],
                'æ•°æ®æº': model['source_file'],
                'æœ€ä½³éªŒè¯æŸå¤±': model['best_val_loss'] if model['best_val_loss'] is not None else 'N/A',
                'æœ€ä½³è½®æ¬¡': model['best_epoch'] if model['best_epoch'] is not None else 'N/A',
                'æœ€ç»ˆéªŒè¯æŸå¤±': model['final_val_loss'] if model['final_val_loss'] is not None else 'N/A',
                'è®­ç»ƒè½®æ•°': len(model['val_losses']) if model['val_losses'] else 'N/A'
            }
            table_data.append(row)
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(table_data)
        
        # æŒ‰æœ€ä½³éªŒè¯æŸå¤±æ’åº
        def sort_key(x):
            if x == 'N/A':
                return float('inf')
            return float(x)
        
        df_sorted = df.copy()
        df_sorted['sort_key'] = df_sorted['æœ€ä½³éªŒè¯æŸå¤±'].apply(sort_key)
        df_sorted = df_sorted.sort_values('sort_key').drop('sort_key', axis=1)
        
        # ä¿å­˜CSV
        csv_path = os.path.join(output_dir, 'model_summary.csv')
        df_sorted.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜HTMLè¡¨æ ¼
        html_path = os.path.join(output_dir, 'model_summary.html')
        df_sorted.to_html(html_path, index=False, escape=False, 
                         table_id='model_summary', classes='table table-striped')
        
        print(f"âœ… æ±‡æ€»è¡¨æ ¼å·²ä¿å­˜:")
        print(f"   CSV: {csv_path}")
        print(f"   HTML: {html_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆæ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…·')
    parser.add_argument('--base_dir', type=str, default=None, help='åŸºç¡€ç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='enhanced_loss_analysis', help='è¾“å‡ºç›®å½•')
    args = parser.parse_args()
    
    print("ğŸš€ å¢å¼ºç‰ˆæ‰¹å¤„ç†æŸå¤±åˆ†æå·¥å…·")
    print("=" * 60)
    print("æ–°å¢åŠŸèƒ½:")
    print("- æ”¯æŒä»training.logè§£æVGGç­‰è®­ç»ƒæ—¥å¿—")
    print("- ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æå–å•ç‹¬çš„éªŒè¯æŸå¤±ä¿¡æ¯")
    print("- ä¼˜åŒ–çš„æ¨¡å‹ç›®å½•æ£€æµ‹å’Œå»é‡")
    print("- å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œå…¼å®¹æ€§")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = EnhancedBatchLossAnalyzer(args.base_dir)
    
    # åˆ†ææ¨¡å‹
    models_data = analyzer.analyze_models()
    
    if models_data:
        print(f"\nğŸ“ˆ ç”Ÿæˆåˆ†æç»“æœ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
        analyzer.create_summary_table(args.output_dir)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")
        print(f"ğŸ“Š æˆåŠŸåˆ†æ {len(models_data)} ä¸ªæ¨¡å‹")
        
        # æ˜¾ç¤ºç»“æœç»Ÿè®¡
        models_with_full_history = sum(1 for m in models_data if m['train_losses'] and m['val_losses'])
        models_with_val_only = sum(1 for m in models_data if m['best_val_loss'] is not None and not m['train_losses'])
        
        print(f"   - å®Œæ•´è®­ç»ƒå†å²: {models_with_full_history} ä¸ª")
        print(f"   - ä»…éªŒè¯æŸå¤±: {models_with_val_only} ä¸ª")
    else:
        print(f"âŒ æœªæ‰¾åˆ°å¯åˆ†æçš„æ¨¡å‹æ•°æ®")

if __name__ == "__main__":
    main() 
 
 
 